# RAG

## LangChain

[toc]

### 一、什么是 LangChain

ChatGPT 的巨大成功激发了越来越多的开发者兴趣，他们希望利用 OpenAI 提供的 API 或者私有化模型，来开发基于大型语言模型的应用程序。尽管大型语言模型的调用相对简单，但要创建完整的应用程序，仍然需要大量的定制开发工作，包括 API 集成、互动逻辑、数据存储等等。

为了解决这个问题，从 2022 年开始，许多机构和个人相继推出了多个开源项目，旨在**帮助开发者们快速构建基于大型语言模型的端到端应用程序或工作流程**。其中一个备受关注的项目就是 LangChain 框架。

**LangChain 框架是一个开源工具，充分利用了大型语言模型的强大能力，以便开发各种下游应用。它的目标是为各种大型语言模型应用提供通用接口，从而简化应用程序的开发流程**。具体来说，LangChain 框架可以实现数据感知和环境互动，也就是说，它能够让语言模型与其他数据来源连接，并且允许语言模型与其所处的环境进行互动。

利用 LangChain 框架，我们可以轻松地构建如下所示的 RAG 应用（[图片来源](https://github.com/chatchat-space/Langchain-Chatchat/blob/master/img/langchain+chatglm.png)）。在下图中，`每个椭圆形代表了 LangChain 的一个模块`，例如数据收集模块或预处理模块。`每个矩形代表了一个数据状态`，例如原始数据或预处理后的数据。箭头表示数据流的方向，从一个模块流向另一个模块。在每一步中，LangChain 都可以提供对应的解决方案，帮助我们处理各种任务。

![Langchain 示意图](assets/C1-3-langchain.png)

### 二、LangChain 的核心组件

LangChian 作为一个大语言模型开发框架，可以将 LLM 模型（对话模型、embedding 模型等）、向量数据库、交互层 Prompt、外部知识、外部代理工具整合到一起，进而可以自由构建 LLM 应用。 LangChain 主要由以下 6 个核心组件组成:

- **模型输入/输出（Model I/O）**：与语言模型交互的接口
- **数据连接（Data connection）**：与特定应用程序的数据进行交互的接口
- **链（Chains）**：将组件组合实现端到端应用。比如后续我们会将搭建`检索问答链`来完成检索问答。
- **记忆（Memory）**：用于链的多次运行之间持久化应用程序状态；
- **代理（Agents）**：扩展模型的推理能力。用于复杂的应用的调用序列；
- **回调（Callbacks）**：扩展模型的推理能力。用于复杂的应用的调用序列；

在开发过程中，我们可以根据自身需求灵活地进行组合。

### 三、LangChain 的稳定版本

在 LLM 技术领域的迅猛发展浪潮中，LangChain 作为一个不断进化的创新平台，持续推动着技术边界的拓展。`2024 年 1 月 9 日`，LangChain 正式发布了其稳定版本 **v0.1.0**，这一里程碑式的更新，为开发者带来了全面而强大的功能支持。其涵盖了模型的输入与输出处理、数据连接、链式操作、记忆机制、代理服务以及回调处理等关键组件，为 LLM 应用的开发和部署提供了坚实的基础。
同时，LangChain 的持续优化和功能迭代，未来将带来更多创新特性和性能提升。

- **兼容性与支持**：LangChain v0.1.0 版本兼顾了对 `Python 和 JavaScript` 的支持，同时保持了向后兼容性，确保开发者能够在升级过程中无缝过渡，享受到更加安全稳定的开发体验。

- **架构改进**：通过将核心组件 langchain-core 与合作伙伴包进行有效分离，LangChain 的架构设计变得更加条理清晰和稳固，为未来的系统化扩展和安全性提升奠定了坚实基础。

- **可观察性**：LangChain 通过与 LangSmith 的深度集成，提供了业界领先的调试和观测功能。这使得开发者能够对 LLM 应用中的每一步操作及其输入输出有一个清晰的认识，极大地简化了调试和问题排查的流程。

- **广泛的集成**：LangChain 拥有近 **700** 个集成，覆盖了从 LLM 到向量存储、工具和智能体（Agent）等多个技术领域，极大地降低了在各种技术栈上构建 LLM 应用的复杂度。

- **可组合性**：借助 `LangChain 表达式语言（LCEL）`，开发者可以轻松地构建和定制 chain，充分利用数据编排框架的优势，包括批量处理、并行化操作和备选方案等高级功能。

- **流式处理**：LangChain 对流式处理进行了深度优化，确保所有利用 LCEL 创建的 chain 均能支持流式处理，包括中间步骤的数据流传输，从而为用户提供更加流畅的体验。

- **输出解析**：LangChain 提供了一系列强大的输出解析工具，确保 LLM 能够以结构化的格式返回信息，这对于 LLM 执行具体行动计划至关重要。

- **检索能力**：LangChain 引入了先进的检索技术，适用于生产环境，包括文本分割、检索机制和索引管道等，使得开发者能够轻松地将私有数据与 LLM 的能力相结合。

- **工具使用与智能体**：LangChain 提供了丰富的智能体和工具集合，并提供了定义工具的简便方法，支持智能体工作负载，包括让 LLM 调用函数或工具，以及如何高效地进行多次调用和推理，极大地提升了开发效率和应用性能。

### 四、LangChain 的生态

- **LangChain Community**: 专注于第三方集成，极大地丰富了 LangChain 的生态系统，使得开发者可以更容易地构建复杂和强大的应用程序，同时也促进了社区的合作和共享。

- **LangChain Core**: LangChain 框架的核心库、核心组件，提供了基础抽象和 LangChain 表达式语言（LCEL），提供基础架构和工具，用于构建、运行和与 LLM 交互的应用程序，为 LangChain 应用程序的开发提供了坚实的基础。我们后续会用到的处理文档、格式化 prompt、输出解析等都来自这个库。

- **LangChain CLI**: 命令行工具，使开发者能够通过终端与 LangChain 框架交互，执行项目初始化、测试、部署等任务。提高开发效率，让开发者能够通过简单的命令来管理整个应用程序的生命周期。

- **LangServe**: 部署服务，用于将 LangChain 应用程序部署到云端，提供可扩展、高可用的托管解决方案，并带有监控和日志功能。简化部署流程，让开发者可以专注于应用程序的开发，而不必担心底层的基础设施和运维工作。

- **LangSmith**: 开发者平台，专注于 LangChain 应用程序的开发、调试和测试，提供可视化界面和性能分析工具，旨在帮助开发者提高应用程序的质量，确保它们在部署前达到预期的性能和稳定性标准。



## 开发 LLM 应用的整体流程

[toc]

### 一、何为大模型开发

我们将开发**以大语言模型为功能核心、通过大语言模型的强大理解能力和生成能力、结合特殊的数据或业务逻辑来提供独特功能的应用**称为**大模型开发**。开发大模型相关应用，其技术核心点虽然在大语言模型上，但一般通过调用 API 或开源模型来实现核心的理解与生成，通过 Prompt Enginnering 来实现大语言模型的控制，因此，虽然大模型是深度学习领域的集大成之作，大模型开发却更多是一个**工程问题**。

在大模型开发中，我们一般不会去大幅度改动模型，而是**将大模型作为一个调用工具，通过 Prompt Engineering、数据工程、业务逻辑分解等手段来充分发挥大模型能力，适配应用任务**，而不会将精力聚焦在优化模型本身上。因此，作为大模型开发的初学者，我们并不需要深研大模型内部原理，而更需要掌握使用大模型的实践技巧。

![大模型开发要素](assets/C1-4-LLM_developing.png)

<div align='center'>大模型开发要素</div>

同时，以调用、发挥大模型为核心的大模型开发与传统的 AI 开发在**整体思路**上有着较大的不同。大语言模型的两个核心能力：`指令遵循`与`文本生成`提供了复杂业务逻辑的简单平替方案。

- `传统的 AI 开发`：首先需要将非常复杂的业务逻辑依次拆解，对于每一个子业务构造训练数据与验证数据，对于每一个子业务训练优化模型，最后形成完整的模型链路来解决整个业务逻辑。
- `大模型开发`：用 Prompt Engineering 来替代子模型的训练调优，通过 Prompt 链路组合来实现业务逻辑，用一个通用大模型 + 若干业务 Prompt 来解决任务，从而将传统的模型训练调优转变成了更简单、轻松、低成本的 Prompt 设计调优。

同时，在**评估思路**上，大模型开发与传统 AI 开发也有质的差异。

- `传统 AI 开发`：需要首先构造训练集、测试集、验证集，通过在训练集上训练模型、在测试集上调优模型、在验证集上最终验证模型效果来实现性能的评估。
- `大模型开发`：流程更为灵活和敏捷。从实际业务需求出发构造小批量验证集，设计合理 Prompt 来满足验证集效果。然后，将不断从业务逻辑中收集当下 Prompt 的 Bad Case，并将 Bad Case 加入到验证集中，针对性优化 Prompt，最后实现较好的泛化效果。

![传统 AI 评估](assets/C1-4-AI_eval.png)

<div align = 'center'>传统 AI 评估</div>
<p>


![LLM 评估](assets/C1-4-LLM_eval.png)

<div align = 'center'>LLM 评估</div>
<p>


在本章中，我们将简述大模型开发的一般流程，并结合项目实际需求，逐步分析完成项目开发的工作和步骤。

### 二、大模型开发的一般流程

结合上述分析，我们一般可以将大模型开发分解为以下几个流程：

![大模型开发流程](assets/C1-4-LLM_developing_whole.png)

1. **确定目标**。在进行开发前，我们首先需要确定开发的目标，即要开发的应用的应用场景、目标人群、核心价值。对于个体开发者或小型开发团队而言，一般应先设定最小化目标，从构建一个 MVP（最小可行性产品）开始，逐步进行完善和优化。

2. **设计功能**。在确定开发目标后，需要设计本应用所要提供的功能，以及每一个功能的大体实现逻辑。虽然我们通过使用大模型来简化了业务逻辑的拆解，但是越清晰、深入的业务逻辑理解往往也能带来更好的 Prompt 效果。同样，对于个体开发者或小型开发团队来说，首先要确定应用的核心功能，然后延展设计核心功能的上下游功能；例如，我们想打造一款个人知识库助手，那么核心功能就是结合个人知识库内容进行问题的回答，那么其上游功能的用户上传知识库、下游功能的用户手动纠正模型回答就是我们也必须要设计实现的子功能。

3. **搭建整体架构**。目前，绝大部分大模型应用都是采用的特定数据库 + Prompt + 通用大模型的架构。我们需要针对我们所设计的功能，搭建项目的整体架构，实现从用户输入到应用输出的全流程贯通。一般来说，我们推荐基于 LangChain 框架进行开发。LangChain 提供了 Chain、Tool 等架构的实现，我们可以基于 LangChain 进行个性化定制，实现从用户输入到数据库再到大模型最后输出的整体架构连接。

4. **搭建数据库**。个性化大模型应用需要有个性化数据库进行支撑。由于大模型应用需要进行向量语义检索，一般使用诸如 Chroma 的向量数据库。在该步骤中，我们需要收集数据并进行预处理，再向量化存储到数据库中。数据预处理一般包括从多种格式向纯文本的转化，例如 PDF、MarkDown、HTML、音视频等，以及对错误数据、异常数据、脏数据进行清洗。完成预处理后，需要进行切片、向量化构建出个性化数据库。

5. **Prompt Engineering**。优质的 Prompt 对大模型能力具有极大影响，我们需要逐步迭代构建优质的 Prompt Engineering 来提升应用性能。在该步中，我们首先应该明确 Prompt 设计的一般原则及技巧，构建出一个来源于实际业务的小型验证集，基于小型验证集设计满足基本要求、具备基本能力的 Prompt。

6. **验证迭代**。验证迭代在大模型开发中是极其重要的一步，一般指通过不断发现 Bad Case 并针对性改进 Prompt Engineering 来提升系统效果、应对边界情况。在完成上一步的初始化 Prompt 设计后，我们应该进行实际业务测试，探讨边界情况，找到 Bad Case，并针对性分析 Prompt 存在的问题，从而不断迭代优化，直到达到一个较为稳定、可以基本实现目标的 Prompt 版本。

7. **前后端搭建**。完成 Prompt Engineering 及其迭代优化之后，我们就完成了应用的核心功能，可以充分发挥大语言模型的强大能力。接下来我们需要搭建前后端，设计产品页面，让我们的应用能够上线成为产品。前后端开发是非常经典且成熟的领域，此处就不再赘述，我们采用 Gradio 和 Streamlit，可以帮助个体开发者迅速搭建可视化页面实现 Demo 上线。

8. **体验优化**。在完成前后端搭建之后，应用就可以上线体验了。接下来就需要进行长期的用户体验跟踪，记录 Bad Case 与用户负反馈，再针对性进行优化即可。

### 三、搭建 LLM 项目的流程简析（以知识库助手为例）

以下我们将结合本实践项目与上文的整体流程介绍，简要分析[知识库助手项目](https://github.com/logan-zou/Chat_with_Datawhale_langchain)开发流程：

#### 步骤一：项目规划与需求分析

**1.项目目标：基于个人知识库的问答助手**

**2.核心功能**

1. 将爬取并总结的 MarkDown 文件及用户上传文档向量化，并创建知识库；
2. 选择知识库，检索用户提问的知识片段；
3. 提供知识片段与提问，获取大模型回答；
4. 流式回复；
5. 历史对话记录

**3.确定技术架构和工具**

1. **框架**：LangChain
2. **Embedding 模型**：GPT、智谱、[M3E](https://huggingface.co/moka-ai/m3e-base)
3. **数据库**：Chroma
4. **大模型**：GPT、讯飞星火、文心一言、GLM 等
5. **前后端**：Gradio 和 Streamlit

#### 步骤二：数据准备与向量知识库构建

本项目实现原理如下图所示（[图片来源](https://github.com/chatchat-space/Langchain-Chatchat/blob/master/img/langchain+chatglm.png)）：加载本地文档 -> 读取文本 -> 文本分割 -> 文本向量化 -> question 向量化 -> 在文本向量中匹配出与问句向量最相似的 top k 个 -> 匹配出的文本作为上下文和问题一起添加到 Prompt 中 -> 提交给 LLM 生成回答。

![](assets/C1-4-flow_chart.png)

**1.收集和整理用户提供的文档**

用户常用文档格式有 PDF、TXT、MD 等，首先，我们可以使用 LangChain 的文档加载器模块方便地加载用户提供的文档，或者使用一些成熟的 Python 包进行读取。

由于目前大模型使用 token 的限制，我们需要对读取的文本进行切分，将较长的文本切分为较小的文本，这时一段文本就是一个单位的知识。

**2.将文档词向量化**

使用`文本嵌入(Embeddings)技术`对分割后的文档进行向量化，使语义相似的文本片段具有接近的向量表示。然后，存入向量数据库，完成 `索引(index)` 的创建。

利用向量数据库对各文档片段进行索引，可以实现快速检索。

**3.将向量化后的文档导入 Chroma 知识库，建立知识库索引**

Langchain 集成了超过 30 个不同的向量数据库。Chroma 数据库轻量级且数据存储在内存中，这使得它非常容易启动和开始使用。

将用户知识库内容经过 Embedding 存入向量数据库，然后用户每一次提问也会经过 Embedding，利用向量相关性算法（例如余弦算法）找到最匹配的几个知识库片段，将这些知识库片段作为上下文，与用户问题一起作为 Prompt 提交给 LLM 回答。

#### 步骤三：大模型集成与 API 连接

1. 集成 GPT、星火、文心、GLM 等大模型，配置 API 连接。
2. 编写代码，实现与大模型 API 的交互，以便获取问题回答。

#### 步骤四：核心功能实现

1. 构建 Prompt Engineering，实现大模型回答功能，根据用户提问和知识库内容生成回答。
2. 实现流式回复，允许用户进行多轮对话。
3. 添加历史对话记录功能，保存用户与助手的交互历史。

#### 步骤五：核心功能迭代优化

1. 进行验证评估，收集 Bad Case。
2. 根据 Bad Case 迭代优化核心功能实现。

#### 步骤六：前端与用户交互界面开发

1. 使用 Gradio 和 Streamlit 搭建前端界面。
2. 实现用户上传文档、创建知识库的功能。
3. 设计用户界面，包括问题输入、知识库选择、历史记录展示等。

#### 步骤七：部署测试与上线

1. 部署问答助手到服务器或云平台，确保可在互联网上访问。
2. 进行生产环境测试，确保系统稳定。
3. 上线并向用户发布。

#### 步骤八：维护与持续改进

1. 监测系统性能和用户反馈，及时处理问题。
2. 定期更新知识库，添加新的文档和信息。
3. 收集用户需求，进行系统改进和功能扩展。

整个流程将确保项目从规划、开发、测试到上线和维护都能够顺利进行，为用户提供高质量的基于个人知识库的问答助手。





