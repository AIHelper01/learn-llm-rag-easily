{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ LLM API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ¬ç« èŠ‚ä¸»è¦ä»‹ç»æ™ºè°± GLMçš„ API ç”³è¯·æŒ‡å¼•å’Œ Python ç‰ˆæœ¬çš„åŸç”Ÿ API è°ƒç”¨æ–¹æ³•ï¼Œè¯»è€…æŒ‰ç…§å®é™…æƒ…å†µé€‰æ‹©ä¸€ç§è‡ªå·±å¯ä»¥ç”³è¯·çš„ API è¿›è¡Œé˜…è¯»å­¦ä¹ å³å¯ã€‚\n",
    "\n",
    "å¦‚æœä½ éœ€è¦åœ¨ LangChain ä¸­ä½¿ç”¨ LLMï¼Œå¯ä»¥å‚ç…§[LLM æ¥å…¥ LangChain](https://github.com/datawhalechina/llm-universe/blob/main/notebook/C4%20%E6%9E%84%E5%BB%BA%20RAG%20%E5%BA%94%E7%94%A8/1.LLM%20%E6%8E%A5%E5%85%A5%20LangChain.ipynb)ä¸­çš„è°ƒç”¨æ–¹å¼ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‚è€ƒä½¿ç”¨ ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPTï¼Œå‘å¸ƒäº 2022 å¹´ 11 æœˆï¼Œæ˜¯ç›®å‰ç«çƒ­å‡ºåœˆçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼ŒLLMï¼‰çš„ä»£è¡¨äº§å“ã€‚åœ¨ 2022 å¹´åº•ï¼Œä¹Ÿæ­£æ˜¯ ChatGPT çš„æƒŠäººè¡¨ç°å¼•å‘äº† LLM çš„çƒ­æ½®ã€‚æ—¶è‡³ç›®å‰ï¼Œç”± OpenAI å‘å¸ƒçš„ GPT-4 ä»ç„¶æ˜¯ LLM æ€§èƒ½ä¸Šé™çš„ä»£è¡¨ï¼ŒChatGPT ä¹Ÿä»ç„¶æ˜¯ç›®å‰ä½¿ç”¨äººæ•°æœ€å¤šã€ä½¿ç”¨çƒ­åº¦æœ€å¤§ã€æœ€å…·å‘å±•æ½œåŠ›çš„ LLM äº§å“ã€‚äº‹å®ä¸Šï¼Œåœ¨åœˆå¤–äººçœ‹æ¥ï¼ŒChatGPT å³æ˜¯ LLM çš„ä»£ç§°ã€‚\n",
    "\n",
    "OpenAI é™¤å‘å¸ƒäº†å…è´¹çš„ Web ç«¯äº§å“å¤–ï¼Œä¹Ÿæä¾›äº†å¤šç§ ChatGPT APIï¼Œæ”¯æŒå¼€å‘è€…é€šè¿‡ Python æˆ– Request è¯·æ±‚æ¥è°ƒç”¨ ChatGPTï¼Œå‘è‡ªå·±çš„æœåŠ¡ä¸­åµŒå…¥ LLM çš„å¼ºå¤§èƒ½åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API ç”³è¯·æŒ‡å¼•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è·å–å¹¶é…ç½® OpenAI API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI API è°ƒç”¨æœåŠ¡æ˜¯ä»˜è´¹çš„ï¼Œæ¯ä¸€ä¸ªå¼€å‘è€…éƒ½éœ€è¦é¦–å…ˆè·å–å¹¶é…ç½® OpenAI API keyï¼Œæ‰èƒ½åœ¨è‡ªå·±æ„å»ºçš„åº”ç”¨ä¸­è®¿é—® ChatGPTã€‚æˆ‘ä»¬å°†åœ¨è¿™éƒ¨åˆ†ç®€è¿°å¦‚ä½•è·å–å¹¶é…ç½® OpenAI API keyã€‚\n",
    "\n",
    "åœ¨è·å– OpenAI API key ä¹‹å‰æˆ‘ä»¬éœ€è¦åœ¨[OpenAI å®˜ç½‘](https://openai.com/)æ³¨å†Œä¸€ä¸ªè´¦å·ã€‚è¿™é‡Œå‡è®¾æˆ‘ä»¬å·²ç»æœ‰äº† OpenAI è´¦å·ï¼Œåœ¨[OpenAI å®˜ç½‘](https://openai.com/)ç™»å½•ï¼Œç™»å½•åå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../../figures/C2-2-openai-choose.png\" width=\"1000\" alt=\"OpenAI å®˜ç½‘ç™»å½•åé€‰æ‹© API\">\n",
    "</p>\n",
    "\n",
    "æˆ‘ä»¬é€‰æ‹© `API`ï¼Œç„¶åç‚¹å‡»å·¦ä¾§è¾¹æ çš„ `API keys`ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../../figures/C2-2-openai-get-key.png\" width=\"1000\" alt=\"OpenAI è·å– API key\">\n",
    "</p>\n",
    "\n",
    "ç‚¹å‡» `Create new secret key` æŒ‰é’®åˆ›å»º OpenAI API key ï¼Œæˆ‘ä»¬å°†åˆ›å»ºå¥½çš„ OpenAI API key å¤åˆ¶ä»¥æ­¤å½¢å¼ `OPENAI_API_KEY=\"sk-...\"` ä¿å­˜åˆ° `.env` æ–‡ä»¶ä¸­ï¼Œå¹¶å°† `.env` æ–‡ä»¶ä¿å­˜åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ã€‚\n",
    "\n",
    "ä¸‹é¢æ˜¯è¯»å– `.env` æ–‡ä»¶çš„ä»£ç ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# è¯»å–æœ¬åœ°/é¡¹ç›®çš„ç¯å¢ƒå˜é‡ã€‚\n",
    "\n",
    "# find_dotenv() å¯»æ‰¾å¹¶å®šä½ .env æ–‡ä»¶çš„è·¯å¾„\n",
    "# load_dotenv() è¯»å–è¯¥ .env æ–‡ä»¶ï¼Œå¹¶å°†å…¶ä¸­çš„ç¯å¢ƒå˜é‡åŠ è½½åˆ°å½“å‰çš„è¿è¡Œç¯å¢ƒä¸­  \n",
    "# å¦‚æœä½ è®¾ç½®çš„æ˜¯å…¨å±€çš„ç¯å¢ƒå˜é‡ï¼Œè¿™è¡Œä»£ç åˆ™æ²¡æœ‰ä»»ä½•ä½œç”¨ã€‚\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# å¦‚æœä½ éœ€è¦é€šè¿‡ä»£ç†ç«¯å£è®¿é—®ï¼Œè¿˜éœ€è¦åšå¦‚ä¸‹é…ç½®\n",
    "# os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "# os.environ[\"HTTP_PROXY\"] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è°ƒç”¨ OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è°ƒç”¨ ChatGPT éœ€è¦ä½¿ç”¨ [ChatCompletion API](https://platform.openai.com/docs/api-reference/chat)ï¼Œè¯¥ API æä¾›äº† ChatGPT ç³»åˆ—æ¨¡å‹çš„è°ƒç”¨ï¼ŒåŒ…æ‹¬ ChatGPT-3.5ï¼ŒGPT-4 ç­‰ã€‚\n",
    "\n",
    "ChatCompletion API è°ƒç”¨æ–¹æ³•å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    # api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    api_key='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'\n",
    ")\n",
    "\n",
    "\n",
    "# å¯¼å…¥æ‰€éœ€åº“\n",
    "# æ³¨æ„ï¼Œæ­¤å¤„æˆ‘ä»¬å‡è®¾ä½ å·²æ ¹æ®ä¸Šæ–‡é…ç½®äº† OpenAI API Keyï¼Œå¦‚æ²¡æœ‰å°†è®¿é—®å¤±è´¥\n",
    "completion = client.chat.completions.create(\n",
    "    # è°ƒç”¨æ¨¡å‹ï¼šChatGPT-3.5\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    # messages æ˜¯å¯¹è¯åˆ—è¡¨\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è°ƒç”¨è¯¥ API ä¼šè¿”å›ä¸€ä¸ª ChatCompletion å¯¹è±¡ï¼Œå…¶ä¸­åŒ…æ‹¬äº†å›ç­”æ–‡æœ¬ã€åˆ›å»ºæ—¶é—´ã€id ç­‰å±æ€§ã€‚æˆ‘ä»¬ä¸€èˆ¬éœ€è¦çš„æ˜¯å›ç­”æ–‡æœ¬ï¼Œä¹Ÿå°±æ˜¯å›ç­”å¯¹è±¡ä¸­çš„ content ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­¤å¤„æˆ‘ä»¬è¯¦ç»†ä»‹ç»è°ƒç”¨ API å¸¸ä¼šç”¨åˆ°çš„å‡ ä¸ªå‚æ•°ï¼š\n",
    "\n",
    "    Â· modelï¼Œå³è°ƒç”¨çš„æ¨¡å‹ï¼Œä¸€èˆ¬å–å€¼åŒ…æ‹¬â€œgpt-3.5-turboâ€ï¼ˆChatGPT-3.5ï¼‰ã€â€œgpt-3.5-turbo-16k-0613â€ï¼ˆChatGPT-3.5 16K ç‰ˆæœ¬ï¼‰ã€â€œgpt-4â€ï¼ˆChatGPT-4ï¼‰ã€‚æ³¨æ„ï¼Œä¸åŒæ¨¡å‹çš„æˆæœ¬æ˜¯ä¸ä¸€æ ·çš„ã€‚\n",
    "\n",
    "    Â· messagesï¼Œå³æˆ‘ä»¬çš„ promptã€‚ChatCompletion çš„ messages éœ€è¦ä¼ å…¥ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­åŒ…æ‹¬å¤šä¸ªä¸åŒè§’è‰²çš„ promptã€‚æˆ‘ä»¬å¯ä»¥é€‰æ‹©çš„è§’è‰²ä¸€èˆ¬åŒ…æ‹¬ systemï¼šå³å‰æ–‡ä¸­æåˆ°çš„ system promptï¼›userï¼šç”¨æˆ·è¾“å…¥çš„ promptï¼›assistantï¼šåŠ©æ‰‹ï¼Œä¸€èˆ¬æ˜¯æ¨¡å‹å†å²å›å¤ï¼Œä½œä¸ºæä¾›ç»™æ¨¡å‹çš„å‚è€ƒå†…å®¹ã€‚\n",
    "\n",
    "    Â· temperatureï¼Œæ¸©åº¦ã€‚å³å‰æ–‡ä¸­æåˆ°çš„ Temperature ç³»æ•°ã€‚\n",
    "\n",
    "    Â· max_tokensï¼Œæœ€å¤§ token æ•°ï¼Œå³æ¨¡å‹è¾“å‡ºçš„æœ€å¤§ token æ•°ã€‚OpenAI è®¡ç®— token æ•°æ˜¯åˆå¹¶è®¡ç®— Prompt å’Œ Completion çš„æ€» token æ•°ï¼Œè¦æ±‚æ€» token æ•°ä¸èƒ½è¶…è¿‡æ¨¡å‹ä¸Šé™ï¼ˆå¦‚é»˜è®¤æ¨¡å‹ token ä¸Šé™ä¸º 4096ï¼‰ã€‚å› æ­¤ï¼Œå¦‚æœè¾“å…¥çš„ prompt è¾ƒé•¿ï¼Œéœ€è¦è®¾ç½®è¾ƒå¤§çš„ max_token å€¼ï¼Œå¦åˆ™ä¼šæŠ¥é”™è¶…å‡ºé™åˆ¶é•¿åº¦ã€‚\n",
    "\n",
    "OpenAI æä¾›äº†å……åˆ†çš„è‡ªå®šä¹‰ç©ºé—´ï¼Œæ”¯æŒæˆ‘ä»¬é€šè¿‡è‡ªå®šä¹‰ prompt æ¥æå‡æ¨¡å‹å›ç­”æ•ˆæœï¼Œå¦‚ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„å°è£… OpenAI æ¥å£çš„å‡½æ•°ï¼Œæ”¯æŒæˆ‘ä»¬ç›´æ¥ä¼ å…¥ prompt å¹¶è·å¾—æ¨¡å‹çš„è¾“å‡ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    # api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    api_key='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'\n",
    ")\n",
    "\n",
    "\n",
    "def gen_gpt_messages(prompt):\n",
    "    '''\n",
    "    æ„é€  GPT æ¨¡å‹è¯·æ±‚å‚æ•° messages\n",
    "    \n",
    "    è¯·æ±‚å‚æ•°ï¼š\n",
    "        prompt: å¯¹åº”çš„ç”¨æˆ·æç¤ºè¯\n",
    "    '''\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature = 0):\n",
    "    '''\n",
    "    è·å– GPT æ¨¡å‹è°ƒç”¨ç»“æœ\n",
    "\n",
    "    è¯·æ±‚å‚æ•°ï¼š\n",
    "        prompt: å¯¹åº”çš„æç¤ºè¯\n",
    "        model: è°ƒç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ä¸º gpt-3.5-turboï¼Œä¹Ÿå¯ä»¥æŒ‰éœ€é€‰æ‹© gpt-4 ç­‰å…¶ä»–æ¨¡å‹\n",
    "        temperature: æ¨¡å‹è¾“å‡ºçš„æ¸©åº¦ç³»æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºç¨‹åº¦ï¼Œå–å€¼èŒƒå›´æ˜¯ 0~2ã€‚æ¸©åº¦ç³»æ•°è¶Šä½ï¼Œè¾“å‡ºå†…å®¹è¶Šä¸€è‡´ã€‚\n",
    "    '''\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=gen_gpt_messages(prompt),\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    if len(response.choices) > 0:\n",
    "        return response.choices[0].message.content\n",
    "    return \"generate answer error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_completion(\"ä½ å¥½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨ä¸Šè¿°å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬å°è£…äº† messages çš„ç»†èŠ‚ï¼Œä»…ä½¿ç”¨ user prompt æ¥å®ç°è°ƒç”¨ã€‚åœ¨ç®€å•åœºæ™¯ä¸­ï¼Œè¯¥å‡½æ•°è¶³å¤Ÿæ»¡è¶³ä½¿ç”¨éœ€æ±‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨æ™ºè°± GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ™ºè°± AI æ˜¯ç”±æ¸…åå¤§å­¦è®¡ç®—æœºç³»æŠ€æœ¯æˆæœè½¬åŒ–è€Œæ¥çš„å…¬å¸ï¼Œè‡´åŠ›äºæ‰“é€ æ–°ä¸€ä»£è®¤çŸ¥æ™ºèƒ½é€šç”¨æ¨¡å‹ã€‚å…¬å¸åˆä½œç ”å‘äº†åŒè¯­åƒäº¿çº§è¶…å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ GLM-130Bï¼Œå¹¶æ„å»ºäº†é«˜ç²¾åº¦é€šç”¨çŸ¥è¯†å›¾è°±ï¼Œå½¢æˆæ•°æ®ä¸çŸ¥è¯†åŒè½®é©±åŠ¨çš„è®¤çŸ¥å¼•æ“ï¼ŒåŸºäºæ­¤æ¨¡å‹æ‰“é€ äº† ChatGLMï¼ˆchatglm.cnï¼‰ã€‚\n",
    "\n",
    "ChatGLM ç³»åˆ—æ¨¡å‹ï¼ŒåŒ…æ‹¬ ChatGLM-130Bã€ChatGLM-6B å’Œ ChatGLM2-6Bï¼ˆChatGLM-6B çš„å‡çº§ç‰ˆæœ¬ï¼‰æ¨¡å‹ï¼Œæ”¯æŒç›¸å¯¹å¤æ‚çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶ä¸”èƒ½å¤Ÿè§£å†³å›°éš¾çš„æ¨ç†ç±»é—®é¢˜ã€‚å…¶ä¸­ï¼ŒChatGLM-6B æ¨¡å‹æ¥è‡ª Huggingface ä¸Šçš„ä¸‹è½½é‡å·²ç»è¶…è¿‡ 300wï¼ˆæˆªè‡³ 2023 å¹´ 6 æœˆ 24 æ—¥ç»Ÿè®¡æ•°æ®ï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨ Hugging Face (HF) å…¨çƒå¤§æ¨¡å‹ä¸‹è½½æ¦œä¸­è¿ç»­ 12 å¤©ä½å±…ç¬¬ä¸€åï¼Œåœ¨å›½å†…å¤–çš„å¼€æºç¤¾åŒºä¸­äº§ç”Ÿäº†è¾ƒå¤§çš„å½±å“ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API ç”³è¯·æŒ‡å¼•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é¦–å…ˆè¿›å…¥åˆ° [æ™ºè°±AIå¼€æ”¾å¹³å°](https://open.bigmodel.cn/overview)ï¼Œç‚¹å‡»`å¼€å§‹ä½¿ç”¨`æˆ–è€…`å¼€å‘å·¥ä½œå°`è¿›è¡Œæ³¨å†Œï¼š\n",
    "\n",
    "![](../../figures/C2-2-zhipuai_home.png)\n",
    "\n",
    "æ–°æ³¨å†Œçš„ç”¨æˆ·å¯ä»¥å…è´¹é¢†å–æœ‰æ•ˆæœŸ 1 ä¸ªæœˆçš„ 100w token çš„ä½“éªŒåŒ…ï¼Œè¿›è¡Œä¸ªäººå®åè®¤è¯åï¼Œè¿˜å¯ä»¥é¢å¤–é¢†å– 400w token ä½“éªŒåŒ…ã€‚æ™ºè°± AI æä¾›äº† GLM-4 å’Œ GLM-3-Turbo è¿™ä¸¤ç§ä¸åŒæ¨¡å‹çš„ä½“éªŒå…¥å£ï¼Œå¯ä»¥ç‚¹å‡»`ç«‹å³ä½“éªŒ`æŒ‰é’®ç›´æ¥ä½“éªŒã€‚\n",
    "\n",
    "![æ™ºè°± AI æ§åˆ¶å°](../../figures/C2-2-zhipuai_overview.png)\n",
    "\n",
    "å¯¹äºéœ€è¦ä½¿ç”¨ API key æ¥æ­å»ºåº”ç”¨çš„è¯ï¼Œéœ€è¦ç‚¹å‡»å³ä¾§çš„`æŸ¥çœ‹ API key`æŒ‰é’®ï¼Œå°±ä¼šè¿›å…¥åˆ°æˆ‘ä»¬ä¸ªäººçš„ API ç®¡ç†åˆ—è¡¨ä¸­ã€‚åœ¨è¯¥ç•Œé¢ï¼Œå°±å¯ä»¥çœ‹åˆ°æˆ‘ä»¬è·å–åˆ°çš„ API æ‰€å¯¹åº”çš„åº”ç”¨åå­—å’Œ `API key` äº†ã€‚\n",
    "\n",
    "![æ™ºè°± AI api ç®¡ç†](../../figures/C2-2-zhipuai_api.png)\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥ç‚¹å‡» `æ·»åŠ æ–°çš„ API key` å¹¶è¾“å…¥å¯¹åº”çš„åå­—å³å¯ç”Ÿæˆæ–°çš„ API keyã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è°ƒç”¨æ™ºè°± GLM API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ™ºè°± AI æä¾›äº† SDK å’ŒåŸç”Ÿ HTTP æ¥å®ç°æ¨¡å‹ API çš„è°ƒç”¨ï¼Œå»ºè®®ä½¿ç”¨ SDK è¿›è¡Œè°ƒç”¨ä»¥è·å¾—æ›´å¥½çš„ç¼–ç¨‹ä½“éªŒã€‚\n",
    "\n",
    "é¦–å…ˆæˆ‘ä»¬éœ€è¦é…ç½®å¯†é’¥ä¿¡æ¯ï¼Œå°†å‰é¢è·å–åˆ°çš„ `API key` è®¾ç½®åˆ° `.env` æ–‡ä»¶ä¸­çš„ `ZHIPUAI_API_KEY` å‚æ•°ï¼Œç„¶åè¿è¡Œä»¥ä¸‹ä»£ç åŠ è½½é…ç½®ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# è¯»å–æœ¬åœ°/é¡¹ç›®çš„ç¯å¢ƒå˜é‡ã€‚\n",
    "\n",
    "# find_dotenv() å¯»æ‰¾å¹¶å®šä½ .env æ–‡ä»¶çš„è·¯å¾„\n",
    "# load_dotenv() è¯»å–è¯¥ .env æ–‡ä»¶ï¼Œå¹¶å°†å…¶ä¸­çš„ç¯å¢ƒå˜é‡åŠ è½½åˆ°å½“å‰çš„è¿è¡Œç¯å¢ƒä¸­  \n",
    "# å¦‚æœä½ è®¾ç½®çš„æ˜¯å…¨å±€çš„ç¯å¢ƒå˜é‡ï¼Œè¿™è¡Œä»£ç åˆ™æ²¡æœ‰ä»»ä½•ä½œç”¨ã€‚\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/will/workspace/llm-rag/.env'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ™ºè°±çš„è°ƒç”¨ä¼ å‚å’Œå…¶ä»–ç±»ä¼¼ï¼Œä¹Ÿéœ€è¦ä¼ å…¥ä¸€ä¸ª messages åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­åŒ…æ‹¬ role å’Œ promptã€‚æˆ‘ä»¬å°è£…å¦‚ä¸‹çš„ `get_completion` å‡½æ•°ï¼Œä¾›åç»­ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai import ZhipuAI\n",
    "\n",
    "client = ZhipuAI(\n",
    "    api_key=os.environ[\"ZHIPUAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "def gen_glm_params(prompt):\n",
    "    '''\n",
    "    æ„é€  GLM æ¨¡å‹è¯·æ±‚å‚æ•° messages\n",
    "\n",
    "    è¯·æ±‚å‚æ•°ï¼š\n",
    "        prompt: å¯¹åº”çš„ç”¨æˆ·æç¤ºè¯\n",
    "    '''\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def get_completion(prompt, model=\"glm-4\", temperature=0.95):\n",
    "    '''\n",
    "    è·å– GLM æ¨¡å‹è°ƒç”¨ç»“æœ\n",
    "\n",
    "    è¯·æ±‚å‚æ•°ï¼š\n",
    "        prompt: å¯¹åº”çš„æç¤ºè¯\n",
    "        model: è°ƒç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ä¸º glm-4ï¼Œä¹Ÿå¯ä»¥æŒ‰éœ€é€‰æ‹© glm-3-turbo ç­‰å…¶ä»–æ¨¡å‹\n",
    "        temperature: æ¨¡å‹è¾“å‡ºçš„æ¸©åº¦ç³»æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºç¨‹åº¦ï¼Œå–å€¼èŒƒå›´æ˜¯ 0~1.0ï¼Œä¸”ä¸èƒ½è®¾ç½®ä¸º 0ã€‚æ¸©åº¦ç³»æ•°è¶Šä½ï¼Œè¾“å‡ºå†…å®¹è¶Šä¸€è‡´ã€‚\n",
    "    '''\n",
    "\n",
    "    messages = gen_glm_params(prompt)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    if len(response.choices) > 0:\n",
    "        return response.choices[0].message.content\n",
    "    return \"generate answer error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹æ™ºè°±æ¸…è¨€ï¼Œå¯ä»¥å«æˆ‘å°æ™ºğŸ¤–ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"ä½ å¥½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œå¯¹ä¼ å…¥ zhipuai çš„å‚æ•°è¿›è¡Œç®€å•ä»‹ç»ï¼š\n",
    "\n",
    "- `messages (list)`ï¼Œè°ƒç”¨å¯¹è¯æ¨¡å‹æ—¶ï¼Œå°†å½“å‰å¯¹è¯ä¿¡æ¯åˆ—è¡¨ä½œä¸ºæç¤ºè¾“å…¥ç»™æ¨¡å‹ï¼›æŒ‰ç…§ {\"role\": \"user\", \"content\": \"ä½ å¥½\"} çš„é”®å€¼å¯¹å½¢å¼è¿›è¡Œä¼ å‚ï¼›æ€»é•¿åº¦è¶…è¿‡æ¨¡å‹æœ€é•¿è¾“å…¥é™åˆ¶åä¼šè‡ªåŠ¨æˆªæ–­ï¼Œéœ€æŒ‰æ—¶é—´ç”±æ—§åˆ°æ–°æ’åº\n",
    "\n",
    "- `temperature (float)`ï¼Œé‡‡æ ·æ¸©åº¦ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼Œå¿…é¡»ä¸ºæ­£æ•°å–å€¼èŒƒå›´æ˜¯ï¼š(0.0, 1.0)ï¼Œä¸èƒ½ç­‰äº 0ï¼Œé»˜è®¤å€¼ä¸º 0.95ã€‚å€¼è¶Šå¤§ï¼Œä¼šä½¿è¾“å‡ºæ›´éšæœºï¼Œæ›´å…·åˆ›é€ æ€§ï¼›å€¼è¶Šå°ï¼Œè¾“å‡ºä¼šæ›´åŠ ç¨³å®šæˆ–ç¡®å®š\n",
    "  \n",
    "- `top_p (float)`ï¼Œç”¨æ¸©åº¦å–æ ·çš„å¦ä¸€ç§æ–¹æ³•ï¼Œç§°ä¸ºæ ¸å–æ ·ã€‚å–å€¼èŒƒå›´æ˜¯ï¼š(0.0, 1.0) å¼€åŒºé—´ï¼Œä¸èƒ½ç­‰äº 0 æˆ– 1ï¼Œé»˜è®¤å€¼ä¸º 0.7ã€‚æ¨¡å‹è€ƒè™‘å…·æœ‰ top_p æ¦‚ç‡è´¨é‡ tokens çš„ç»“æœã€‚ä¾‹å¦‚ï¼š0.1 æ„å‘³ç€æ¨¡å‹è§£ç å™¨åªè€ƒè™‘ä»å‰ 10% çš„æ¦‚ç‡çš„å€™é€‰é›†ä¸­å– tokens\n",
    "\n",
    "- `request_id (string)`ï¼Œç”±ç”¨æˆ·ç«¯ä¼ å‚ï¼Œéœ€ä¿è¯å”¯ä¸€æ€§ï¼›ç”¨äºåŒºåˆ†æ¯æ¬¡è¯·æ±‚çš„å”¯ä¸€æ ‡è¯†ï¼Œç”¨æˆ·ç«¯ä¸ä¼ æ—¶å¹³å°ä¼šé»˜è®¤ç”Ÿæˆ\n",
    "\n",
    "- **å»ºè®®æ‚¨æ ¹æ®åº”ç”¨åœºæ™¯è°ƒæ•´ top_p æˆ– temperature å‚æ•°ï¼Œä½†ä¸è¦åŒæ—¶è°ƒæ•´ä¸¤ä¸ªå‚æ•°**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å…¶ä»–æ¨èçš„APIï¼š**  \n",
    "ChatGPT  \n",
    "æ–‡å¿ƒä¸€è¨€  \n",
    "ä½¿ç”¨è®¯é£æ˜Ÿç«  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ æœ¬åœ°å¼€æºLLM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è°ƒç”¨æœ¬åœ°ä½¿ç”¨ollamaéƒ¨ç½²çš„å¤§æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è°ƒç”¨æœ¬åœ°å¤§æ¨¡å‹ï¼ŒåŒæ ·å¯ä»¥ä½¿ç”¨ [ChatCompletion API](https://platform.openai.com/docs/api-reference/chat)ï¼Œè¯¥ API æä¾›äº† ChatGPT ç³»åˆ—æ¨¡å‹çš„è°ƒç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'äººç±»åˆ›ä½œå’Œä½¿ç”¨çš„è®¡ç®—æœºç¨‹åºï¼Œä¸€èˆ¬æŒ‡ç”¨äºç”Ÿæˆæ–‡æœ¬çš„å¤§å‹äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚å®ƒä»¬æ‹¥æœ‰æ¨¡ä»¿äººç±»èƒ½åŠ›çš„è¯­è¨€å¤„ç†åŠŸèƒ½ï¼Œå…·å¤‡è¯­ä¹‰ç†è§£ã€è¡¨è¾¾èƒ½åŠ›å’Œé€»è¾‘æ¨ç†ç­‰è‡ªç„¶è¯­è¨€å¤„ç†çš„èƒ½åŠ›ã€‚å¤§è¯­è¨€æ¨¡å‹ä¸»è¦åŸºäºè®­ç»ƒï¼Œæ¨¡æ‹Ÿäººç±»è¯­è¨€ï¼Œé€šå¸¸è¢«ç”¨æ¥è¿›è¡Œæ™ºèƒ½æœåŠ¡ã€èŠå¤©äº¤æµã€ææ„è§ä»¥åŠè‡ªåŠ¨ç¿»è¯‘ç­‰å·¥ä½œã€‚'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1/',\n",
    "    api_key = 'ollama'\n",
    ")\n",
    "\n",
    "prompt = 'å¤§è¯­è¨€æ¨¡å‹æ˜¯ä»€ä¹ˆï¼Ÿ'\n",
    "messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "response = client.chat.completions.create(\n",
    "    model = 'qwen2:1.5b',\n",
    "    messages = messages,\n",
    "    temperature=0.95\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1/',\n",
    "    api_key = 'ollama'\n",
    ")\n",
    "\n",
    "def gen_llama_params(prompt):\n",
    "    '''\n",
    "    æ„é€ æ¨¡å‹è¯·æ±‚å‚æ•° messages\n",
    "\n",
    "    è¯·æ±‚å‚æ•°ï¼š\n",
    "        prompt: å¯¹åº”çš„ç”¨æˆ·æç¤ºè¯\n",
    "    '''\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def get_completion(prompt, model=\"qwen2:1.5b\", temperature=0.95):\n",
    "    '''\n",
    "    è·å–æ¨¡å‹è°ƒç”¨ç»“æœ\n",
    "\n",
    "    è¯·æ±‚å‚æ•°ï¼š\n",
    "        prompt: å¯¹åº”çš„æç¤ºè¯\n",
    "        model: è°ƒç”¨çš„æ¨¡å‹\"llama3.2:1b\"ï¼Œ\n",
    "        temperature: æ¨¡å‹è¾“å‡ºçš„æ¸©åº¦ç³»æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºç¨‹åº¦ï¼Œå–å€¼èŒƒå›´æ˜¯ 0~1.0ï¼Œä¸”ä¸èƒ½è®¾ç½®ä¸º 0ã€‚æ¸©åº¦ç³»æ•°è¶Šä½ï¼Œè¾“å‡ºå†…å®¹è¶Šä¸€è‡´ã€‚\n",
    "    '''\n",
    "\n",
    "    messages = gen_llama_params(prompt)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    if len(response.choices) > 0:\n",
    "        return response.choices[0].message.content\n",
    "    return \"generate answer error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'å¤§è¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§å¤æ‚ä¸”å¼ºå¤§çš„è‡ªç„¶è¯­è¨€ç”ŸæˆæŠ€æœ¯ï¼Œå¯ä»¥æ¨¡ä»¿æˆ–è¶…è¶Šäººç±»çš„è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›ã€‚é€šä¿—è§£é‡Šå°±æ˜¯ï¼Œè¿™äº›æŠ€æœ¯èƒ½å¤Ÿç†è§£å¹¶æ ¹æ®äººä»¬çš„è¯­å¢ƒï¼ˆæ¯”å¦‚ä¸Šä¸‹æ–‡ã€è¯­æ°”å’Œæ„å›¾ï¼‰è¿›è¡Œæœ‰ç›®çš„çš„è¯­è¨€å¤„ç†ã€‚ä¸¾ä¸ªä¾‹å­çš„è¯ï¼Œåœ¨ç»™å®šä¸€ç¯‡åŒ…å«ç‰¹å®šè¯é¢˜çš„æ–‡ç« ä¸­ï¼Œå¤§è¯­è¨€æ¨¡å‹å¯ä»¥æ ¹æ®æ–‡æœ¬å†…å®¹å¯¹æ–°çš„æ–‡ç« å†…å®¹è¿›è¡Œæ€»ç»“/åˆ†æ/å»ºè®®ï¼Œå¹¶ä¸”å¯ä»¥åŸºäºå¯¹è¯å†å²æˆ–ä¸ªäººçŸ¥è¯†ï¼Œä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–çš„æœåŠ¡æ¨èæˆ–è§£å†³å¤æ‚é—®é¢˜çš„å»ºè®®ã€‚'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"å¤§è¯­è¨€æ¨¡å‹æ˜¯ä»€ä¹ˆï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-rag",
   "language": "python",
   "name": "llm-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
