{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ LLM API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ¬ç« èŠ‚ä¸»è¦ä»‹ç»æ™ºè°± GLMçš„ API ç”³è¯·æŒ‡å¼•å’Œ Python ç‰ˆæœ¬çš„åŸç”Ÿ API è°ƒç”¨æ–¹æ³•ï¼Œè¯»è€…æŒ‰ç…§å®é™…æƒ…å†µé€‰æ‹©ä¸€ç§è‡ªå·±å¯ä»¥ç”³è¯·çš„ API è¿›è¡Œé˜…è¯»å­¦ä¹ å³å¯ã€‚\n",
    "\n",
    "å¦‚æœä½ éœ€è¦åœ¨ LangChain ä¸­ä½¿ç”¨ LLMï¼Œå¯ä»¥å‚ç…§[LLM æ¥å…¥ LangChain](https://github.com/datawhalechina/llm-universe/blob/main/notebook/C4%20%E6%9E%84%E5%BB%BA%20RAG%20%E5%BA%94%E7%94%A8/1.LLM%20%E6%8E%A5%E5%85%A5%20LangChain.ipynb)ä¸­çš„è°ƒç”¨æ–¹å¼ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‚è€ƒä½¿ç”¨ ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPTï¼Œå‘å¸ƒäº 2022 å¹´ 11 æœˆï¼Œæ˜¯ç›®å‰ç«çƒ­å‡ºåœˆçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼ŒLLMï¼‰çš„ä»£è¡¨äº§å“ã€‚åœ¨ 2022 å¹´åº•ï¼Œä¹Ÿæ­£æ˜¯ ChatGPT çš„æƒŠäººè¡¨ç°å¼•å‘äº† LLM çš„çƒ­æ½®ã€‚æ—¶è‡³ç›®å‰ï¼Œç”± OpenAI å‘å¸ƒçš„ GPT-4 ä»ç„¶æ˜¯ LLM æ€§èƒ½ä¸Šé™çš„ä»£è¡¨ï¼ŒChatGPT ä¹Ÿä»ç„¶æ˜¯ç›®å‰ä½¿ç”¨äººæ•°æœ€å¤šã€ä½¿ç”¨çƒ­åº¦æœ€å¤§ã€æœ€å…·å‘å±•æ½œåŠ›çš„ LLM äº§å“ã€‚äº‹å®ä¸Šï¼Œåœ¨åœˆå¤–äººçœ‹æ¥ï¼ŒChatGPT å³æ˜¯ LLM çš„ä»£ç§°ã€‚\n",
    "\n",
    "OpenAI é™¤å‘å¸ƒäº†å…è´¹çš„ Web ç«¯äº§å“å¤–ï¼Œä¹Ÿæä¾›äº†å¤šç§ ChatGPT APIï¼Œæ”¯æŒå¼€å‘è€…é€šè¿‡ Python æˆ– Request è¯·æ±‚æ¥è°ƒç”¨ ChatGPTï¼Œå‘è‡ªå·±çš„æœåŠ¡ä¸­åµŒå…¥ LLM çš„å¼ºå¤§èƒ½åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API ç”³è¯·æŒ‡å¼•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è·å–å¹¶é…ç½® OpenAI API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI API è°ƒç”¨æœåŠ¡æ˜¯ä»˜è´¹çš„ï¼Œæ¯ä¸€ä¸ªå¼€å‘è€…éƒ½éœ€è¦é¦–å…ˆè·å–å¹¶é…ç½® OpenAI API keyï¼Œæ‰èƒ½åœ¨è‡ªå·±æ„å»ºçš„åº”ç”¨ä¸­è®¿é—® ChatGPTã€‚æˆ‘ä»¬å°†åœ¨è¿™éƒ¨åˆ†ç®€è¿°å¦‚ä½•è·å–å¹¶é…ç½® OpenAI API keyã€‚\n",
    "\n",
    "åœ¨è·å– OpenAI API key ä¹‹å‰æˆ‘ä»¬éœ€è¦åœ¨[OpenAI å®˜ç½‘](https://openai.com/)æ³¨å†Œä¸€ä¸ªè´¦å·ã€‚è¿™é‡Œå‡è®¾æˆ‘ä»¬å·²ç»æœ‰äº† OpenAI è´¦å·ï¼Œåœ¨[OpenAI å®˜ç½‘](https://openai.com/)ç™»å½•ï¼Œç™»å½•åå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../../figures/C2-2-openai-choose.png\" width=\"1000\" alt=\"OpenAI å®˜ç½‘ç™»å½•åé€‰æ‹© API\">\n",
    "</p>\n",
    "\n",
    "æˆ‘ä»¬é€‰æ‹© `API`ï¼Œç„¶åç‚¹å‡»å·¦ä¾§è¾¹æ çš„ `API keys`ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../../figures/C2-2-openai-get-key.png\" width=\"1000\" alt=\"OpenAI è·å– API key\">\n",
    "</p>\n",
    "\n",
    "ç‚¹å‡» `Create new secret key` æŒ‰é’®åˆ›å»º OpenAI API key ï¼Œæˆ‘ä»¬å°†åˆ›å»ºå¥½çš„ OpenAI API key å¤åˆ¶ä»¥æ­¤å½¢å¼ `OPENAI_API_KEY=\"sk-...\"` ä¿å­˜åˆ° `.env` æ–‡ä»¶ä¸­ï¼Œå¹¶å°† `.env` æ–‡ä»¶ä¿å­˜åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ã€‚\n",
    "\n",
    "ä¸‹é¢æ˜¯è¯»å– `.env` æ–‡ä»¶çš„ä»£ç ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# è¯»å–æœ¬åœ°/é¡¹ç›®çš„ç¯å¢ƒå˜é‡ã€‚\n",
    "\n",
    "# find_dotenv() å¯»æ‰¾å¹¶å®šä½ .env æ–‡ä»¶çš„è·¯å¾„\n",
    "# load_dotenv() è¯»å–è¯¥ .env æ–‡ä»¶ï¼Œå¹¶å°†å…¶ä¸­çš„ç¯å¢ƒå˜é‡åŠ è½½åˆ°å½“å‰çš„è¿è¡Œç¯å¢ƒä¸­  \n",
    "# å¦‚æœä½ è®¾ç½®çš„æ˜¯å…¨å±€çš„ç¯å¢ƒå˜é‡ï¼Œè¿™è¡Œä»£ç åˆ™æ²¡æœ‰ä»»ä½•ä½œç”¨ã€‚\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# å¦‚æœä½ éœ€è¦é€šè¿‡ä»£ç†ç«¯å£è®¿é—®ï¼Œè¿˜éœ€è¦åšå¦‚ä¸‹é…ç½®\n",
    "# os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "# os.environ[\"HTTP_PROXY\"] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è°ƒç”¨ OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è°ƒç”¨ ChatGPT éœ€è¦ä½¿ç”¨ [ChatCompletion API](https://platform.openai.com/docs/api-reference/chat)ï¼Œè¯¥ API æä¾›äº† ChatGPT ç³»åˆ—æ¨¡å‹çš„è°ƒç”¨ï¼ŒåŒ…æ‹¬ ChatGPT-3.5ï¼ŒGPT-4 ç­‰ã€‚\n",
    "\n",
    "ChatCompletion API è°ƒç”¨æ–¹æ³•å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    # api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    api_key='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'\n",
    ")\n",
    "\n",
    "\n",
    "# å¯¼å…¥æ‰€éœ€åº“\n",
    "# æ³¨æ„ï¼Œæ­¤å¤„æˆ‘ä»¬å‡è®¾ä½ å·²æ ¹æ®ä¸Šæ–‡é…ç½®äº† OpenAI API Keyï¼Œå¦‚æ²¡æœ‰å°†è®¿é—®å¤±è´¥\n",
    "completion = client.chat.completions.create(\n",
    "    # è°ƒç”¨æ¨¡å‹ï¼šChatGPT-3.5\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    # messages æ˜¯å¯¹è¯åˆ—è¡¨\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è°ƒç”¨è¯¥ API ä¼šè¿”å›ä¸€ä¸ª ChatCompletion å¯¹è±¡ï¼Œå…¶ä¸­åŒ…æ‹¬äº†å›ç­”æ–‡æœ¬ã€åˆ›å»ºæ—¶é—´ã€id ç­‰å±æ€§ã€‚æˆ‘ä»¬ä¸€èˆ¬éœ€è¦çš„æ˜¯å›ç­”æ–‡æœ¬ï¼Œä¹Ÿå°±æ˜¯å›ç­”å¯¹è±¡ä¸­çš„ content ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­¤å¤„æˆ‘ä»¬è¯¦ç»†ä»‹ç»è°ƒç”¨ API å¸¸ä¼šç”¨åˆ°çš„å‡ ä¸ªå‚æ•°ï¼š\n",
    "\n",
    "    Â· modelï¼Œå³è°ƒç”¨çš„æ¨¡å‹ï¼Œä¸€èˆ¬å–å€¼åŒ…æ‹¬â€œgpt-3.5-turboâ€ï¼ˆChatGPT-3.5ï¼‰ã€â€œgpt-3.5-turbo-16k-0613â€ï¼ˆChatGPT-3.5 16K ç‰ˆæœ¬ï¼‰ã€â€œgpt-4â€ï¼ˆChatGPT-4ï¼‰ã€‚æ³¨æ„ï¼Œä¸åŒæ¨¡å‹çš„æˆæœ¬æ˜¯ä¸ä¸€æ ·çš„ã€‚\n",
    "\n",
    "    Â· messagesï¼Œå³æˆ‘ä»¬çš„ promptã€‚ChatCompletion çš„ messages éœ€è¦ä¼ å…¥ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­åŒ…æ‹¬å¤šä¸ªä¸åŒè§’è‰²çš„ promptã€‚æˆ‘ä»¬å¯ä»¥é€‰æ‹©çš„è§’è‰²ä¸€èˆ¬åŒ…æ‹¬ systemï¼šå³å‰æ–‡ä¸­æåˆ°çš„ system promptï¼›userï¼šç”¨æˆ·è¾“å…¥çš„ promptï¼›assistantï¼šåŠ©æ‰‹ï¼Œä¸€èˆ¬æ˜¯æ¨¡å‹å†å²å›å¤ï¼Œä½œä¸ºæä¾›ç»™æ¨¡å‹çš„å‚è€ƒå†…å®¹ã€‚\n",
    "\n",
    "    Â· temperatureï¼Œæ¸©åº¦ã€‚å³å‰æ–‡ä¸­æåˆ°çš„ Temperature ç³»æ•°ã€‚\n",
    "\n",
    "    Â· max_tokensï¼Œæœ€å¤§ token æ•°ï¼Œå³æ¨¡å‹è¾“å‡ºçš„æœ€å¤§ token æ•°ã€‚OpenAI è®¡ç®— token æ•°æ˜¯åˆå¹¶è®¡ç®— Prompt å’Œ Completion çš„æ€» token æ•°ï¼Œè¦æ±‚æ€» token æ•°ä¸èƒ½è¶…è¿‡æ¨¡å‹ä¸Šé™ï¼ˆå¦‚é»˜è®¤æ¨¡å‹ token ä¸Šé™ä¸º 4096ï¼‰ã€‚å› æ­¤ï¼Œå¦‚æœè¾“å…¥çš„ prompt è¾ƒé•¿ï¼Œéœ€è¦è®¾ç½®è¾ƒå¤§çš„ max_token å€¼ï¼Œå¦åˆ™ä¼šæŠ¥é”™è¶…å‡ºé™åˆ¶é•¿åº¦ã€‚\n",
    "\n",
    "OpenAI æä¾›äº†å……åˆ†çš„è‡ªå®šä¹‰ç©ºé—´ï¼Œæ”¯æŒæˆ‘ä»¬é€šè¿‡è‡ªå®šä¹‰ prompt æ¥æå‡æ¨¡å‹å›ç­”æ•ˆæœï¼Œå¦‚ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„å°è£… OpenAI æ¥å£çš„å‡½æ•°ï¼Œæ”¯æŒæˆ‘ä»¬ç›´æ¥ä¼ å…¥ prompt å¹¶è·å¾—æ¨¡å‹çš„è¾“å‡ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    # api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    api_key='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'\n",
    ")\n",
    "\n",
    "\n",
    "def gen_gpt_messages(prompt):\n",
    "    '''\n",
    "    æ„é€  GPT æ¨¡å‹è¯·æ±‚å‚æ•° messages\n",
    "    \n",
    "    è¯·æ±‚å‚æ•°ï¼š\n",
    "        prompt: å¯¹åº”çš„ç”¨æˆ·æç¤ºè¯\n",
    "    '''\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature = 0):\n",
    "    '''\n",
    "    è·å– GPT æ¨¡å‹è°ƒç”¨ç»“æœ\n",
    "\n",
    "    è¯·æ±‚å‚æ•°ï¼š\n",
    "        prompt: å¯¹åº”çš„æç¤ºè¯\n",
    "        model: è°ƒç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ä¸º gpt-3.5-turboï¼Œä¹Ÿå¯ä»¥æŒ‰éœ€é€‰æ‹© gpt-4 ç­‰å…¶ä»–æ¨¡å‹\n",
    "        temperature: æ¨¡å‹è¾“å‡ºçš„æ¸©åº¦ç³»æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºç¨‹åº¦ï¼Œå–å€¼èŒƒå›´æ˜¯ 0~2ã€‚æ¸©åº¦ç³»æ•°è¶Šä½ï¼Œè¾“å‡ºå†…å®¹è¶Šä¸€è‡´ã€‚\n",
    "    '''\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=gen_gpt_messages(prompt),\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    if len(response.choices) > 0:\n",
    "        return response.choices[0].message.content\n",
    "    return \"generate answer error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_completion(\"ä½ å¥½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨ä¸Šè¿°å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬å°è£…äº† messages çš„ç»†èŠ‚ï¼Œä»…ä½¿ç”¨ user prompt æ¥å®ç°è°ƒç”¨ã€‚åœ¨ç®€å•åœºæ™¯ä¸­ï¼Œè¯¥å‡½æ•°è¶³å¤Ÿæ»¡è¶³ä½¿ç”¨éœ€æ±‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨æ™ºè°± GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ™ºè°± AI æ˜¯ç”±æ¸…åå¤§å­¦è®¡ç®—æœºç³»æŠ€æœ¯æˆæœè½¬åŒ–è€Œæ¥çš„å…¬å¸ï¼Œè‡´åŠ›äºæ‰“é€ æ–°ä¸€ä»£è®¤çŸ¥æ™ºèƒ½é€šç”¨æ¨¡å‹ã€‚å…¬å¸åˆä½œç ”å‘äº†åŒè¯­åƒäº¿çº§è¶…å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ GLM-130Bï¼Œå¹¶æ„å»ºäº†é«˜ç²¾åº¦é€šç”¨çŸ¥è¯†å›¾è°±ï¼Œå½¢æˆæ•°æ®ä¸çŸ¥è¯†åŒè½®é©±åŠ¨çš„è®¤çŸ¥å¼•æ“ï¼ŒåŸºäºæ­¤æ¨¡å‹æ‰“é€ äº† ChatGLMï¼ˆchatglm.cnï¼‰ã€‚\n",
    "\n",
    "ChatGLM ç³»åˆ—æ¨¡å‹ï¼ŒåŒ…æ‹¬ ChatGLM-130Bã€ChatGLM-6B å’Œ ChatGLM2-6Bï¼ˆChatGLM-6B çš„å‡çº§ç‰ˆæœ¬ï¼‰æ¨¡å‹ï¼Œæ”¯æŒç›¸å¯¹å¤æ‚çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶ä¸”èƒ½å¤Ÿè§£å†³å›°éš¾çš„æ¨ç†ç±»é—®é¢˜ã€‚å…¶ä¸­ï¼ŒChatGLM-6B æ¨¡å‹æ¥è‡ª Huggingface ä¸Šçš„ä¸‹è½½é‡å·²ç»è¶…è¿‡ 300wï¼ˆæˆªè‡³ 2023 å¹´ 6 æœˆ 24 æ—¥ç»Ÿè®¡æ•°æ®ï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨ Hugging Face (HF) å…¨çƒå¤§æ¨¡å‹ä¸‹è½½æ¦œä¸­è¿ç»­ 12 å¤©ä½å±…ç¬¬ä¸€åï¼Œåœ¨å›½å†…å¤–çš„å¼€æºç¤¾åŒºä¸­äº§ç”Ÿäº†è¾ƒå¤§çš„å½±å“ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API ç”³è¯·æŒ‡å¼•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é¦–å…ˆè¿›å…¥åˆ° [æ™ºè°±AIå¼€æ”¾å¹³å°](https://open.bigmodel.cn/overview)ï¼Œç‚¹å‡»`å¼€å§‹ä½¿ç”¨`æˆ–è€…`å¼€å‘å·¥ä½œå°`è¿›è¡Œæ³¨å†Œï¼š\n",
    "\n",
    "![](../../figures/C2-2-zhipuai_home.png)\n",
    "\n",
    "æ–°æ³¨å†Œçš„ç”¨æˆ·å¯ä»¥å…è´¹é¢†å–æœ‰æ•ˆæœŸ 1 ä¸ªæœˆçš„ 100w token çš„ä½“éªŒåŒ…ï¼Œè¿›è¡Œä¸ªäººå®åè®¤è¯åï¼Œè¿˜å¯ä»¥é¢å¤–é¢†å– 400w token ä½“éªŒåŒ…ã€‚æ™ºè°± AI æä¾›äº† GLM-4 å’Œ GLM-3-Turbo è¿™ä¸¤ç§ä¸åŒæ¨¡å‹çš„ä½“éªŒå…¥å£ï¼Œå¯ä»¥ç‚¹å‡»`ç«‹å³ä½“éªŒ`æŒ‰é’®ç›´æ¥ä½“éªŒã€‚\n",
    "\n",
    "![æ™ºè°± AI æ§åˆ¶å°](../../figures/C2-2-zhipuai_overview.png)\n",
    "\n",
    "å¯¹äºéœ€è¦ä½¿ç”¨ API key æ¥æ­å»ºåº”ç”¨çš„è¯ï¼Œéœ€è¦ç‚¹å‡»å³ä¾§çš„`æŸ¥çœ‹ API key`æŒ‰é’®ï¼Œå°±ä¼šè¿›å…¥åˆ°æˆ‘ä»¬ä¸ªäººçš„ API ç®¡ç†åˆ—è¡¨ä¸­ã€‚åœ¨è¯¥ç•Œé¢ï¼Œå°±å¯ä»¥çœ‹åˆ°æˆ‘ä»¬è·å–åˆ°çš„ API æ‰€å¯¹åº”çš„åº”ç”¨åå­—å’Œ `API key` äº†ã€‚\n",
    "\n",
    "![æ™ºè°± AI api ç®¡ç†](../../figures/C2-2-zhipuai_api.png)\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥ç‚¹å‡» `æ·»åŠ æ–°çš„ API key` å¹¶è¾“å…¥å¯¹åº”çš„åå­—å³å¯ç”Ÿæˆæ–°çš„ API keyã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è°ƒç”¨æ™ºè°± GLM API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ™ºè°± AI æä¾›äº† SDK å’ŒåŸç”Ÿ HTTP æ¥å®ç°æ¨¡å‹ API çš„è°ƒç”¨ï¼Œå»ºè®®ä½¿ç”¨ SDK è¿›è¡Œè°ƒç”¨ä»¥è·å¾—æ›´å¥½çš„ç¼–ç¨‹ä½“éªŒã€‚\n",
    "\n",
    "é¦–å…ˆæˆ‘ä»¬éœ€è¦é…ç½®å¯†é’¥ä¿¡æ¯ï¼Œå°†å‰é¢è·å–åˆ°çš„ `API key` è®¾ç½®åˆ° `.env` æ–‡ä»¶ä¸­çš„ `ZHIPUAI_API_KEY` å‚æ•°ï¼Œç„¶åè¿è¡Œä»¥ä¸‹ä»£ç åŠ è½½é…ç½®ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# è¯»å–æœ¬åœ°/é¡¹ç›®çš„ç¯å¢ƒå˜é‡ã€‚\n",
    "\n",
    "# find_dotenv() å¯»æ‰¾å¹¶å®šä½ .env æ–‡ä»¶çš„è·¯å¾„\n",
    "# load_dotenv() è¯»å–è¯¥ .env æ–‡ä»¶ï¼Œå¹¶å°†å…¶ä¸­çš„ç¯å¢ƒå˜é‡åŠ è½½åˆ°å½“å‰çš„è¿è¡Œç¯å¢ƒä¸­  \n",
    "# å¦‚æœä½ è®¾ç½®çš„æ˜¯å…¨å±€çš„ç¯å¢ƒå˜é‡ï¼Œè¿™è¡Œä»£ç åˆ™æ²¡æœ‰ä»»ä½•ä½œç”¨ã€‚\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\llm-rag\\\\.env'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ™ºè°±çš„è°ƒç”¨ä¼ å‚å’Œå…¶ä»–ç±»ä¼¼ï¼Œä¹Ÿéœ€è¦ä¼ å…¥ä¸€ä¸ª messages åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­åŒ…æ‹¬ role å’Œ promptã€‚æˆ‘ä»¬å°è£…å¦‚ä¸‹çš„ `get_completion` å‡½æ•°ï¼Œä¾›åç»­ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai import ZhipuAI\n",
    "\n",
    "client = ZhipuAI(\n",
    "    api_key=os.environ[\"ZHIPUAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "def gen_glm_params(prompt):\n",
    "    '''\n",
    "    æ„é€  GLM æ¨¡å‹è¯·æ±‚å‚æ•° messages\n",
    "\n",
    "    è¯·æ±‚å‚æ•°ï¼š\n",
    "        prompt: å¯¹åº”çš„ç”¨æˆ·æç¤ºè¯\n",
    "    '''\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def get_completion(prompt, model=\"glm-4-plus\", temperature=0.95):\n",
    "    '''\n",
    "    è·å– GLM æ¨¡å‹è°ƒç”¨ç»“æœ\n",
    "\n",
    "    è¯·æ±‚å‚æ•°ï¼š\n",
    "        prompt: å¯¹åº”çš„æç¤ºè¯\n",
    "        model: è°ƒç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ä¸º glm-4ï¼Œä¹Ÿå¯ä»¥æŒ‰éœ€é€‰æ‹© glm-3-turbo ç­‰å…¶ä»–æ¨¡å‹\n",
    "        temperature: æ¨¡å‹è¾“å‡ºçš„æ¸©åº¦ç³»æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºç¨‹åº¦ï¼Œå–å€¼èŒƒå›´æ˜¯ 0~1.0ï¼Œä¸”ä¸èƒ½è®¾ç½®ä¸º 0ã€‚æ¸©åº¦ç³»æ•°è¶Šä½ï¼Œè¾“å‡ºå†…å®¹è¶Šä¸€è‡´ã€‚\n",
    "    '''\n",
    "\n",
    "    messages = gen_glm_params(prompt)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    if len(response.choices) > 0:\n",
    "        return response.choices[0].message.content\n",
    "    return \"generate answer error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹æ™ºè°±æ¸…è¨€ï¼ˆChatGLMï¼‰ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"ä½ å¥½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œå¯¹ä¼ å…¥ zhipuai çš„å‚æ•°è¿›è¡Œç®€å•ä»‹ç»ï¼š\n",
    "\n",
    "- `messages (list)`ï¼Œè°ƒç”¨å¯¹è¯æ¨¡å‹æ—¶ï¼Œå°†å½“å‰å¯¹è¯ä¿¡æ¯åˆ—è¡¨ä½œä¸ºæç¤ºè¾“å…¥ç»™æ¨¡å‹ï¼›æŒ‰ç…§ {\"role\": \"user\", \"content\": \"ä½ å¥½\"} çš„é”®å€¼å¯¹å½¢å¼è¿›è¡Œä¼ å‚ï¼›æ€»é•¿åº¦è¶…è¿‡æ¨¡å‹æœ€é•¿è¾“å…¥é™åˆ¶åä¼šè‡ªåŠ¨æˆªæ–­ï¼Œéœ€æŒ‰æ—¶é—´ç”±æ—§åˆ°æ–°æ’åº\n",
    "\n",
    "- `temperature (float)`ï¼Œé‡‡æ ·æ¸©åº¦ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼Œå¿…é¡»ä¸ºæ­£æ•°å–å€¼èŒƒå›´æ˜¯ï¼š(0.0, 1.0)ï¼Œä¸èƒ½ç­‰äº 0ï¼Œé»˜è®¤å€¼ä¸º 0.95ã€‚å€¼è¶Šå¤§ï¼Œä¼šä½¿è¾“å‡ºæ›´éšæœºï¼Œæ›´å…·åˆ›é€ æ€§ï¼›å€¼è¶Šå°ï¼Œè¾“å‡ºä¼šæ›´åŠ ç¨³å®šæˆ–ç¡®å®š\n",
    "  \n",
    "- `top_p (float)`ï¼Œç”¨æ¸©åº¦å–æ ·çš„å¦ä¸€ç§æ–¹æ³•ï¼Œç§°ä¸ºæ ¸å–æ ·ã€‚å–å€¼èŒƒå›´æ˜¯ï¼š(0.0, 1.0) å¼€åŒºé—´ï¼Œä¸èƒ½ç­‰äº 0 æˆ– 1ï¼Œé»˜è®¤å€¼ä¸º 0.7ã€‚æ¨¡å‹è€ƒè™‘å…·æœ‰ top_p æ¦‚ç‡è´¨é‡ tokens çš„ç»“æœã€‚ä¾‹å¦‚ï¼š0.1 æ„å‘³ç€æ¨¡å‹è§£ç å™¨åªè€ƒè™‘ä»å‰ 10% çš„æ¦‚ç‡çš„å€™é€‰é›†ä¸­å– tokens\n",
    "\n",
    "- `request_id (string)`ï¼Œç”±ç”¨æˆ·ç«¯ä¼ å‚ï¼Œéœ€ä¿è¯å”¯ä¸€æ€§ï¼›ç”¨äºåŒºåˆ†æ¯æ¬¡è¯·æ±‚çš„å”¯ä¸€æ ‡è¯†ï¼Œç”¨æˆ·ç«¯ä¸ä¼ æ—¶å¹³å°ä¼šé»˜è®¤ç”Ÿæˆ\n",
    "\n",
    "- **å»ºè®®æ‚¨æ ¹æ®åº”ç”¨åœºæ™¯è°ƒæ•´ top_p æˆ– temperature å‚æ•°ï¼Œä½†ä¸è¦åŒæ—¶è°ƒæ•´ä¸¤ä¸ªå‚æ•°**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å…¶ä»–æ¨èçš„APIï¼š**  \n",
    "ChatGPT  \n",
    "æ–‡å¿ƒä¸€è¨€  \n",
    "ä½¿ç”¨è®¯é£æ˜Ÿç«  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ æœ¬åœ°å¼€æºLLM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è°ƒç”¨æœ¬åœ°å¤§æ¨¡å‹ï¼ŒåŒæ ·å¯ä»¥ä½¿ç”¨ [ChatCompletion API](https://platform.openai.com/docs/api-reference/chat)ï¼Œè¯¥ API æä¾›äº† ChatGPT ç³»åˆ—æ¨¡å‹çš„è°ƒç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 502",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124må¤§è¯­è¨€æ¨¡å‹æ˜¯ä»€ä¹ˆï¼Ÿ\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:prompt}]\n\u001b[1;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mqwen2:1.5b\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mE:\\ProgramData\\Python312\\venv\\pytorch\\Lib\\site-packages\\openai\\_utils\\_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\ProgramData\\Python312\\venv\\pytorch\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\ProgramData\\Python312\\venv\\pytorch\\Lib\\site-packages\\openai\\_base_client.py:1278\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1266\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1273\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1275\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1276\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1277\u001b[0m     )\n\u001b[1;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mE:\\ProgramData\\Python312\\venv\\pytorch\\Lib\\site-packages\\openai\\_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\ProgramData\\Python312\\venv\\pytorch\\Lib\\site-packages\\openai\\_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1043\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mE:\\ProgramData\\Python312\\venv\\pytorch\\Lib\\site-packages\\openai\\_base_client.py:1093\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1093\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\ProgramData\\Python312\\venv\\pytorch\\Lib\\site-packages\\openai\\_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1043\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mE:\\ProgramData\\Python312\\venv\\pytorch\\Lib\\site-packages\\openai\\_base_client.py:1093\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1093\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\ProgramData\\Python312\\venv\\pytorch\\Lib\\site-packages\\openai\\_base_client.py:1059\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1056\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1058\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1059\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1062\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1063\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1067\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1068\u001b[0m )\n",
      "\u001b[1;31mInternalServerError\u001b[0m: Error code: 502"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url = 'http://192.168.0.211:11434/v1/',\n",
    "    api_key = 'ollama'\n",
    ")\n",
    "\n",
    "prompt = 'å¤§è¯­è¨€æ¨¡å‹æ˜¯ä»€ä¹ˆï¼Ÿ'\n",
    "messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "response = client.chat.completions.create(\n",
    "    model = 'qwen2:1.5b',\n",
    "    messages = messages,\n",
    "    temperature=0.95\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
