|           |           | ollama                                                       | vllm                                                         | cinference                                                   |
| --------- | --------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| LLM       | openai    |                                                              | from openai import OpenAI<br/>client = OpenAI(<br/>    base_url="http://localhost:8081/v1",<br/>    api_key="token-abc123",<br/>)<br/><br/><br/>completion = client.chat.completions.create(<br/>  model="Qwen2.5-14B-Instruct",<br/>  messages=[<br/>    {"role": "user", "content": "Hello!"}<br/>  ]<br/>)<br/><br/><br/>print(completion.choices[0].message) | import openai<br/><br/># Assume that the model is already launched.<br/># The api_key can't be empty, any string is OK.<br/>client = openai.Client(api_key="not empty", base_url="http://localhost:9997/v1")<br/>response = client.embeddings.create(model="acustom-BAAI-bge-m3", input=["What is the capital of China?"])<br/>print(response) |
|           | langchain | <br />from langchain_ollama import OllamaLLM<br/><br/>llm_deepseek = OllamaLLM(base_url='http://localhost:11434', model='deepseek-r1:1.5b', temperature=0.1)<br /><br />from langchain_community.llms import Ollama<br/>llm_deepseek = Ollama(base_url='http://localhost:11434', model='deepseek-r1:1.5b'，temperature=0.1) | from langchain_community.llms import VLLMOpenAI  # 注意类名为 VLLMOpenAI[3](@ref)<br/>llm = VLLMOpenAI(<br/>    openai_api_key="token-abc123",          # vLLM 无需鉴权，设为空字符串[3](@ref)<br/>    openai_api_base="http://129.201.70.31:8081/v1",  # 服务端地址<br/>    model_name="deepseek-r1-distill-qwen-7b",  # 需与部署的模型路径一致<br/>    max_tokens=1024,                # 控制生成文本最大长度<br/>    temperature=0.45,               # 生成多样性参数（0~1）<br/>    top_p=0.9,                      # 采样阈值<br/>    streaming=True                  # 支持流式输出（可选）<br/>)<br/>response = llm.invoke("你是谁？")<br/>print(response) | from langchain_community.embeddings import XinferenceEmbeddings<br/><br/>xinference = XinferenceEmbeddings(<br/>    server_url="http://0.0.0.0:9997",<br/>    model_uid = "acustom-BAAI-bge-m3" # replace model_uid with the model UID return from launching the model<br/>)<br/><br/>r1 = xinference.embed_documents(<br/>    [<br/>        "Alpha is the first letter of Greek alphabet",<br/>        "Beta is the second letter of Greek alphabet",<br/>    ]<br/>)<br/>r2 = xinference.embed_query(<br/>    "What is the second letter of Greek alphabet"<br/>)<br/><br/>print(r1) |
| embedding | openai    |                                                              |                                                              | import openai<br/><br/># Assume that the model is already launched.<br/># The api_key can't be empty, any string is OK.<br/>client = openai.Client(api_key="not empty", base_url="http://localhost:9997/v1")<br/>response = client.embeddings.create(model="acustom-BAAI-bge-m3", input=["What is the capital of China?"])<br/>print(response) |
|           | langchain | from langchain_community.embeddings import OllamaEmbeddings<br/>ollama_emb = OllamaEmbeddings(<br/>    base_url='http://localhost:11434',<br/>    model="nomic-embed-text:latest"<br/>)<br/>r1 = ollama_emb.embed_documents(<br/>    [<br/>        "Alpha is the first letter of Greek alphabet",<br/>        "Beta is the second letter of Greek alphabet",<br/>    ]<br/>)<br/>r2 = ollama_emb.embed_query(<br/>    "What is the second letter of Greek alphabet"<br/>)<br/><br/># 打印结果<br/>print("Document embeddings:")<br/>for i, embedding in enumerate(r1):<br/>    print(f"Document {i+1} embedding: {embedding[:10]}")<br/><br/>print("\nQuery embedding:")<br/>print(f"Query embedding: {r2[:10]}") | from langchain_community.embeddings import XinferenceEmbeddings<br/><br/>xinference = XinferenceEmbeddings(<br/>    server_url="http://0.0.0.0:9997",<br/>    model_uid = "acustom-BAAI-bge-m3" # replace model_uid with the model UID return from launching the model<br/>)<br/><br/>r1 = xinference.embed_documents(<br/>    [<br/>        "Alpha is the first letter of Greek alphabet",<br/>        "Beta is the second letter of Greek alphabet",<br/>    ]<br/>)<br/>r2 = xinference.embed_query(<br/>    "What is the second letter of Greek alphabet"<br/>)<br/><br/>print(r1) | from langchain_community.embeddings import XinferenceEmbeddings<br/><br/>xinference = XinferenceEmbeddings(<br/>    server_url="http://0.0.0.0:9997",<br/>    model_uid = "acustom-BAAI-bge-m3" # replace model_uid with the model UID return from launching the model<br/>)<br/><br/>r1 = xinference.embed_documents(<br/>    [<br/>        "Alpha is the first letter of Greek alphabet",<br/>        "Beta is the second letter of Greek alphabet",<br/>    ]<br/>)<br/>r2 = xinference.embed_query(<br/>    "What is the second letter of Greek alphabet"<br/>)<br/><br/>print(r1) |
|           |           |                                                              |                                                              |                                                              |
|           |           |                                                              |                                                              |                                                              |

