from langchain_chroma import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain_ollama import OllamaLLM
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import RetrievalQA
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CohereRerank
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
import cohere
from langchain_deepseek import ChatDeepSeek
import os
from dotenv import load_dotenv, find_dotenv


# åˆå§‹åŒ–å‡½æ•° (ä¿æŒä½ çš„åŸå§‹ä»£ç ä¸å˜)
def get_vectordb():
    emb_bgem3 = OllamaEmbeddings(base_url='http://localhost:11434', model="bge-m3:latest")
    persist_directory = '../chroma-vmax'
    vectordb = Chroma(
        persist_directory=persist_directory,
        collection_name="vmax-s",
        embedding_function=emb_bgem3
    )
    return vectordb


def get_llm():
    return OllamaLLM(base_url='http://localhost:11434', model='deepseek-r1:1.5b', temperature=0.1)


def generate_response(input_text):
    llm = get_llm()
    output = llm.invoke(input_text)
    output_parser = StrOutputParser()
    return output_parser.invoke(output)


def get_qa_chain_without_memory(question: str):
    vectordb = get_vectordb()
    myllm = get_llm()

    cohere_client = cohere.Client(api_key="Tahx1eySFbKvu9sTyTXrRLf59la3ZUG9vy02stRZ")
    compressor = CohereRerank(
        client=cohere_client,
        top_n=5,
        model="rerank-multilingual-v3.0"
    )

    base_retriever = vectordb.as_retriever(
        search_kwargs={"k": 15},
        search_type="mmr",
    )

    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=base_retriever
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=myllm,
        retriever=compression_retriever,
        return_source_documents=True,
        chain_type_kwargs={
            "prompt": PromptTemplate(
                input_variables=["context", "question"],
                template="""ä½ æ˜¯DeepSeek VMAX-SçŸ¥è¯†åŠ©æ‰‹ã€‚ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”
                æ¡ˆã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚
                {context}
                é—®é¢˜: {question}
                """
            ),
        }
    )

    result = qa_chain({"query": question})
    return result["result"]


def get_qa_chain_with_memory(question: str):
    vectordb = get_vectordb()
    myllm = get_llm()

    cohere_client = cohere.Client(api_key="Tahx1eySFbKvu9sTyTXrRLf59la3ZUG9vy02stRZ")
    compressor = CohereRerank(
        client=cohere_client,
        top_n=5,
        model="rerank-multilingual-v3.0"
    )

    base_retriever = vectordb.as_retriever(
        search_kwargs={"k": 15},
        search_type="mmr",
    )

    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=base_retriever
    )

    # QA_CHAIN_PROMPT = PromptTemplate(
    #     input_variables=["chat_history", "question", "context"],
    #     template="""
    #     ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®å¯¹è¯å†å²å’Œæä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

    #     å†å²å¯¹è¯ï¼š
    #     {chat_history}

    #     ä¸Šä¸‹æ–‡ï¼š
    #     {context}

    #     é—®é¢˜ï¼š{question}

    #     å›ç­”ï¼š
    #     """
    # )

    QA_CHAIN_PROMPT = PromptTemplate(
        input_variables=["context", "question"],
        template="""ä½ æ˜¯DeepSeek VMAX-SçŸ¥è¯†åŠ©æ‰‹ã€‚ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚
                {context}
                é—®é¢˜: {question}
        """
    )

    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=get_llm(),
        retriever=compression_retriever,
        memory=memory,
        output_key="answer",
        combine_docs_chain_kwargs={
            "prompt": QA_CHAIN_PROMPT
        },
        verbose=True,
    )

    result = qa_chain({"question": question})  # Changed from "query" to "question"
    return result

# print(get_qa_chain_without_memory("ä»‹ç»ä¸‹VMAXçš„ä¸Šç½‘æ—¥å¿—ä¸šåŠ¡"))




# Initialize memory outside the function so it persists across questions
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)


def get_vectordb():
    emb_bgem3 = OllamaEmbeddings(base_url='http://localhost:11434', model="bge-m3:latest")
    persist_directory = '../chroma-vmax'
    vectordb = Chroma(
        persist_directory=persist_directory,
        collection_name="vmax-s",
        embedding_function=emb_bgem3
    )
    return vectordb


def get_llm():
    return OllamaLLM(base_url='http://localhost:11434', model='deepseek-r1:1.5b', temperature=0.1)


def get_qa_chain_with_memory(question: str):
    vectordb = get_vectordb()
    myllm = get_llm()

    cohere_client = cohere.Client(api_key="Tahx1eySFbKvu9sTyTXrRLf59la3ZUG9vy02stRZ")
    compressor = CohereRerank(
        client=cohere_client,
        top_n=5,
        model="rerank-multilingual-v3.0"
    )

    base_retriever = vectordb.as_retriever(
        search_kwargs={"k": 15},
        search_type="mmr",
    )

    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=base_retriever
    )

    QA_CHAIN_PROMPT = PromptTemplate(
        input_variables=["chat_history", "question", "context"],
        template="""
        ä½ æ˜¯DeepSeek VMAX-SçŸ¥è¯†åŠ©æ‰‹ã€‚ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼

        å†å²å¯¹è¯ï¼š
        {chat_history}

        ä¸Šä¸‹æ–‡ï¼š
        {context}

        é—®é¢˜ï¼š{question}

        å›ç­”ï¼š
        """
    )

    # QA_CHAIN_PROMPT = PromptTemplate(
    #     input_variables=["context","question"],
    #     template="""ä½ æ˜¯DeepSeek VMAX-SçŸ¥è¯†åŠ©æ‰‹ã€‚ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚
    #             {context}
    #             é—®é¢˜: {question}
    #     """
    # )

    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=get_llm(),
        retriever=compression_retriever,
        memory=memory,
        output_key="answer",
        combine_docs_chain_kwargs={
            "prompt": QA_CHAIN_PROMPT
        },
        verbose=True,
    )

    result = qa_chain({"question": question})  # Changed from "query" to "question"
    return result


# questions = [
#     "VMAXä¸Šç½‘æ—¥å¿—ä¸šåŠ¡åŒ…å«å“ªäº›åŠŸèƒ½ï¼Ÿ",  # éœ€è®°å¿†å‰ä¸€è½®çš„"ä¸»è¦å†…å®¹"
#     "æ•´ç†æˆmarkdownæ ¼å¼excelè¡¨æ ¼"  # éœ€åˆå¹¶å¤šè½®ä¿¡æ¯
# ]
#
# for question in questions:
#     result = get_qa_chain_with_memory(question)  # Pass string directly, not dict
#     print(f"é—®é¢˜ï¼š{question}")
#     print(f"å›ç­”ï¼š{result['answer']}")
#     print("å¯¹è¯å†å²ï¼š", memory.load_memory_variables({}))
#     print("\n" + "=" * 50 + "\n")

import streamlit as st

# é¡µé¢é…ç½®
st.set_page_config(page_title="DeepSeek VMAX çŸ¥è¯†åŠ©æ‰‹", page_icon="ğŸ¤–")
st.title("DeepSeek VMAX çŸ¥è¯†åŠ©æ‰‹")
st.markdown("""
    â€‹**â€‹ä¸‰ç§æ¨¡å¼â€‹**â€‹ï¼š
    - ğŸš€ ç›´æ¥ç”Ÿæˆï¼šLLM è‡ªç”±å‘æŒ¥ï¼ˆæ— æ£€ç´¢ï¼‰
    - ğŸ” å•æ¬¡é—®ç­”ï¼šåŸºäºçŸ¥è¯†åº“æ£€ç´¢å›ç­”ï¼ˆæ— è®°å¿†ï¼‰
    - ğŸ’¬ è¿ç»­å¯¹è¯ï¼šä¿ç•™å†å²ä¸Šä¸‹æ–‡çš„æ£€ç´¢é—®ç­”
""")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []
if "mode" not in st.session_state:
    st.session_state.mode = "ç›´æ¥ç”Ÿæˆ"

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("é…ç½®")
    mode = st.radio(
        "æ¨¡å¼é€‰æ‹©",
        ["ç›´æ¥ç”Ÿæˆ", "å•æ¬¡é—®ç­”", "è¿ç»­å¯¹è¯"],
        index=["ç›´æ¥ç”Ÿæˆ", "å•æ¬¡é—®ç­”", "è¿ç»­å¯¹è¯"].index(st.session_state.get("mode", "ç›´æ¥ç”Ÿæˆ"))
    )

    # åŠ¨æ€æ˜¾ç¤ºå‚æ•°
    if mode != "ç›´æ¥ç”Ÿæˆ":
        st.subheader("æ£€ç´¢å‚æ•°")
        search_k = st.slider("æ£€ç´¢æ–‡æ¡£æ•°é‡ (k)", 1, 20, 15)
        rerank_top_n = st.slider("é‡æ’åºä¿ç•™æ•° (top_n)", 1, 10, 5)

    st.subheader("æ¨¡å‹å‚æ•°")
    temperature = st.slider("æ¸©åº¦ (temperature)", 0.0, 1.0, 0.1, 0.05)

    if st.button("æ¸…ç©ºå¯¹è¯å†å²"):
        st.session_state.messages = []
        st.rerun()

# æ¨¡å¼åˆ‡æ¢æ—¶æ¸…ç©ºå†å²ï¼ˆé¿å…ä¸Šä¸‹æ–‡æ··æ·†ï¼‰
if st.session_state.get("mode") != mode:
    st.session_state.messages = []
    st.session_state.mode = mode

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ç”¨æˆ·è¾“å…¥å¤„ç†
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ç”Ÿæˆå›ç­”
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            try:
                if mode == "ç›´æ¥ç”Ÿæˆ":
                    response = generate_response(prompt)
                elif mode == "å•æ¬¡é—®ç­”":
                    response = get_qa_chain_without_memory(prompt)
                else:
                    result = get_qa_chain_with_memory(prompt)
                    response = result.get("answer", "æœªèƒ½ç”Ÿæˆå›ç­”")

                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
            except Exception as e:
                st.error(f"âš ï¸ é”™è¯¯ï¼š{str(e)}")
                st.session_state.messages.append({"role": "assistant", "content": f"å‡ºé”™: {str(e)}"})