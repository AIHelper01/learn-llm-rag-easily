{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e3f209-1b47-41aa-bb33-d0e7b564203c",
   "metadata": {
    "height": 30
   },
   "source": [
    "# éƒ¨ç½²çŸ¥è¯†åº“åŠ©æ‰‹\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896c72f-1aa0-4b93-aea6-45908a6e42a1",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å¯¹çŸ¥è¯†åº“å’ŒLLMå·²ç»æœ‰äº†åŸºæœ¬çš„ç†è§£ï¼Œç°åœ¨æ˜¯å°†å®ƒä»¬å·§å¦™åœ°èåˆå¹¶æ‰“é€ æˆä¸€ä¸ªå¯Œæœ‰è§†è§‰æ•ˆæœçš„ç•Œé¢çš„æ—¶å€™äº†ã€‚è¿™æ ·çš„ç•Œé¢ä¸ä»…å¯¹æ“ä½œæ›´åŠ ä¾¿æ·ï¼Œè¿˜èƒ½ä¾¿äºä¸ä»–äººåˆ†äº«ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c975542-100a-431f-bfdb-2e948fd1e360",
   "metadata": {
    "height": 30
   },
   "source": [
    "Streamlit æ˜¯ä¸€ç§å¿«é€Ÿä¾¿æ·çš„æ–¹æ³•ï¼Œå¯ä»¥ç›´æ¥åœ¨ **Python ä¸­é€šè¿‡å‹å¥½çš„ Web ç•Œé¢æ¼”ç¤ºæœºå™¨å­¦ä¹ æ¨¡å‹**ã€‚åœ¨æœ¬è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ *å¦‚ä½•ä½¿ç”¨å®ƒä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½åº”ç”¨ç¨‹åºæ„å»ºç”¨æˆ·ç•Œé¢*ã€‚åœ¨æ„å»ºäº†æœºå™¨å­¦ä¹ æ¨¡å‹åï¼Œå¦‚æœä½ æƒ³æ„å»ºä¸€ä¸ª demo ç»™å…¶ä»–äººçœ‹ï¼Œä¹Ÿè®¸æ˜¯ä¸ºäº†è·å¾—åé¦ˆå¹¶æ¨åŠ¨ç³»ç»Ÿçš„æ”¹è¿›ï¼Œæˆ–è€…åªæ˜¯å› ä¸ºä½ è§‰å¾—è¿™ä¸ªç³»ç»Ÿå¾ˆé…·ï¼Œæ‰€ä»¥æƒ³æ¼”ç¤ºä¸€ä¸‹ï¼šStreamlit å¯ä»¥è®©æ‚¨é€šè¿‡ Python æ¥å£ç¨‹åºå¿«é€Ÿå®ç°è¿™ä¸€ç›®æ ‡ï¼Œè€Œæ— éœ€ç¼–å†™ä»»ä½•å‰ç«¯ã€ç½‘é¡µæˆ– JavaScript ä»£ç ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa74cbe-96ed-4652-a761-8740615597ed",
   "metadata": {
    "height": 30
   },
   "source": [
    "## ä¸€ã€Streamlit ç®€ä»‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf25020-22a5-435d-925a-26cbe71a5f59",
   "metadata": {},
   "source": [
    "\n",
    "`Streamlit` æ˜¯ä¸€ä¸ªç”¨äºå¿«é€Ÿåˆ›å»ºæ•°æ®åº”ç”¨ç¨‹åºçš„å¼€æº Python åº“ã€‚å®ƒçš„è®¾è®¡ç›®æ ‡æ˜¯è®©æ•°æ®ç§‘å­¦å®¶èƒ½å¤Ÿè½»æ¾åœ°å°†æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ æ¨¡å‹è½¬åŒ–ä¸ºå…·æœ‰äº¤äº’æ€§çš„ Web åº”ç”¨ç¨‹åºï¼Œè€Œæ— éœ€æ·±å…¥äº†è§£ Web å¼€å‘ã€‚å’Œå¸¸è§„ Web æ¡†æ¶ï¼Œå¦‚ Flask/Django çš„ä¸åŒä¹‹å¤„åœ¨äºï¼Œå®ƒä¸éœ€è¦ä½ å»ç¼–å†™ä»»ä½•å®¢æˆ·ç«¯ä»£ç ï¼ˆHTML/CSS/JSï¼‰ï¼Œåªéœ€è¦ç¼–å†™æ™®é€šçš„ Python æ¨¡å—ï¼Œå°±å¯ä»¥åœ¨å¾ˆçŸ­çš„æ—¶é—´å†…åˆ›å»ºç¾è§‚å¹¶å…·å¤‡é«˜åº¦äº¤äº’æ€§çš„ç•Œé¢ï¼Œä»è€Œå¿«é€Ÿç”Ÿæˆæ•°æ®åˆ†ææˆ–è€…æœºå™¨å­¦ä¹ çš„ç»“æœï¼›å¦ä¸€æ–¹é¢ï¼Œå’Œé‚£äº›åªèƒ½é€šè¿‡æ‹–æ‹½ç”Ÿæˆçš„å·¥å…·ä¹Ÿä¸åŒçš„æ˜¯ï¼Œä½ ä»ç„¶å…·æœ‰å¯¹ä»£ç çš„å®Œæ•´æ§åˆ¶æƒã€‚\n",
    "\n",
    "Streamlit æä¾›äº†ä¸€ç»„ç®€å•è€Œå¼ºå¤§çš„åŸºç¡€æ¨¡å—ï¼Œç”¨äºæ„å»ºæ•°æ®åº”ç”¨ç¨‹åºï¼š\n",
    "\n",
    "- st.write()ï¼šè¿™æ˜¯æœ€åŸºæœ¬çš„æ¨¡å—ä¹‹ä¸€ï¼Œç”¨äºåœ¨åº”ç”¨ç¨‹åºä¸­å‘ˆç°æ–‡æœ¬ã€å›¾åƒã€è¡¨æ ¼ç­‰å†…å®¹ã€‚\n",
    "\n",
    "- st.title()ã€st.header()ã€st.subheader()ï¼šè¿™äº›æ¨¡å—ç”¨äºæ·»åŠ æ ‡é¢˜ã€å­æ ‡é¢˜å’Œåˆ†ç»„æ ‡é¢˜ï¼Œä»¥ç»„ç»‡åº”ç”¨ç¨‹åºçš„å¸ƒå±€ã€‚\n",
    "\n",
    "- st.text()ã€st.markdown()ï¼šç”¨äºæ·»åŠ æ–‡æœ¬å†…å®¹ï¼Œæ”¯æŒ Markdown è¯­æ³•ã€‚\n",
    "\n",
    "- st.image()ï¼šç”¨äºæ·»åŠ å›¾åƒåˆ°åº”ç”¨ç¨‹åºä¸­ã€‚\n",
    "\n",
    "- st.dataframe()ï¼šç”¨äºå‘ˆç° Pandas æ•°æ®æ¡†ã€‚\n",
    "\n",
    "- st.table()ï¼šç”¨äºå‘ˆç°ç®€å•çš„æ•°æ®è¡¨æ ¼ã€‚\n",
    "\n",
    "- st.pyplot()ã€st.altair_chart()ã€st.plotly_chart()ï¼šç”¨äºå‘ˆç° Matplotlibã€Altair æˆ– Plotly ç»˜åˆ¶çš„å›¾è¡¨ã€‚\n",
    "\n",
    "- st.selectbox()ã€st.multiselect()ã€st.slider()ã€st.text_input()ï¼šç”¨äºæ·»åŠ äº¤äº’å¼å°éƒ¨ä»¶ï¼Œå…è®¸ç”¨æˆ·åœ¨åº”ç”¨ç¨‹åºä¸­è¿›è¡Œé€‰æ‹©ã€è¾“å…¥æˆ–æ»‘åŠ¨æ“ä½œã€‚\n",
    "\n",
    "- st.button()ã€st.checkbox()ã€st.radio()ï¼šç”¨äºæ·»åŠ æŒ‰é’®ã€å¤é€‰æ¡†å’Œå•é€‰æŒ‰é’®ï¼Œä»¥è§¦å‘ç‰¹å®šçš„æ“ä½œã€‚\n",
    "\n",
    "è¿™äº›åŸºç¡€æ¨¡å—ä½¿å¾—é€šè¿‡ Streamlit èƒ½å¤Ÿè½»æ¾åœ°æ„å»ºäº¤äº’å¼æ•°æ®åº”ç”¨ç¨‹åºï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨æ—¶å¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œç»„åˆå’Œå®šåˆ¶ï¼Œæ›´å¤šå†…å®¹è¯·æŸ¥çœ‹[å®˜æ–¹æ–‡æ¡£](https://docs.streamlit.io/get-started)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3209b0",
   "metadata": {},
   "source": [
    "## äºŒã€æ„å»ºåº”ç”¨ç¨‹åº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6cf145-06f0-40de-98cd-2e501c3377eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "# from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "import sys\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_community.vectorstores import Milvus\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())    # read local .env file\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "\n",
    "def get_llm():\n",
    "    return Ollama(base_url='http://localhost:11434', model='deepseek-r1:14b', temperature=0.1)\n",
    "\n",
    "def get_emd():\n",
    "    return OllamaEmbeddings(base_url='http://localhost:11434', model=\"bge-m3:latest\")\n",
    "\n",
    "# åˆå§‹åŒ– Milvus å‘é‡æ•°æ®åº“\n",
    "def get_vectordb():\n",
    "    my_emb = get_emd()\n",
    "    # Milvus è¿æ¥å‚æ•°\n",
    "    vectordb = Milvus(\n",
    "        embedding_function=my_emb,\n",
    "        collection_name=\"Vmaxs\",  # Milvus é›†åˆåç§°\n",
    "        connection_args={\n",
    "            \"host\": \"192.168.0.188\",  # Milvus æœåŠ¡å™¨åœ°å€\n",
    "            \"port\": \"19530\",  # Milvus é»˜è®¤ç«¯å£\n",
    "        },\n",
    "    )\n",
    "    return vectordb\n",
    "\n",
    "# ä¸å¸¦çŸ¥è¯†åº“çš„å›ç­”\n",
    "def generate_response(input_text):\n",
    "    my_llm = get_llm()\n",
    "    output = my_llm.invoke(input_text)\n",
    "    output_parser = StrOutputParser()\n",
    "    output = output_parser.invoke(output)\n",
    "    return output\n",
    "\n",
    "# åŸºäºçŸ¥è¯†åº“çš„é—®ç­”é“¾\n",
    "def generate_response_with_rag(question:str):\n",
    "    vectordb = get_vectordb()\n",
    "    my_llm = get_llm()\n",
    "    template = \"\"\"ä½ æ˜¯VMAXè¿ç»´åŠ©æ‰‹ï¼Œä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”\n",
    "    æ¡ˆã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚\n",
    "    {context}\n",
    "    é—®é¢˜: {question}\n",
    "    \"\"\"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],\n",
    "                                 template=template)\n",
    "    qa_chain = RetrievalQA.from_chain_type(my_llm,\n",
    "                                       retriever=vectordb.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})\n",
    "    result = qa_chain({\"query\": question})\n",
    "    return result[\"result\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_response_with_rag_memory(question: str):\n",
    "    # åˆå§‹åŒ–å‘é‡æ•°æ®åº“å’ŒLLM\n",
    "    vectordb = get_vectordb()\n",
    "    my_llm = get_llm()\n",
    "\n",
    "    memory = ConversationBufferMemory( memory_key=\"chat_history\",  # ä¸ prompt çš„è¾“å…¥å˜é‡ä¿æŒä¸€è‡´ã€‚\n",
    "    return_messages=True  # å°†ä»¥æ¶ˆæ¯åˆ—è¡¨çš„å½¢å¼è¿”å›èŠå¤©è®°å½•ï¼Œè€Œä¸æ˜¯å•ä¸ªå­—ç¬¦ä¸²\n",
    "    )\n",
    "    \n",
    "    # ä¿®æ”¹åçš„Promptæ¨¡æ¿ï¼ˆæ·»åŠ chat_historyå˜é‡ï¼‰\n",
    "    template = \"\"\"ä½ æ˜¯VMAXè¿ç»´åŠ©æ‰‹ï¼Œè¯·å‚è€ƒä»¥ä¸‹å¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ï¼š\n",
    "    {chat_history}\n",
    "    \n",
    "    ç›¸å…³ä¸Šä¸‹æ–‡ï¼š\n",
    "    {context}\n",
    "    \n",
    "    é—®é¢˜ï¼š{question}\n",
    "    å›ç­”ç»“æŸæ—¶è¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€\n",
    "    \"\"\"\n",
    "    \n",
    "    QA_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"chat_history\", \"context\", \"question\"],\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    # åˆ›å»ºå¯¹è¯é“¾\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=my_llm,\n",
    "        retriever=vectordb.as_retriever(),\n",
    "        memory=memory,\n",
    "        combine_docs_chain_kwargs={\"prompt\": QA_PROMPT},\n",
    "        chain_type=\"stuff\"\n",
    "    )\n",
    "    \n",
    "    result = qa_chain({\"question\": question})\n",
    "    return result[\"answer\"]\n",
    "\n",
    "\n",
    "# Streamlit åº”ç”¨ç¨‹åºç•Œé¢\n",
    "def main():\n",
    "    st.title('ğŸ¦œğŸ”— VMAX-Sè¿ç»´åŠ©æ‰‹Demo')\n",
    "    # zhipuai_api_key = st.sidebar.text_input('GLM API Key', type='password')\n",
    "\n",
    "    # æ·»åŠ ä¸€ä¸ªé€‰æ‹©æŒ‰é’®æ¥é€‰æ‹©ä¸åŒçš„æ¨¡å‹\n",
    "    #selected_method = st.sidebar.selectbox(\"é€‰æ‹©æ¨¡å¼\", [\"qa_chain\", \"chat_qa_chain\", \"None\"])\n",
    "    selected_method = st.radio(\n",
    "        \"ä½ æƒ³é€‰æ‹©å“ªç§æ¨¡å¼è¿›è¡Œå¯¹è¯ï¼Ÿ\",\n",
    "        [\"No-RAG\", \"generate_response_with_rag\", \"generate_response_with_rag_memory\"],\n",
    "        captions = [\"ä¸ä½¿ç”¨åŸºäºçŸ¥è¯†åº“çš„æ£€ç´¢é—®ç­”æ¨¡å¼\", \"åŸºäºçŸ¥è¯†åº“çš„æ£€ç´¢é—®ç­”æ¨¡å¼\", \"åŸºäºçŸ¥è¯†åº“çš„æ£€ç´¢é—®ç­”æ¨¡å¼ï¼ˆå¸¦è®°å¿†ï¼‰\"])\n",
    "\n",
    "    # ç”¨äºè·Ÿè¸ªå¯¹è¯å†å²\n",
    "    if 'messages' not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "\n",
    "    messages = st.container(height=300)\n",
    "    if prompt := st.chat_input(\"Say something\"):\n",
    "        # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"text\": prompt})\n",
    "\n",
    "        if selected_method == \"No-RAG\":\n",
    "            # è°ƒç”¨ respond å‡½æ•°è·å–å›ç­”\n",
    "            answer = generate_response(prompt)\n",
    "        elif selected_method == \"generate_response_with_rag\":\n",
    "            answer = generate_response_with_rag(prompt)\n",
    "        elif selected_method == \"generate_response_with_rag_memory\":\n",
    "            answer = generate_response_with_rag(prompt)\n",
    "\n",
    "        # æ£€æŸ¥å›ç­”æ˜¯å¦ä¸º None\n",
    "        if answer is not None:\n",
    "            # å°†LLMçš„å›ç­”æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­\n",
    "            st.session_state.messages.append({\"role\": \"assistant\", \"text\": answer})\n",
    "\n",
    "        # æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯å†å²\n",
    "        for message in st.session_state.messages:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                messages.chat_message(\"user\").write(message[\"text\"])\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                messages.chat_message(\"assistant\").write(message[\"text\"])   \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
