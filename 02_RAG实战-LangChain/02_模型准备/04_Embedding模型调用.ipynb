{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的部署参考： [learn-llm-deploy-easily](https://gitee.com/coderwillyan/learn-llm-deploy-easily) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里主要介绍如何调用已部署的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用embedding API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以zhipu为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "my_emb = ZhipuAIEmbeddings(\n",
    "    model=\"embedding-2\",\n",
    "    api_key=os.environ[\"ZHIPUAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用本地部署的embedding模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Transformers部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Requires transformers>=4.51.0\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Requires sentence-transformers>=2.7.0\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen3-Embedding-0.6B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "# Requires transformers>=4.51.0\n",
    "# Requires sentence-transformers>=2.7.0\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer(model = \"Qwen/Qwen3-Embedding-0.6B\")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving,\n",
    "# together with setting `padding_side` to \"left\":\n",
    "# model = SentenceTransformer(\n",
    "#     \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "#     model_kwargs={\"attn_implementation\": \"flash_attention_2\", \"device_map\": \"auto\"},\n",
    "#     tokenizer_kwargs={\"padding_side\": \"left\"},\n",
    "# )\n",
    "\n",
    "# The queries and documents to embed\n",
    "queries = [\n",
    "    \"What is the capital of China?\",\n",
    "    \"Explain gravity\",\n",
    "]\n",
    "documents = [\n",
    "    \"The capital of China is Beijing.\",\n",
    "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n",
    "]\n",
    "\n",
    "# Encode the queries and documents. Note that queries benefit from using a prompt\n",
    "# Here we use the prompt called \"query\" stored under `model.prompts`, but you can\n",
    "# also pass your own prompt via the `prompt` argument\n",
    "query_embeddings = model.encode(queries, prompt_name=\"query\")\n",
    "document_embeddings = model.encode(documents)\n",
    "\n",
    "# Compute the (cosine) similarity between the query and document embeddings\n",
    "similarity = model.similarity(query_embeddings, document_embeddings)\n",
    "print(similarity)\n",
    "# tensor([[0.7646, 0.1414],\n",
    "#         [0.1355, 0.6000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用HuggingFaceEmbeddings部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\will\\AppData\\Local\\Temp\\ipykernel_1576\\602453622.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  my_emb = HuggingFaceEmbeddings(model_name='/opt/workspace/models/BAAI/bge-small-zh-v1.5')\n",
      "E:\\ProgramData\\Python312\\venv\\langchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Path /opt/workspace/models/BAAI/bge-small-zh-v1.5 not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhuggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m my_emb = \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/opt/workspace/models/BAAI/bge-small-zh-v1.5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33m如何调用HuggingFaceEmbeddings？\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m query_vector = my_emb.embed_query(query)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\ProgramData\\Python312\\venv\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:214\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    213\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\ProgramData\\Python312\\venv\\langchain\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:92\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import sentence_transformers python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     89\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28mself\u001b[39m.client = \u001b[43msentence_transformers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\ProgramData\\Python312\\venv\\langchain\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:296\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(model_name_or_path):\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# Not a path, load from hub\u001b[39;00m\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mor\u001b[39;00m model_name_or_path.count(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPath \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mand\u001b[39;00m model_name_or_path.lower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m basic_transformer_models:\n\u001b[32m    299\u001b[39m         \u001b[38;5;66;03m# A model from sentence-transformers\u001b[39;00m\n\u001b[32m    300\u001b[39m         model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + model_name_or_path\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Path /opt/workspace/models/BAAI/bge-small-zh-v1.5 not found"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "my_emb = HuggingFaceEmbeddings(model_name='/opt/workspace/models/BAAI/bge-small-zh-v1.5')\n",
    "\n",
    "query = \"如何调用HuggingFaceEmbeddings？\"\n",
    "\n",
    "query_vector = my_emb.embed_query(query) \n",
    "\n",
    "print(\"查询向量:\", query_vector[:5], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用ollama部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "my_emb = OllamaEmbeddings(base_url='http://localhost:11434',model=\"bge-m3:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查询向量: [-0.017827146, -0.009976904, -0.03116692, 0.029771997, -0.024388244] ...\n",
      "文档向量列表: [[-0.029825423, -0.0006194668, -0.031351015, 0.02808682, -0.018649695], [0.002058215, 0.013199092, -0.020036552, 0.024129553, -0.003552613]] ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "# 初始化嵌入模型\n",
    "my_emb = OllamaEmbeddings(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"bge-m3:latest\"\n",
    ")\n",
    "\n",
    "# 示例文本\n",
    "query = \"如何调用OllamaEmbeddings？\"\n",
    "documents = [\n",
    "    \"OllamaEmbeddings 用于生成文本嵌入向量。\",\n",
    "    \"调用方法包括 embed_query 和 embed_documents。\"\n",
    "]\n",
    "\n",
    "# 生成嵌入向量\n",
    "query_vector = my_emb.embed_query(query)\n",
    "doc_vectors = my_emb.embed_documents(documents)\n",
    "\n",
    "# 输出结果\n",
    "print(\"查询向量:\", query_vector[:5], \"...\")\n",
    "print(\"文档向量列表:\", [vec[:5] for vec in doc_vectors], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用xinference部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import XinferenceEmbeddings\n",
    "\n",
    "# 替换为你的Xinference服务器URL和模型UID\n",
    "xinference = XinferenceEmbeddings(\n",
    "    server_url=\"http://localhost:9997\",  # 注意：原代码中的\"loaclhost\"拼写错误，应为\"localhost\"\n",
    "    model_uid=\"your_model_uid\"  # 替换为实际的模型UID\n",
    ")\n",
    "\n",
    "# 输入文本\n",
    "texts = [\"你好，世界。\", \"LangChain 是一个强大的工具。\"]\n",
    "\n",
    "# 生成嵌入向量\n",
    "vectors = xinference.embed_documents(texts)\n",
    "\n",
    "# 打印结果\n",
    "for idx, vector in enumerate(vectors):\n",
    "    print(f\"文本 {idx + 1}: {texts[idx]}\")\n",
    "    print(f\"嵌入向量: {vector[:5]}... (维度: {len(vector)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (langchain-yw)",
   "language": "python",
   "name": "langchain-yw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
