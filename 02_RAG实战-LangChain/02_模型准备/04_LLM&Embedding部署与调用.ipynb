{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨¡å‹çš„éƒ¨ç½²å‚è€ƒï¼š [learn-llm-deploy-easily](https://gitee.com/coderwillyan/learn-llm-deploy-easily) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œä¸»è¦ä»‹ç»å¦‚ä½•è°ƒç”¨å·²éƒ¨ç½²çš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ LLM API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ¬ç« èŠ‚ä¸»è¦ä»‹ç»æ™ºè°± GLMçš„ API ç”³è¯·æŒ‡å¼•å’Œ Python ç‰ˆæœ¬çš„åŸç”Ÿ API è°ƒç”¨æ–¹æ³•ï¼Œè¯»è€…æŒ‰ç…§å®é™…æƒ…å†µé€‰æ‹©ä¸€ç§è‡ªå·±å¯ä»¥ç”³è¯·çš„ API è¿›è¡Œé˜…è¯»å­¦ä¹ å³å¯ã€‚\n",
    "\n",
    "å¦‚æœä½ éœ€è¦åœ¨ LangChain ä¸­ä½¿ç”¨ LLMï¼Œå¯ä»¥å‚ç…§[LLM æ¥å…¥ LangChain]ä¸­çš„è°ƒç”¨æ–¹å¼ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¯»å– `.env` æ–‡ä»¶ä¸­ä¿å­˜çš„API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# è¯»å–æœ¬åœ°/é¡¹ç›®çš„ç¯å¢ƒå˜é‡ã€‚\n",
    "\n",
    "# find_dotenv() å¯»æ‰¾å¹¶å®šä½ .env æ–‡ä»¶çš„è·¯å¾„\n",
    "# load_dotenv() è¯»å–è¯¥ .env æ–‡ä»¶ï¼Œå¹¶å°†å…¶ä¸­çš„ç¯å¢ƒå˜é‡åŠ è½½åˆ°å½“å‰çš„è¿è¡Œç¯å¢ƒä¸­  \n",
    "# å¦‚æœä½ è®¾ç½®çš„æ˜¯å…¨å±€çš„ç¯å¢ƒå˜é‡ï¼Œè¿™è¡Œä»£ç åˆ™æ²¡æœ‰ä»»ä½•ä½œç”¨ã€‚\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\learn-llm-rag-easily\\\\.env'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-b03a5d47f1094751ac79560dcf91ddd0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key=os.environ[\"DEEPSEEK_API_KEY\"]\n",
    "\n",
    "api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å‚è€ƒä½¿ç”¨ ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPTï¼Œå‘å¸ƒäº 2022 å¹´ 11 æœˆï¼Œæ˜¯ç›®å‰ç«çƒ­å‡ºåœˆçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼ŒLLMï¼‰çš„ä»£è¡¨äº§å“ã€‚åœ¨ 2022 å¹´åº•ï¼Œä¹Ÿæ­£æ˜¯ ChatGPT çš„æƒŠäººè¡¨ç°å¼•å‘äº† LLM çš„çƒ­æ½®ã€‚æ—¶è‡³ç›®å‰ï¼Œç”± OpenAI å‘å¸ƒçš„ GPT-4 ä»ç„¶æ˜¯ LLM æ€§èƒ½ä¸Šé™çš„ä»£è¡¨ï¼ŒChatGPT ä¹Ÿä»ç„¶æ˜¯ç›®å‰ä½¿ç”¨äººæ•°æœ€å¤šã€ä½¿ç”¨çƒ­åº¦æœ€å¤§ã€æœ€å…·å‘å±•æ½œåŠ›çš„ LLM äº§å“ã€‚äº‹å®ä¸Šï¼Œåœ¨åœˆå¤–äººçœ‹æ¥ï¼ŒChatGPT å³æ˜¯ LLM çš„ä»£ç§°ã€‚\n",
    "\n",
    "OpenAI é™¤å‘å¸ƒäº†å…è´¹çš„ Web ç«¯äº§å“å¤–ï¼Œä¹Ÿæä¾›äº†å¤šç§ ChatGPT APIï¼Œæ”¯æŒå¼€å‘è€…é€šè¿‡ Python æˆ– Request è¯·æ±‚æ¥è°ƒç”¨ ChatGPTï¼Œå‘è‡ªå·±çš„æœåŠ¡ä¸­åµŒå…¥ LLM çš„å¼ºå¤§èƒ½åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è°ƒç”¨ OpenAI API**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è°ƒç”¨ ChatGPT éœ€è¦ä½¿ç”¨ [ChatCompletion API](https://platform.openai.com/docs/api-reference/chat)ï¼Œè¯¥ API æä¾›äº† ChatGPT ç³»åˆ—æ¨¡å‹çš„è°ƒç”¨ï¼ŒåŒ…æ‹¬ ChatGPT-3.5ï¼ŒGPT-4 ç­‰ã€‚\n",
    "\n",
    "ChatCompletion API è°ƒç”¨æ–¹æ³•å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    # api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    api_key='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'\n",
    ")\n",
    "\n",
    "\n",
    "# å¯¼å…¥æ‰€éœ€åº“\n",
    "# æ³¨æ„ï¼Œæ­¤å¤„æˆ‘ä»¬å‡è®¾ä½ å·²æ ¹æ®ä¸Šæ–‡é…ç½®äº† OpenAI API Keyï¼Œå¦‚æ²¡æœ‰å°†è®¿é—®å¤±è´¥\n",
    "completion = client.chat.completions.create(\n",
    "    # è°ƒç”¨æ¨¡å‹ï¼šChatGPT-3.5\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    # messages æ˜¯å¯¹è¯åˆ—è¡¨\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è°ƒç”¨è¯¥ API ä¼šè¿”å›ä¸€ä¸ª ChatCompletion å¯¹è±¡ï¼Œå…¶ä¸­åŒ…æ‹¬äº†å›ç­”æ–‡æœ¬ã€åˆ›å»ºæ—¶é—´ã€id ç­‰å±æ€§ã€‚æˆ‘ä»¬ä¸€èˆ¬éœ€è¦çš„æ˜¯å›ç­”æ–‡æœ¬ï¼Œä¹Ÿå°±æ˜¯å›ç­”å¯¹è±¡ä¸­çš„ content ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­¤å¤„æˆ‘ä»¬è¯¦ç»†ä»‹ç»è°ƒç”¨ API å¸¸ä¼šç”¨åˆ°çš„å‡ ä¸ªå‚æ•°ï¼š\n",
    "\n",
    "    Â· modelï¼Œå³è°ƒç”¨çš„æ¨¡å‹ï¼Œä¸€èˆ¬å–å€¼åŒ…æ‹¬â€œgpt-3.5-turboâ€ï¼ˆChatGPT-3.5ï¼‰ã€â€œgpt-3.5-turbo-16k-0613â€ï¼ˆChatGPT-3.5 16K ç‰ˆæœ¬ï¼‰ã€â€œgpt-4â€ï¼ˆChatGPT-4ï¼‰ã€‚æ³¨æ„ï¼Œä¸åŒæ¨¡å‹çš„æˆæœ¬æ˜¯ä¸ä¸€æ ·çš„ã€‚\n",
    "\n",
    "    Â· messagesï¼Œå³æˆ‘ä»¬çš„ promptã€‚ChatCompletion çš„ messages éœ€è¦ä¼ å…¥ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­åŒ…æ‹¬å¤šä¸ªä¸åŒè§’è‰²çš„ promptã€‚æˆ‘ä»¬å¯ä»¥é€‰æ‹©çš„è§’è‰²ä¸€èˆ¬åŒ…æ‹¬ systemï¼šå³å‰æ–‡ä¸­æåˆ°çš„ system promptï¼›userï¼šç”¨æˆ·è¾“å…¥çš„ promptï¼›assistantï¼šåŠ©æ‰‹ï¼Œä¸€èˆ¬æ˜¯æ¨¡å‹å†å²å›å¤ï¼Œä½œä¸ºæä¾›ç»™æ¨¡å‹çš„å‚è€ƒå†…å®¹ã€‚\n",
    "\n",
    "    Â· temperatureï¼Œæ¸©åº¦ã€‚å³å‰æ–‡ä¸­æåˆ°çš„ Temperature ç³»æ•°ã€‚\n",
    "\n",
    "    Â· max_tokensï¼Œæœ€å¤§ token æ•°ï¼Œå³æ¨¡å‹è¾“å‡ºçš„æœ€å¤§ token æ•°ã€‚OpenAI è®¡ç®— token æ•°æ˜¯åˆå¹¶è®¡ç®— Prompt å’Œ Completion çš„æ€» token æ•°ï¼Œè¦æ±‚æ€» token æ•°ä¸èƒ½è¶…è¿‡æ¨¡å‹ä¸Šé™ï¼ˆå¦‚é»˜è®¤æ¨¡å‹ token ä¸Šé™ä¸º 4096ï¼‰ã€‚å› æ­¤ï¼Œå¦‚æœè¾“å…¥çš„ prompt è¾ƒé•¿ï¼Œéœ€è¦è®¾ç½®è¾ƒå¤§çš„ max_token å€¼ï¼Œå¦åˆ™ä¼šæŠ¥é”™è¶…å‡ºé™åˆ¶é•¿åº¦ã€‚\n",
    "\n",
    "OpenAI æä¾›äº†å……åˆ†çš„è‡ªå®šä¹‰ç©ºé—´ï¼Œæ”¯æŒæˆ‘ä»¬é€šè¿‡è‡ªå®šä¹‰ prompt æ¥æå‡æ¨¡å‹å›ç­”æ•ˆæœï¼Œå¦‚ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„å°è£… OpenAI æ¥å£çš„å‡½æ•°ï¼Œæ”¯æŒæˆ‘ä»¬ç›´æ¥ä¼ å…¥ prompt å¹¶è·å¾—æ¨¡å‹çš„è¾“å‡ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    # api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    api_key='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'\n",
    ")\n",
    "\n",
    "\n",
    "def gen_gpt_messages(prompt):\n",
    "    '''\n",
    "    æ„é€  GPT æ¨¡å‹è¯·æ±‚å‚æ•° messages\n",
    "    \n",
    "    è¯·æ±‚å‚æ•°ï¼š\n",
    "        prompt: å¯¹åº”çš„ç”¨æˆ·æç¤ºè¯\n",
    "    '''\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature = 0):\n",
    "    '''\n",
    "    è·å– GPT æ¨¡å‹è°ƒç”¨ç»“æœ\n",
    "\n",
    "    è¯·æ±‚å‚æ•°ï¼š\n",
    "        prompt: å¯¹åº”çš„æç¤ºè¯\n",
    "        model: è°ƒç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ä¸º gpt-3.5-turboï¼Œä¹Ÿå¯ä»¥æŒ‰éœ€é€‰æ‹© gpt-4 ç­‰å…¶ä»–æ¨¡å‹\n",
    "        temperature: æ¨¡å‹è¾“å‡ºçš„æ¸©åº¦ç³»æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºç¨‹åº¦ï¼Œå–å€¼èŒƒå›´æ˜¯ 0~2ã€‚æ¸©åº¦ç³»æ•°è¶Šä½ï¼Œè¾“å‡ºå†…å®¹è¶Šä¸€è‡´ã€‚\n",
    "    '''\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=gen_gpt_messages(prompt),\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    if len(response.choices) > 0:\n",
    "        return response.choices[0].message.content\n",
    "    return \"generate answer error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_completion(\"ä½ å¥½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨ä¸Šè¿°å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬å°è£…äº† messages çš„ç»†èŠ‚ï¼Œä»…ä½¿ç”¨ user prompt æ¥å®ç°è°ƒç”¨ã€‚åœ¨ç®€å•åœºæ™¯ä¸­ï¼Œè¯¥å‡½æ•°è¶³å¤Ÿæ»¡è¶³ä½¿ç”¨éœ€æ±‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨æ™ºè°± GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ™ºè°± AI æ˜¯ç”±æ¸…åå¤§å­¦è®¡ç®—æœºç³»æŠ€æœ¯æˆæœè½¬åŒ–è€Œæ¥çš„å…¬å¸ï¼Œè‡´åŠ›äºæ‰“é€ æ–°ä¸€ä»£è®¤çŸ¥æ™ºèƒ½é€šç”¨æ¨¡å‹ã€‚å…¬å¸åˆä½œç ”å‘äº†åŒè¯­åƒäº¿çº§è¶…å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ GLM-130Bï¼Œå¹¶æ„å»ºäº†é«˜ç²¾åº¦é€šç”¨çŸ¥è¯†å›¾è°±ï¼Œå½¢æˆæ•°æ®ä¸çŸ¥è¯†åŒè½®é©±åŠ¨çš„è®¤çŸ¥å¼•æ“ï¼ŒåŸºäºæ­¤æ¨¡å‹æ‰“é€ äº† ChatGLMï¼ˆchatglm.cnï¼‰ã€‚\n",
    "\n",
    "ChatGLM ç³»åˆ—æ¨¡å‹ï¼ŒåŒ…æ‹¬ ChatGLM-130Bã€ChatGLM-6B å’Œ ChatGLM2-6Bï¼ˆChatGLM-6B çš„å‡çº§ç‰ˆæœ¬ï¼‰æ¨¡å‹ï¼Œæ”¯æŒç›¸å¯¹å¤æ‚çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶ä¸”èƒ½å¤Ÿè§£å†³å›°éš¾çš„æ¨ç†ç±»é—®é¢˜ã€‚å…¶ä¸­ï¼ŒChatGLM-6B æ¨¡å‹æ¥è‡ª Huggingface ä¸Šçš„ä¸‹è½½é‡å·²ç»è¶…è¿‡ 300wï¼ˆæˆªè‡³ 2023 å¹´ 6 æœˆ 24 æ—¥ç»Ÿè®¡æ•°æ®ï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨ Hugging Face (HF) å…¨çƒå¤§æ¨¡å‹ä¸‹è½½æ¦œä¸­è¿ç»­ 12 å¤©ä½å±…ç¬¬ä¸€åï¼Œåœ¨å›½å†…å¤–çš„å¼€æºç¤¾åŒºä¸­äº§ç”Ÿäº†è¾ƒå¤§çš„å½±å“ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è°ƒç”¨æ™ºè°± GLM API**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ™ºè°± AI æä¾›äº† SDK å’ŒåŸç”Ÿ HTTP æ¥å®ç°æ¨¡å‹ API çš„è°ƒç”¨ï¼Œå»ºè®®ä½¿ç”¨ SDK è¿›è¡Œè°ƒç”¨ä»¥è·å¾—æ›´å¥½çš„ç¼–ç¨‹ä½“éªŒã€‚\n",
    "\n",
    "é¦–å…ˆæˆ‘ä»¬éœ€è¦é…ç½®å¯†é’¥ä¿¡æ¯ï¼Œå°†å‰é¢è·å–åˆ°çš„ `API key` è®¾ç½®åˆ° `.env` æ–‡ä»¶ä¸­çš„ `ZHIPUAI_API_KEY` å‚æ•°ï¼Œç„¶åè¿è¡Œä»¥ä¸‹ä»£ç åŠ è½½é…ç½®ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# è¯»å–æœ¬åœ°/é¡¹ç›®çš„ç¯å¢ƒå˜é‡ã€‚\n",
    "\n",
    "# find_dotenv() å¯»æ‰¾å¹¶å®šä½ .env æ–‡ä»¶çš„è·¯å¾„\n",
    "# load_dotenv() è¯»å–è¯¥ .env æ–‡ä»¶ï¼Œå¹¶å°†å…¶ä¸­çš„ç¯å¢ƒå˜é‡åŠ è½½åˆ°å½“å‰çš„è¿è¡Œç¯å¢ƒä¸­  \n",
    "# å¦‚æœä½ è®¾ç½®çš„æ˜¯å…¨å±€çš„ç¯å¢ƒå˜é‡ï¼Œè¿™è¡Œä»£ç åˆ™æ²¡æœ‰ä»»ä½•ä½œç”¨ã€‚\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\learn-llm-rag-easily\\\\.env'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"ZHIPUAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ™ºè°±çš„è°ƒç”¨ä¼ å‚å’Œå…¶ä»–ç±»ä¼¼ï¼Œä¹Ÿéœ€è¦ä¼ å…¥ä¸€ä¸ª messages åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­åŒ…æ‹¬ role å’Œ promptã€‚æˆ‘ä»¬å°è£…å¦‚ä¸‹çš„ `get_completion` å‡½æ•°ï¼Œä¾›åç»­ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai import ZhipuAI\n",
    "\n",
    "client = ZhipuAI(\n",
    "    api_key=os.environ[\"ZHIPUAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "def gen_glm_params(prompt):\n",
    "    '''\n",
    "    æ„é€  GLM æ¨¡å‹è¯·æ±‚å‚æ•° messages\n",
    "\n",
    "    è¯·æ±‚å‚æ•°ï¼š\n",
    "        prompt: å¯¹åº”çš„ç”¨æˆ·æç¤ºè¯\n",
    "    '''\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def get_completion(prompt, model=\"glm-4\", temperature=0.95):\n",
    "    '''\n",
    "    è·å– GLM æ¨¡å‹è°ƒç”¨ç»“æœ\n",
    "\n",
    "    è¯·æ±‚å‚æ•°ï¼š\n",
    "        prompt: å¯¹åº”çš„æç¤ºè¯\n",
    "        model: è°ƒç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ä¸º glm-4ï¼Œä¹Ÿå¯ä»¥æŒ‰éœ€é€‰æ‹© glm-3-turbo ç­‰å…¶ä»–æ¨¡å‹\n",
    "        temperature: æ¨¡å‹è¾“å‡ºçš„æ¸©åº¦ç³»æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºç¨‹åº¦ï¼Œå–å€¼èŒƒå›´æ˜¯ 0~1.0ï¼Œä¸”ä¸èƒ½è®¾ç½®ä¸º 0ã€‚æ¸©åº¦ç³»æ•°è¶Šä½ï¼Œè¾“å‡ºå†…å®¹è¶Šä¸€è‡´ã€‚\n",
    "    '''\n",
    "\n",
    "    messages = gen_glm_params(prompt)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    if len(response.choices) > 0:\n",
    "        return response.choices[0].message.content\n",
    "    return \"generate answer error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹æ™ºè°±æ¸…è¨€ï¼Œå¯ä»¥å«æˆ‘å°æ™ºğŸ¤–ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"ä½ å¥½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œå¯¹ä¼ å…¥ zhipuai çš„å‚æ•°è¿›è¡Œç®€å•ä»‹ç»ï¼š\n",
    "\n",
    "- `messages (list)`ï¼Œè°ƒç”¨å¯¹è¯æ¨¡å‹æ—¶ï¼Œå°†å½“å‰å¯¹è¯ä¿¡æ¯åˆ—è¡¨ä½œä¸ºæç¤ºè¾“å…¥ç»™æ¨¡å‹ï¼›æŒ‰ç…§ {\"role\": \"user\", \"content\": \"ä½ å¥½\"} çš„é”®å€¼å¯¹å½¢å¼è¿›è¡Œä¼ å‚ï¼›æ€»é•¿åº¦è¶…è¿‡æ¨¡å‹æœ€é•¿è¾“å…¥é™åˆ¶åä¼šè‡ªåŠ¨æˆªæ–­ï¼Œéœ€æŒ‰æ—¶é—´ç”±æ—§åˆ°æ–°æ’åº\n",
    "\n",
    "- `temperature (float)`ï¼Œé‡‡æ ·æ¸©åº¦ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼Œå¿…é¡»ä¸ºæ­£æ•°å–å€¼èŒƒå›´æ˜¯ï¼š(0.0, 1.0)ï¼Œä¸èƒ½ç­‰äº 0ï¼Œé»˜è®¤å€¼ä¸º 0.95ã€‚å€¼è¶Šå¤§ï¼Œä¼šä½¿è¾“å‡ºæ›´éšæœºï¼Œæ›´å…·åˆ›é€ æ€§ï¼›å€¼è¶Šå°ï¼Œè¾“å‡ºä¼šæ›´åŠ ç¨³å®šæˆ–ç¡®å®š\n",
    "  \n",
    "- `top_p (float)`ï¼Œç”¨æ¸©åº¦å–æ ·çš„å¦ä¸€ç§æ–¹æ³•ï¼Œç§°ä¸ºæ ¸å–æ ·ã€‚å–å€¼èŒƒå›´æ˜¯ï¼š(0.0, 1.0) å¼€åŒºé—´ï¼Œä¸èƒ½ç­‰äº 0 æˆ– 1ï¼Œé»˜è®¤å€¼ä¸º 0.7ã€‚æ¨¡å‹è€ƒè™‘å…·æœ‰ top_p æ¦‚ç‡è´¨é‡ tokens çš„ç»“æœã€‚ä¾‹å¦‚ï¼š0.1 æ„å‘³ç€æ¨¡å‹è§£ç å™¨åªè€ƒè™‘ä»å‰ 10% çš„æ¦‚ç‡çš„å€™é€‰é›†ä¸­å– tokens\n",
    "\n",
    "- `request_id (string)`ï¼Œç”±ç”¨æˆ·ç«¯ä¼ å‚ï¼Œéœ€ä¿è¯å”¯ä¸€æ€§ï¼›ç”¨äºåŒºåˆ†æ¯æ¬¡è¯·æ±‚çš„å”¯ä¸€æ ‡è¯†ï¼Œç”¨æˆ·ç«¯ä¸ä¼ æ—¶å¹³å°ä¼šé»˜è®¤ç”Ÿæˆ\n",
    "\n",
    "- **å»ºè®®æ‚¨æ ¹æ®åº”ç”¨åœºæ™¯è°ƒæ•´ top_p æˆ– temperature å‚æ•°ï¼Œä½†ä¸è¦åŒæ—¶è°ƒæ•´ä¸¤ä¸ªå‚æ•°**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai import ZhipuAI\n",
    "client = ZhipuAI(api_key=os.environ[\"ZHIPUAI_API_KEY\"]) \n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4\",  # å¡«å†™éœ€è¦è°ƒç”¨çš„æ¨¡å‹ç¼–ç \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªä¹äºè§£ç­”å„ç§é—®é¢˜çš„åŠ©æ‰‹ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä¸ºç”¨æˆ·æä¾›ä¸“ä¸šã€å‡†ç¡®ã€æœ‰è§åœ°çš„å»ºè®®ã€‚\"},\n",
    "        {\"role\": \"user\", \"content\": \"å†œå¤«éœ€è¦æŠŠç‹¼ã€ç¾Šå’Œç™½èœéƒ½å¸¦è¿‡æ²³ï¼Œä½†æ¯æ¬¡åªèƒ½å¸¦ä¸€æ ·ç‰©å“ï¼Œè€Œä¸”ç‹¼å’Œç¾Šä¸èƒ½å•ç‹¬ç›¸å¤„ï¼Œç¾Šå’Œç™½èœä¹Ÿä¸èƒ½å•ç‹¬ç›¸å¤„ï¼Œé—®å†œå¤«è¯¥å¦‚ä½•è¿‡æ²³ã€‚\"}\n",
    "    ],\n",
    ")\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "myllm = ChatOpenAI(\n",
    "    temperature=0.95,\n",
    "    model=\"glm-4\",\n",
    "    openai_api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\",\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "myllm.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯DeepSeek Chatï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ çš„æ™ºèƒ½åŠ©æ‰‹ï¼ğŸ¤–âœ¨ æˆ‘å¯ä»¥å¸®ä½ è§£ç­”å„ç§é—®é¢˜ã€æä¾›ä¿¡æ¯ã€é™ªä½ èŠå¤©ï¼Œç”šè‡³å¸®ä½ å¤„ç†æ–‡æ¡£ã€‚å¦‚æœæœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œå°½ç®¡é—®æˆ‘å§ï¼ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# Please install OpenAI SDK first: `pip3 install openai`\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"sk-c93efe07b0ef4445baca2cd28f54cb78\", base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"ä½ æ˜¯è°ï¼Ÿ\"},\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_deepseek'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv, find_dotenv\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_deepseek\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatDeepSeek\n\u001b[32m      5\u001b[39m _ = load_dotenv(find_dotenv())    \u001b[38;5;66;03m# read local .env file\u001b[39;00m\n\u001b[32m      6\u001b[39m deepseek_api_key = os.environ[\u001b[33m'\u001b[39m\u001b[33mDEEPSEEK_API_KEY\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_deepseek'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "_ = load_dotenv(find_dotenv())    # read local .env file\n",
    "deepseek_api_key = os.environ['DEEPSEEK_API_KEY']\n",
    "llm_deepseek = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    # max_retries=2,\n",
    "    api_key=deepseek_api_key,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "llm_deepseek.invoke(\"ä½ æ˜¯è°ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ æœ¬åœ°å¼€æºLLM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è°ƒç”¨ollamaéƒ¨ç½²çš„å¤§æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è°ƒç”¨æœ¬åœ°å¤§æ¨¡å‹ï¼ŒåŒæ ·å¯ä»¥ä½¿ç”¨ [ChatCompletion API](https://platform.openai.com/docs/api-reference/chat)ï¼Œè¯¥ API æä¾›äº† ChatGPT ç³»åˆ—æ¨¡å‹çš„è°ƒç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nå¥½ï¼Œæˆ‘æ¥æƒ³æƒ³ç”¨æˆ·é—®çš„é—®é¢˜ï¼šâ€œå¤§è¯­è¨€æ¨¡å‹æ˜¯ä»€ä¹ˆï¼Ÿâ€å¤§å®¶ä»¥å‰çœ‹åˆ°è¿‡èŠå¤©æœºå™¨äººä»€ä¹ˆçš„ï¼Œæˆ–è€…å¬è¯´ hearing-a-loudè¿™ç§ä¸œè¥¿ï¼Œé‚£åº”è¯¥å°±æ˜¯å¤§è¯­è¨€æ¨¡å‹å§ã€‚é‚£æˆ‘å¾—ä¸€æ­¥æ­¥è§£é‡Šæ¸…æ¥šã€‚\\n\\né¦–å…ˆï¼Œç”¨æˆ·å¯èƒ½å¯¹ç§‘æŠ€æœ‰ä¸€å®šäº†è§£ï¼Œæ‰€ä»¥éœ€è¦è§£é‡Šå‡ ä¸ªå…³é”®ç‚¹ï¼šå®ƒåˆ°åº•æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿå®ƒæ˜¯æ€ä¹ˆå·¥ä½œçš„ï¼Ÿè¿˜æœ‰å®ƒçš„Applicationsæ˜¯ä»€ä¹ˆï¼Ÿæ¯ä¸ªéƒ¨åˆ†æˆ‘éƒ½å¾—è¯¦ç»†ä¸€ç‚¹ï¼Œå¹¶ä¸”ç”¨ä¾‹å­æ¥è¯´æ˜ï¼Œè¿™æ ·ç”¨æˆ·èƒ½æ›´å¥½åœ°ç†è§£ã€‚\\n\\nå…ˆä»å®šä¹‰å¼€å§‹ï¼šå¤§è¯­è¨€æ¨¡å‹å°±æ˜¯ç”¨æ¥ç”Ÿæˆè‡ªç„¶æ®µè½çš„æ™ºèƒ½æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚è¿™æ ·ç›´æ¥å›åº”äº†ä»–å¯èƒ½çš„é—®é¢˜ï¼Œâ€œä»€ä¹ˆâ€å˜›ï¼Œè¿™å°±æ˜¯å¤§è¯­è¨€æ¨¡å‹å•Šï¼\\n\\næ¥ä¸‹æ¥æ˜¯å·¥ä½œåŸç†éƒ¨åˆ†ã€‚è¿™éƒ¨åˆ†è¦åˆ†å‡ ä¸ªæ­¥éª¤ã€‚ç¬¬ä¸€ï¼Œæ•°æ®è¾“å…¥ï¼Œä¹Ÿå°±æ˜¯æŠŠä¸­æ–‡å¥å­æˆ–è€…æ‰“å­—çš„ä¸œè¥¿ä¼ è¿›å»ã€‚ç„¶åï¼Œåœ¨ç¥ç»ç½‘ç»œé‡Œé¢å»å¤„ç†è¿™äº›è¾“å…¥ä¿¡æ¯ï¼ŒæŠŠå®ƒä»¬è½¬åŒ–ä¸ºå¯ä»¥ç†è§£çš„ä»£ç ã€‚è¿™é‡Œå¯èƒ½éœ€è¦è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„æ¦‚å¿µï¼Œè¿™æ ·ç”¨æˆ·èƒ½æ˜ç™½å®ƒç”¨äº†ä»€ä¹ˆæŠ€æœ¯ã€‚\\n\\nç„¶åæ˜¯ç”Ÿæˆè¿‡ç¨‹ï¼Œæ¨¡å‹é€šè¿‡å‰é¢çš„è¿‡ç¨‹å¾—åˆ°çš„çŸ¥è¯†åº“ï¼Œå†ç»“åˆè¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼ŒæŠŠå®ƒç¿»è¯‘æˆè‡ªç„¶è¯­è¨€æ ¼å¼çš„è¾“å‡ºç»“æœã€‚è¿™éƒ¨åˆ†å¾—å…·ä½“ç‚¹è®²ï¼Œæ¯”å¦‚å…·ä½“çš„ç®—æ³•å’Œå‚æ•°è®¾ç½®ã€‚\\n\\næœ€åæ˜¯åº”ç”¨éƒ¨åˆ†ã€‚é™¤äº†èŠå¤©æœºå™¨äººä¹‹å¤–ï¼Œå¤§è¯­è¨€æ¨¡å‹è¿˜å¯ä»¥åšäº›ä»€ä¹ˆï¼Ÿæ¯”å¦‚çŸ¥è¯†é—®ç­”ã€å¯¹è¯åˆ†æã€æ–‡æœ¬ç”Ÿæˆç­‰åº”ç”¨ã€‚æ¯ä¸ªä¾‹å­éƒ½è§£é‡Šä¸€ä¸‹å®ƒçš„ä¼˜åŠ¿ï¼Œè¿™æ ·ç”¨æˆ·èƒ½æ˜ç™½å®ƒä¸ºä»€ä¹ˆè¿™ä¹ˆé‡è¦ã€‚\\n\\nåœ¨æ€è€ƒè¿‡ç¨‹ä¸­ï¼Œå¯èƒ½ä¼šæœ‰ç‚¹ç–‘é—®ï¼Œâ€œæœºå™¨å­¦ä¹ â€æ˜¯æ€ä¹ˆå…·ä½“å®ç°çš„ï¼Ÿæ˜¯ä¸æ˜¯æœ‰å…·ä½“çš„ç®—æ³•å’Œå‚æ•°è°ƒæ•´ï¼Ÿè¿˜æœ‰ï¼šâ€œè¯­è¨€æ¨¡å‹çš„èƒ½åŠ›â€æ˜¯å¦‚ä½•è¿ä½œçš„ï¼Ÿæ˜¯é€šè¿‡ä¸€äº›ç‰¹å®šçš„è¯­è¨€æ¨¡å‹ç»“æ„å—ï¼Ÿ\\n\\nå¯èƒ½è¿˜ä¼šå¥½å¥‡ï¼Œè¿™äº›å¤§è¯­è¨€æ¨¡å‹å¦‚ä½•åº”å¯¹ä¸åŒçš„äººç¾¤ï¼Œæ¯”å¦‚è€å¹´äººæˆ–è€…å„¿ç«¥ï¼Œä¼šä¸ä¼šæœ‰é—®é¢˜ï¼Ÿç”¨æˆ·å¯ä»¥è¿™æ ·è¡¥å……ä¸€ä¸‹ï¼Œè¯´æ˜è€ƒè™‘åˆ°å„ç§å¹´é¾„æ®µçš„éœ€æ±‚ã€‚\\n\\næ€»çš„æ¥è¯´ï¼Œè¦ç¡®ä¿æ¯éƒ¨åˆ†å†…å®¹éƒ½æ¸…æ™°è¯¦ç»†ï¼ŒåŒæ—¶ç”¨ç®€å•çš„è¯­è¨€è®©ç”¨æˆ·å®¹æ˜“ç†è§£å’Œæ¥å—ã€‚å¦å¤–ï¼Œä¾‹å­è¦ç”¨å®é™…çš„åº”ç”¨åœºæ™¯æ¥è¯´æ˜ï¼Œè¿™æ ·æ›´æœ‰è¯´æœåŠ›ã€‚\\n</think>\\n\\nå¤§è¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ çš„æ™ºèƒ½ç³»ç»Ÿå·¥å…·ï¼Œä¸»è¦åº”ç”¨äºç”Ÿæˆè‡ªç„¶æ®µè½æˆ–æ–‡æœ¬å†…å®¹ã€‚å®ƒä»¥â€œè¯­è¨€â€ä¸ºæ ¸å¿ƒæ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆå°±ã€‚\\n\\n### å®šä¹‰\\nå¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Model, LLMï¼‰æ˜¯ç”¨ç»Ÿè®¡å­¦ä¹ æ–¹æ³•å¯¹å¤§é‡çš„ä¸Šä¸‹æ–‡æ•°æ®è¿›è¡Œè®­ç»ƒè€Œå½¢æˆçš„é«˜çº§æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚å®ƒä»¬ä¸»è¦çš„åŠŸèƒ½æ˜¯ç”Ÿæˆå¤šæ ·åŒ–çš„ä¸­æ–‡ã€è‹±æ–‡ã€æˆ–å…¶ä»–è¯­è¨€çš„æ–‡æœ¬å†…å®¹ã€‚\\n\\n### å·¥ä½œåŸç†\\n\\n1. **æ•°æ®è¾“å…¥**\\n   - å¤§è¯­è¨€æ¨¡å‹æ¥æ”¶ç”¨æˆ·è¾“å…¥çš„ä¸­æ–‡æˆ–è‹±æ–‡å¥å­ï¼Œä»¥åŠå¯èƒ½åŒ…å«æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚å¯¹è¯å²ã€æ ‡ç­¾ç­‰ï¼‰ã€‚\\n\\n2. **çŸ¥è¯†åº“æ„å»º**\\n   - ä½¿ç”¨å¤§è§„æ¨¡çš„è¯­è¨€çŸ¥è¯†åº“ï¼ˆLKDï¼‰å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚è¿™ç§çŸ¥è¯†æ˜¯ä»¥è¯è¡¨å½¢å¼å­˜å‚¨åœ¨ä¸€èµ·ï¼Œé€šè¿‡æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰å¤„ç†ç”Ÿæˆæ¦‚ç‡ã€‚\\n\\n3. **è¯­è¨€æ¨¡å‹æ¨ç†**\\n   - æ¨¡å‹æ ¹æ®è¾“å…¥æ•°æ®å’Œå·²æœ‰çš„ä¸Šä¸‹æ–‡çŸ¥è¯†é¢„æµ‹æ–‡æœ¬åºåˆ—æˆ–å¥å­ã€‚è¿™ç±»ä¼¼äºè‡ªç„¶è¯­è¨€ç†è§£ä¸ç”Ÿæˆçš„ä½œç”¨æœºåˆ¶ã€‚\\n\\n### åº”ç”¨åœºæ™¯\\n\\n1. **èŠå¤©æœºå™¨äºº**\\n   - ç±»ä¼¼æ‰“å­—æœºä¸­çš„è¯­éŸ³åˆæˆï¼Œè¾“å‡ºè‡ªç„¶æµç•…çš„ä¸­æ–‡è¯­å¥ï¼Œå¦‚ï¼š\"æ¬¢è¿å…‰ä¸´å“ˆå–½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\"\\n\\n2. **çŸ¥è¯†é—®ç­”ç³»ç»Ÿ**\\n   - è§£é‡Šå„ç§ä¸“ä¸šæœ¯è¯­å’Œæ¦‚å¿µï¼Œå¹¶æä¾›å‡†ç¡®çš„çŸ¥è¯†æ”¯æŒå›ç­”ã€‚\\n\\n3. **å¯¹è¯åˆ†æä¸å»ºæ¨¡**\\n   - åˆ†æç”¨æˆ·æˆ–æƒ…å¢ƒä¹‹é—´çš„æ–‡æœ¬äº’åŠ¨ï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„è§£é‡Šæˆ–é¢„æµ‹ã€‚\\n\\n4. **æ–‡æœ¬ç”Ÿæˆä¸ç¿»è¯‘**\\n   - è‡ªåŠ¨ç”Ÿæˆæ–°æ®µè½ã€è‡ªåŠ¨ç¿»è¯‘ç­‰ä»»åŠ¡ï¼Œä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ–¹æ³•å’Œå¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯ã€‚\\n\\n5. **å†…å®¹æ¨èç³»ç»Ÿ**\\n   - ä»å¤§é‡å†å²å†…å®¹ä¸­é€‰æ‹©æœ€ä½³åŒ¹é…çš„æ¨èï¼ŒåŸºäºç”¨æˆ·è¡Œä¸ºå’Œåå¥½ã€‚\\n\\n6. **é—®ç­”ç³»ç»Ÿ**\\n   - æä¾›è¯¦ç»†çš„ä¸Šä¸‹æ–‡å’Œä¿¡æ¯ï¼Œå¸®åŠ©ç”¨æˆ·åœ¨ç‰¹å®šä¸»é¢˜ä¸‹è·å–å‡†ç¡®çš„å›ç­”ã€‚\\n\\n### æ€»ç»“\\nå¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒåœ¨äºå…¶å¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿç†è§£ã€ç”Ÿæˆå’Œäº’åŠ¨ä½¿ç”¨äººç±»ç»éªŒä¸°å¯Œçš„ä¸­æ–‡çŸ¥è¯†ï¼Œå¹¶æ ¹æ®è¾“å…¥ä»»åŠ¡è‡ªåŠ¨å®Œæˆå¤šç§ä»»åŠ¡ã€‚'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1/',\n",
    "    api_key = 'ollama'\n",
    ")\n",
    "\n",
    "prompt = 'å¤§è¯­è¨€æ¨¡å‹æ˜¯ä»€ä¹ˆï¼Ÿ'\n",
    "messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "response = client.chat.completions.create(\n",
    "    model = 'deepseek-r1:1.5b',\n",
    "    messages = messages,\n",
    "    temperature=0.95,\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\n\\n</think>\\n\\nä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯ç”Ÿæ´»ä¸­çš„é—®é¢˜ï¼Œéƒ½å¯ä»¥å‘Šè¯‰æˆ‘å“¦ï¼ğŸ˜Š'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "my_llm = OllamaLLM(base_url='http://localhost:11434', model='deepseek-r1:1.5b', temperature=0.1)\n",
    "my_llm.invoke(\"ä½ å¥½\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_411069/4034943906.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  my_llm = Ollama(base_url='http://localhost:11434', model='deepseek-r1:1.5b')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<think>\\n\\n</think>\\n\\nä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯èŠå¤©ã€è§£ç­”é—®é¢˜è¿˜æ˜¯æä¾›å»ºè®®ï¼Œæˆ‘éƒ½åœ¨è¿™å„¿ä¸ºä½ æœåŠ¡ã€‚ğŸ˜Š'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "my_llm = Ollama(base_url='http://localhost:11434', model='deepseek-r1:1.5b')\n",
    "\n",
    "response = my_llm.invoke(\"ä½ å¥½\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è°ƒç”¨vllméƒ¨ç½²çš„å¤§æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response: ChatCompletion(id='chatcmpl-21421752cdcd48b7b1ed53379a4fddff', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Alright, so the user asked me to tell them a joke. I need to respond in a way that\\'s helpful and engaging. Let me think of a good joke that\\'s a bit light-hearted. Maybe something about animals since that\\'s a popular topic. \\n\\nI remember a classic joke about bees and honey. It goes like this: Why do bees build hives in the spring? Because they\\'re looking for a sweet deal! That\\'s a simple and funny one. It\\'s a play on words with \"sweet deal\" sounding like \"hives\" and \"spring\" sounding like \"hives\" too.\\n\\nI should make sure the joke is clear and the pun works well. It should be easy to understand and bring a smile. I\\'ll go ahead and provide the joke as requested. I don\\'t want to overcomplicate it. Keeping it simple is better for humor.\\n</think>\\n\\nWhy do bees build hives in the spring?  \\nBecause they\\'re looking for a sweet deal!', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None)], created=1743494974, model='deepseek-r1-distill-qwen-7b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=200, prompt_tokens=16, total_tokens=216, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"token-abc123\"\n",
    "openai_api_base = \"http://localhost:8081/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"deepseek-r1-distill-qwen-14b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "    ]\n",
    ")\n",
    "print(\"Chat response:\", chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLMOpenAI  # æ³¨æ„ç±»åä¸º VLLMOpenAI[3](@ref)\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_key=\"token-abc123\",          # vLLM æ— éœ€é‰´æƒï¼Œè®¾ä¸ºç©ºå­—ç¬¦ä¸²[3](@ref)\n",
    "    openai_api_base=\"deepseek-r1-distill-qwen-14b\",  # æœåŠ¡ç«¯åœ°å€\n",
    "    model_name=\"deepseek-r1-1.5b\",  # éœ€ä¸éƒ¨ç½²çš„æ¨¡å‹è·¯å¾„ä¸€è‡´\n",
    "    max_tokens=1024,                # æ§åˆ¶ç”Ÿæˆæ–‡æœ¬æœ€å¤§é•¿åº¦\n",
    "    temperature=0,               # ç”Ÿæˆå¤šæ ·æ€§å‚æ•°ï¼ˆ0~1ï¼‰\n",
    "    # top_p=0.9,                      # é‡‡æ ·é˜ˆå€¼\n",
    "    streaming=True                  # æ”¯æŒæµå¼è¾“å‡ºï¼ˆå¯é€‰ï¼‰\n",
    ")\n",
    "response = llm.invoke(\"ä½ æ˜¯è°ï¼Ÿ\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è°ƒç”¨xinferenceéƒ¨ç½²çš„å¤§æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Assume that the model is already launched.\n",
    "# The api_key can't be empty, any string is OK.\n",
    "client = openai.Client(api_key=\"not empty\", base_url=\"http://localhost:9997/v1\")\n",
    "client.chat.completions.create(\n",
    "    model=model_uid,\n",
    "    messages=[\n",
    "        {\n",
    "            \"content\": \"What is the largest animal?\",\n",
    "            \"role\": \"user\",\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xinference list æŸ¥çœ‹å·²å®‰è£…æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Xinference\n",
    "\n",
    "llm = Xinference(\n",
    "    server_url=\"http://localhost:9997\",\n",
    "    model_uid = {model_uid} # replace model_uid with the model UID return from launching the model\n",
    ")\n",
    "\n",
    "llm.invoke(\n",
    "    prompt=\"Q: where can we visit in the capital of France? A:\",\n",
    "    generate_config={\"max_tokens\": 1024, \"stream\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨embedding API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥zhipuä¸ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "my_emb = ZhipuAIEmbeddings(\n",
    "    model=\"embedding-2\",\n",
    "    api_key=os.environ[\"ZHIPUAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨æœ¬åœ°éƒ¨ç½²çš„embeddingæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨HuggingFaceéƒ¨ç½²"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 7,
>>>>>>> b957d12aabc9c38e54c3c249d31f2d9ec52f3e2c
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\will\\AppData\\Local\\Temp\\ipykernel_1576\\602453622.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  my_emb = HuggingFaceEmbeddings(model_name='/opt/workspace/models/BAAI/bge-small-zh-v1.5')\n",
      "E:\\ProgramData\\Python312\\venv\\langchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Path /opt/workspace/models/BAAI/bge-small-zh-v1.5 not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhuggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m my_emb = \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/opt/workspace/models/BAAI/bge-small-zh-v1.5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33må¦‚ä½•è°ƒç”¨HuggingFaceEmbeddingsï¼Ÿ\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m query_vector = my_emb.embed_query(query)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\ProgramData\\Python312\\venv\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:214\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    213\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\ProgramData\\Python312\\venv\\langchain\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:92\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import sentence_transformers python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     89\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28mself\u001b[39m.client = \u001b[43msentence_transformers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\ProgramData\\Python312\\venv\\langchain\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:296\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(model_name_or_path):\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# Not a path, load from hub\u001b[39;00m\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mor\u001b[39;00m model_name_or_path.count(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPath \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mand\u001b[39;00m model_name_or_path.lower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m basic_transformer_models:\n\u001b[32m    299\u001b[39m         \u001b[38;5;66;03m# A model from sentence-transformers\u001b[39;00m\n\u001b[32m    300\u001b[39m         model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + model_name_or_path\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Path /opt/workspace/models/BAAI/bge-small-zh-v1.5 not found"
     ]
    }
   ],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer(\"/opt/workspace/models/BAAI/bge-small-zh-v1.5\")\n",
    "# embeddings = model.encode([\"Hello, world!\"], normalize_embeddings=True)\n",
    "# embeddings[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/langchain/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/anaconda3/envs/langchain/lib/python3.12/site-packages/torch/cuda/__init__.py:789: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æŸ¥è¯¢å‘é‡: [-0.023251689970493317, 0.03897852823138237, -0.015023290179669857, 0.037697553634643555, -0.031113281846046448] ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "my_emb = HuggingFaceEmbeddings(model_name='/opt/workspace/models/BAAI/bge-small-zh-v1.5')\n",
    "\n",
    "query = \"å¦‚ä½•è°ƒç”¨HuggingFaceEmbeddingsï¼Ÿ\"\n",
    "\n",
    "query_vector = my_emb.embed_query(query) \n",
    "\n",
    "print(\"æŸ¥è¯¢å‘é‡:\", query_vector[:5], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨ollamaéƒ¨ç½²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "my_emb = OllamaEmbeddings(base_url='http://localhost:11434',model=\"bge-m3:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æŸ¥è¯¢å‘é‡: [-0.01782775, -0.010210706, -0.031165373, 0.029789878, -0.024330704] ...\n",
      "æ–‡æ¡£å‘é‡åˆ—è¡¨: [[-0.029961862, -0.0005994315, -0.03138702, 0.027966358, -0.018949162], [0.0018570031, 0.013093924, -0.020074107, 0.024226347, -0.0036691278]] ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "my_emb = OllamaEmbeddings(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"bge-m3:latest\"\n",
    ")\n",
    "\n",
    "# ç¤ºä¾‹æ–‡æœ¬\n",
    "query = \"å¦‚ä½•è°ƒç”¨OllamaEmbeddingsï¼Ÿ\"\n",
    "documents = [\n",
    "    \"OllamaEmbeddings ç”¨äºç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡ã€‚\",\n",
    "    \"è°ƒç”¨æ–¹æ³•åŒ…æ‹¬ embed_query å’Œ embed_documentsã€‚\"\n",
    "]\n",
    "\n",
    "# ç”ŸæˆåµŒå…¥å‘é‡\n",
    "query_vector = my_emb.embed_query(query)\n",
    "doc_vectors = my_emb.embed_documents(documents)\n",
    "\n",
    "# è¾“å‡ºç»“æœ\n",
    "print(\"æŸ¥è¯¢å‘é‡:\", query_vector[:5], \"...\")\n",
    "print(\"æ–‡æ¡£å‘é‡åˆ—è¡¨:\", [vec[:5] for vec in doc_vectors], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨xinferenceéƒ¨ç½²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import XinferenceEmbeddings\n",
    "\n",
    "# æ›¿æ¢ä¸ºä½ çš„XinferenceæœåŠ¡å™¨URLå’Œæ¨¡å‹UID\n",
    "xinference = XinferenceEmbeddings(\n",
    "    server_url=\"http://localhost:9997\",  # æ³¨æ„ï¼šåŸä»£ç ä¸­çš„\"loaclhost\"æ‹¼å†™é”™è¯¯ï¼Œåº”ä¸º\"localhost\"\n",
    "    model_uid=\"your_model_uid\"  # æ›¿æ¢ä¸ºå®é™…çš„æ¨¡å‹UID\n",
    ")\n",
    "\n",
    "# è¾“å…¥æ–‡æœ¬\n",
    "texts = [\"ä½ å¥½ï¼Œä¸–ç•Œã€‚\", \"LangChain æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ã€‚\"]\n",
    "\n",
    "# ç”ŸæˆåµŒå…¥å‘é‡\n",
    "vectors = xinference.embed_documents(texts)\n",
    "\n",
    "# æ‰“å°ç»“æœ\n",
    "for idx, vector in enumerate(vectors):\n",
    "    print(f\"æ–‡æœ¬ {idx + 1}: {texts[idx]}\")\n",
    "    print(f\"åµŒå…¥å‘é‡: {vector[:5]}... (ç»´åº¦: {len(vector)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨reranker API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "import cohere\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "cohere_client = cohere.Client(api_key=\"Tahx1eySFbKvu9sTyTXrRLf59la3ZUG9vy02stRZ\")\n",
    "\n",
    "compressor = CohereRerank(\n",
    "    client=cohere_client,\n",
    "    top_n=3,\n",
    "    model=\"rerank-multilingual-v3.0\"  # æ”¯æŒå¤šè¯­è¨€çš„æ–°ç‰ˆæœ¬\n",
    ")\n",
    "\n",
    "# æµ‹è¯•æ ·ä¾‹æ•°æ®\n",
    "documents = [\n",
    "    Document(page_content=\"å·´é»æ˜¯æ³•å›½çš„é¦–éƒ½ï¼Œä¹Ÿæ˜¯è‘—åçš„è‰ºæœ¯æ–‡åŒ–ä¸­å¿ƒã€‚\", metadata={\"source\": \"wiki\"}),\n",
    "    Document(page_content=\"åŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»å’Œæ–‡åŒ–ä¸­å¿ƒï¼Œæ‹¥æœ‰ç´«ç¦åŸç­‰å†å²å»ºç­‘ã€‚\", metadata={\"source\": \"gov\"}),\n",
    "    Document(page_content=\"Capital punishment refers to the death penalty in legal systems.\", metadata={\"source\": \"law\"}),\n",
    "    Document(page_content=\"ä¸œäº¬æ˜¯æ—¥æœ¬æœ€å¤§çš„åŸå¸‚ï¼Œä¹Ÿæ˜¯å…¨çƒé‡è¦çš„ç»æµä¸­å¿ƒã€‚\", metadata={\"source\": \"news\"}),\n",
    "    Document(page_content=\"Washington D.C. is the capital of the United States.\", metadata={\"source\": \"edu\"}),\n",
    "    Document(page_content=\"é¦–å°”æ˜¯éŸ©å›½çš„é¦–éƒ½ï¼Œä»¥ç°ä»£åŒ–ä¸ä¼ ç»Ÿæ–‡åŒ–çš„èåˆé—»åã€‚\", metadata={\"source\": \"travel\"}),\n",
    "    Document(page_content=\"Capitalization in finance refers to the total value of a company's shares.\", metadata={\"source\": \"finance\"})\n",
    "]\n",
    "\n",
    "query = \"å„å›½é¦–éƒ½åŸå¸‚çš„ä»‹ç»æœ‰å“ªäº›ï¼Ÿ\"\n",
    "\n",
    "# æ‰§è¡Œé‡æ’åº\n",
    "compressed_docs = compressor.compress_documents(documents=documents, query=query)\n",
    "\n",
    "# æ‰“å°æ’åºç»“æœ\n",
    "print(\"===== é‡æ’åºåçš„Top 5æ–‡æ¡£ =====\")\n",
    "for i, doc in enumerate(compressed_docs):\n",
    "    print(f\"Rank {i+1} (Score: {doc.metadata['relevance_score']:.4f}):\")\n",
    "    print(f\"å†…å®¹ï¼š{doc.page_content}\")\n",
    "    print(f\"å…ƒæ•°æ®ï¼š{doc.metadata}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== é‡æ’åºåçš„Top 3æ–‡æ¡£ =====\n",
      "Rank 1 (Score: 0.2323):\n",
      "å†…å®¹ï¼šWashington D.C. is the capital of the United States.\n",
      "å…ƒæ•°æ®ï¼š{'source': 'edu', 'relevance_score': 0.23231014609336853}\n",
      "--------------------------------------------------\n",
      "Rank 2 (Score: 0.1541):\n",
      "å†…å®¹ï¼šå·´é»æ˜¯æ³•å›½çš„é¦–éƒ½ï¼Œä¹Ÿæ˜¯è‘—åçš„è‰ºæœ¯æ–‡åŒ–ä¸­å¿ƒã€‚\n",
      "å…ƒæ•°æ®ï¼š{'source': 'wiki', 'relevance_score': 0.154057577252388}\n",
      "--------------------------------------------------\n",
      "Rank 3 (Score: 0.1432):\n",
      "å†…å®¹ï¼šé¦–å°”æ˜¯éŸ©å›½çš„é¦–éƒ½ï¼Œä»¥ç°ä»£åŒ–ä¸ä¼ ç»Ÿæ–‡åŒ–çš„èåˆé—»åã€‚\n",
      "å…ƒæ•°æ®ï¼š{'source': 'travel', 'relevance_score': 0.14318770170211792}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_compressors import JinaRerank  # ä½¿ç”¨Jinaçš„rerankç»„ä»¶\n",
    "\n",
    "# Jina Reranké…ç½®\n",
    "JINA_API_KEY = \"jina_63bb115e2d5f42d581f42643294792b5CE4nrEINMDcT4vJZJaSLcr5tkbIB\"  # æ›¿æ¢ä¸ºä½ çš„Jina APIå¯†é’¥\n",
    "\n",
    "compressor = JinaRerank(\n",
    "    jina_api_key=JINA_API_KEY,\n",
    "    top_n=3,\n",
    "    model=\"jina-reranker-v2-base-multilingual\"  # Jinaçš„å¤šè¯­è¨€rerankæ¨¡å‹[5](@ref)\n",
    ")\n",
    "\n",
    "# æµ‹è¯•æ ·ä¾‹æ•°æ®\n",
    "documents = [\n",
    "    Document(page_content=\"å·´é»æ˜¯æ³•å›½çš„é¦–éƒ½ï¼Œä¹Ÿæ˜¯è‘—åçš„è‰ºæœ¯æ–‡åŒ–ä¸­å¿ƒã€‚\", metadata={\"source\": \"wiki\"}),\n",
    "    Document(page_content=\"åŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»å’Œæ–‡åŒ–ä¸­å¿ƒï¼Œæ‹¥æœ‰ç´«ç¦åŸç­‰å†å²å»ºç­‘ã€‚\", metadata={\"source\": \"gov\"}),\n",
    "    Document(page_content=\"Capital punishment refers to the death penalty in legal systems.\", metadata={\"source\": \"law\"}),\n",
    "    Document(page_content=\"ä¸œäº¬æ˜¯æ—¥æœ¬æœ€å¤§çš„åŸå¸‚ï¼Œä¹Ÿæ˜¯å…¨çƒé‡è¦çš„ç»æµä¸­å¿ƒã€‚\", metadata={\"source\": \"news\"}),\n",
    "    Document(page_content=\"Washington D.C. is the capital of the United States.\", metadata={\"source\": \"edu\"}),\n",
    "    Document(page_content=\"é¦–å°”æ˜¯éŸ©å›½çš„é¦–éƒ½ï¼Œä»¥ç°ä»£åŒ–ä¸ä¼ ç»Ÿæ–‡åŒ–çš„èåˆé—»åã€‚\", metadata={\"source\": \"travel\"}),\n",
    "    Document(page_content=\"Capitalization in finance refers to the total value of a company's shares.\", metadata={\"source\": \"finance\"})\n",
    "]\n",
    "\n",
    "query = \"å„å›½é¦–éƒ½åŸå¸‚çš„ä»‹ç»æœ‰å“ªäº›ï¼Ÿ\"\n",
    "\n",
    "# æ‰§è¡Œé‡æ’åº\n",
    "compressed_docs = compressor.compress_documents(documents=documents, query=query)\n",
    "\n",
    "# æ‰“å°æ’åºç»“æœ\n",
    "print(\"===== é‡æ’åºåçš„Top 3æ–‡æ¡£ =====\")\n",
    "for i, doc in enumerate(compressed_docs):\n",
    "    print(f\"Rank {i+1} (Score: {doc.metadata['relevance_score']:.4f}):\")\n",
    "    print(f\"å†…å®¹ï¼š{doc.page_content}\")\n",
    "    print(f\"å…ƒæ•°æ®ï¼š{doc.metadata}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨å¼€æºçš„Rerankeræ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ï¼modelscope download --model BAAI/bge-reranker-base --cache_dir /opt/workspace/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨¡å‹è·¯å¾„ï¼š  /opt/workspace/models/BAAI/bge-reranker-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…ˆå°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "model = HuggingFaceCrossEncoder(model_name=\"/opt/workspace/models/BAAI/bge-reranker-base\")\n",
    "compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\"What is the plan for the economy?\")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\")\n",
    "compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\"What is the plan for the economy?\")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
