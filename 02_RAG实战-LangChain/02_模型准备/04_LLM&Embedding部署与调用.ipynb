{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨¡å‹çš„éƒ¨ç½²å‚è€ƒï¼š [learn-llm-deploy-easily](https://gitee.com/coderwillyan/learn-llm-deploy-easily) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œä¸»è¦ä»‹ç»å¦‚ä½•è°ƒç”¨å·²éƒ¨ç½²çš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ LLM API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ¬ç« èŠ‚ä¸»è¦ä»‹ç»æ™ºè°± GLMçš„ API ç”³è¯·æŒ‡å¼•å’Œ Python ç‰ˆæœ¬çš„åŸç”Ÿ API è°ƒç”¨æ–¹æ³•ï¼Œè¯»è€…æŒ‰ç…§å®é™…æƒ…å†µé€‰æ‹©ä¸€ç§è‡ªå·±å¯ä»¥ç”³è¯·çš„ API è¿›è¡Œé˜…è¯»å­¦ä¹ å³å¯ã€‚\n",
    "\n",
    "å¦‚æœä½ éœ€è¦åœ¨ LangChain ä¸­ä½¿ç”¨ LLMï¼Œå¯ä»¥å‚ç…§[LLM æ¥å…¥ LangChain]ä¸­çš„è°ƒç”¨æ–¹å¼ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¯»å– `.env` æ–‡ä»¶ä¸­ä¿å­˜çš„API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# è¯»å–æœ¬åœ°/é¡¹ç›®çš„ç¯å¢ƒå˜é‡ã€‚\n",
    "\n",
    "# find_dotenv() å¯»æ‰¾å¹¶å®šä½ .env æ–‡ä»¶çš„è·¯å¾„\n",
    "# load_dotenv() è¯»å–è¯¥ .env æ–‡ä»¶ï¼Œå¹¶å°†å…¶ä¸­çš„ç¯å¢ƒå˜é‡åŠ è½½åˆ°å½“å‰çš„è¿è¡Œç¯å¢ƒä¸­  \n",
    "# å¦‚æœä½ è®¾ç½®çš„æ˜¯å…¨å±€çš„ç¯å¢ƒå˜é‡ï¼Œè¿™è¡Œä»£ç åˆ™æ²¡æœ‰ä»»ä½•ä½œç”¨ã€‚\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\learn-llm-rag-easily\\\\.env'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-b03a5d47f1094751ac79560dcf91ddd0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key=os.environ[\"DEEPSEEK_API_KEY\"]\n",
    "\n",
    "api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å‚è€ƒä½¿ç”¨ ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPTï¼Œå‘å¸ƒäº 2022 å¹´ 11 æœˆï¼Œæ˜¯ç›®å‰ç«çƒ­å‡ºåœˆçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼ŒLLMï¼‰çš„ä»£è¡¨äº§å“ã€‚åœ¨ 2022 å¹´åº•ï¼Œä¹Ÿæ­£æ˜¯ ChatGPT çš„æƒŠäººè¡¨ç°å¼•å‘äº† LLM çš„çƒ­æ½®ã€‚æ—¶è‡³ç›®å‰ï¼Œç”± OpenAI å‘å¸ƒçš„ GPT-4 ä»ç„¶æ˜¯ LLM æ€§èƒ½ä¸Šé™çš„ä»£è¡¨ï¼ŒChatGPT ä¹Ÿä»ç„¶æ˜¯ç›®å‰ä½¿ç”¨äººæ•°æœ€å¤šã€ä½¿ç”¨çƒ­åº¦æœ€å¤§ã€æœ€å…·å‘å±•æ½œåŠ›çš„ LLM äº§å“ã€‚äº‹å®ä¸Šï¼Œåœ¨åœˆå¤–äººçœ‹æ¥ï¼ŒChatGPT å³æ˜¯ LLM çš„ä»£ç§°ã€‚\n",
    "\n",
    "OpenAI é™¤å‘å¸ƒäº†å…è´¹çš„ Web ç«¯äº§å“å¤–ï¼Œä¹Ÿæä¾›äº†å¤šç§ ChatGPT APIï¼Œæ”¯æŒå¼€å‘è€…é€šè¿‡ Python æˆ– Request è¯·æ±‚æ¥è°ƒç”¨ ChatGPTï¼Œå‘è‡ªå·±çš„æœåŠ¡ä¸­åµŒå…¥ LLM çš„å¼ºå¤§èƒ½åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è°ƒç”¨ OpenAI API**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è°ƒç”¨ ChatGPT éœ€è¦ä½¿ç”¨ [ChatCompletion API](https://platform.openai.com/docs/api-reference/chat)ï¼Œè¯¥ API æä¾›äº† ChatGPT ç³»åˆ—æ¨¡å‹çš„è°ƒç”¨ï¼ŒåŒ…æ‹¬ ChatGPT-3.5ï¼ŒGPT-4 ç­‰ã€‚\n",
    "\n",
    "ChatCompletion API è°ƒç”¨æ–¹æ³•å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    # api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    api_key='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'\n",
    ")\n",
    "\n",
    "\n",
    "# å¯¼å…¥æ‰€éœ€åº“\n",
    "# æ³¨æ„ï¼Œæ­¤å¤„æˆ‘ä»¬å‡è®¾ä½ å·²æ ¹æ®ä¸Šæ–‡é…ç½®äº† OpenAI API Keyï¼Œå¦‚æ²¡æœ‰å°†è®¿é—®å¤±è´¥\n",
    "completion = client.chat.completions.create(\n",
    "    # è°ƒç”¨æ¨¡å‹ï¼šChatGPT-3.5\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    # messages æ˜¯å¯¹è¯åˆ—è¡¨\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è°ƒç”¨è¯¥ API ä¼šè¿”å›ä¸€ä¸ª ChatCompletion å¯¹è±¡ï¼Œå…¶ä¸­åŒ…æ‹¬äº†å›ç­”æ–‡æœ¬ã€åˆ›å»ºæ—¶é—´ã€id ç­‰å±æ€§ã€‚æˆ‘ä»¬ä¸€èˆ¬éœ€è¦çš„æ˜¯å›ç­”æ–‡æœ¬ï¼Œä¹Ÿå°±æ˜¯å›ç­”å¯¹è±¡ä¸­çš„ content ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­¤å¤„æˆ‘ä»¬è¯¦ç»†ä»‹ç»è°ƒç”¨ API å¸¸ä¼šç”¨åˆ°çš„å‡ ä¸ªå‚æ•°ï¼š\n",
    "\n",
    "    Â· modelï¼Œå³è°ƒç”¨çš„æ¨¡å‹ï¼Œä¸€èˆ¬å–å€¼åŒ…æ‹¬â€œgpt-3.5-turboâ€ï¼ˆChatGPT-3.5ï¼‰ã€â€œgpt-3.5-turbo-16k-0613â€ï¼ˆChatGPT-3.5 16K ç‰ˆæœ¬ï¼‰ã€â€œgpt-4â€ï¼ˆChatGPT-4ï¼‰ã€‚æ³¨æ„ï¼Œä¸åŒæ¨¡å‹çš„æˆæœ¬æ˜¯ä¸ä¸€æ ·çš„ã€‚\n",
    "\n",
    "    Â· messagesï¼Œå³æˆ‘ä»¬çš„ promptã€‚ChatCompletion çš„ messages éœ€è¦ä¼ å…¥ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­åŒ…æ‹¬å¤šä¸ªä¸åŒè§’è‰²çš„ promptã€‚æˆ‘ä»¬å¯ä»¥é€‰æ‹©çš„è§’è‰²ä¸€èˆ¬åŒ…æ‹¬ systemï¼šå³å‰æ–‡ä¸­æåˆ°çš„ system promptï¼›userï¼šç”¨æˆ·è¾“å…¥çš„ promptï¼›assistantï¼šåŠ©æ‰‹ï¼Œä¸€èˆ¬æ˜¯æ¨¡å‹å†å²å›å¤ï¼Œä½œä¸ºæä¾›ç»™æ¨¡å‹çš„å‚è€ƒå†…å®¹ã€‚\n",
    "\n",
    "    Â· temperatureï¼Œæ¸©åº¦ã€‚å³å‰æ–‡ä¸­æåˆ°çš„ Temperature ç³»æ•°ã€‚\n",
    "\n",
    "    Â· max_tokensï¼Œæœ€å¤§ token æ•°ï¼Œå³æ¨¡å‹è¾“å‡ºçš„æœ€å¤§ token æ•°ã€‚OpenAI è®¡ç®— token æ•°æ˜¯åˆå¹¶è®¡ç®— Prompt å’Œ Completion çš„æ€» token æ•°ï¼Œè¦æ±‚æ€» token æ•°ä¸èƒ½è¶…è¿‡æ¨¡å‹ä¸Šé™ï¼ˆå¦‚é»˜è®¤æ¨¡å‹ token ä¸Šé™ä¸º 4096ï¼‰ã€‚å› æ­¤ï¼Œå¦‚æœè¾“å…¥çš„ prompt è¾ƒé•¿ï¼Œéœ€è¦è®¾ç½®è¾ƒå¤§çš„ max_token å€¼ï¼Œå¦åˆ™ä¼šæŠ¥é”™è¶…å‡ºé™åˆ¶é•¿åº¦ã€‚\n",
    "\n",
    "OpenAI æä¾›äº†å……åˆ†çš„è‡ªå®šä¹‰ç©ºé—´ï¼Œæ”¯æŒæˆ‘ä»¬é€šè¿‡è‡ªå®šä¹‰ prompt æ¥æå‡æ¨¡å‹å›ç­”æ•ˆæœï¼Œå¦‚ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„å°è£… OpenAI æ¥å£çš„å‡½æ•°ï¼Œæ”¯æŒæˆ‘ä»¬ç›´æ¥ä¼ å…¥ prompt å¹¶è·å¾—æ¨¡å‹çš„è¾“å‡ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    # api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    api_key='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'\n",
    ")\n",
    "\n",
    "\n",
    "def gen_gpt_messages(prompt):\n",
    "    '''\n",
    "    æ„é€  GPT æ¨¡å‹è¯·æ±‚å‚æ•° messages\n",
    "    \n",
    "    è¯·æ±‚å‚æ•°ï¼š\n",
    "        prompt: å¯¹åº”çš„ç”¨æˆ·æç¤ºè¯\n",
    "    '''\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature = 0):\n",
    "    '''\n",
    "    è·å– GPT æ¨¡å‹è°ƒç”¨ç»“æœ\n",
    "\n",
    "    è¯·æ±‚å‚æ•°ï¼š\n",
    "        prompt: å¯¹åº”çš„æç¤ºè¯\n",
    "        model: è°ƒç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ä¸º gpt-3.5-turboï¼Œä¹Ÿå¯ä»¥æŒ‰éœ€é€‰æ‹© gpt-4 ç­‰å…¶ä»–æ¨¡å‹\n",
    "        temperature: æ¨¡å‹è¾“å‡ºçš„æ¸©åº¦ç³»æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºç¨‹åº¦ï¼Œå–å€¼èŒƒå›´æ˜¯ 0~2ã€‚æ¸©åº¦ç³»æ•°è¶Šä½ï¼Œè¾“å‡ºå†…å®¹è¶Šä¸€è‡´ã€‚\n",
    "    '''\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=gen_gpt_messages(prompt),\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    if len(response.choices) > 0:\n",
    "        return response.choices[0].message.content\n",
    "    return \"generate answer error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_completion(\"ä½ å¥½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨ä¸Šè¿°å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬å°è£…äº† messages çš„ç»†èŠ‚ï¼Œä»…ä½¿ç”¨ user prompt æ¥å®ç°è°ƒç”¨ã€‚åœ¨ç®€å•åœºæ™¯ä¸­ï¼Œè¯¥å‡½æ•°è¶³å¤Ÿæ»¡è¶³ä½¿ç”¨éœ€æ±‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨æ™ºè°± GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ™ºè°± AI æ˜¯ç”±æ¸…åå¤§å­¦è®¡ç®—æœºç³»æŠ€æœ¯æˆæœè½¬åŒ–è€Œæ¥çš„å…¬å¸ï¼Œè‡´åŠ›äºæ‰“é€ æ–°ä¸€ä»£è®¤çŸ¥æ™ºèƒ½é€šç”¨æ¨¡å‹ã€‚å…¬å¸åˆä½œç ”å‘äº†åŒè¯­åƒäº¿çº§è¶…å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ GLM-130Bï¼Œå¹¶æ„å»ºäº†é«˜ç²¾åº¦é€šç”¨çŸ¥è¯†å›¾è°±ï¼Œå½¢æˆæ•°æ®ä¸çŸ¥è¯†åŒè½®é©±åŠ¨çš„è®¤çŸ¥å¼•æ“ï¼ŒåŸºäºæ­¤æ¨¡å‹æ‰“é€ äº† ChatGLMï¼ˆchatglm.cnï¼‰ã€‚\n",
    "\n",
    "ChatGLM ç³»åˆ—æ¨¡å‹ï¼ŒåŒ…æ‹¬ ChatGLM-130Bã€ChatGLM-6B å’Œ ChatGLM2-6Bï¼ˆChatGLM-6B çš„å‡çº§ç‰ˆæœ¬ï¼‰æ¨¡å‹ï¼Œæ”¯æŒç›¸å¯¹å¤æ‚çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶ä¸”èƒ½å¤Ÿè§£å†³å›°éš¾çš„æ¨ç†ç±»é—®é¢˜ã€‚å…¶ä¸­ï¼ŒChatGLM-6B æ¨¡å‹æ¥è‡ª Huggingface ä¸Šçš„ä¸‹è½½é‡å·²ç»è¶…è¿‡ 300wï¼ˆæˆªè‡³ 2023 å¹´ 6 æœˆ 24 æ—¥ç»Ÿè®¡æ•°æ®ï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨ Hugging Face (HF) å…¨çƒå¤§æ¨¡å‹ä¸‹è½½æ¦œä¸­è¿ç»­ 12 å¤©ä½å±…ç¬¬ä¸€åï¼Œåœ¨å›½å†…å¤–çš„å¼€æºç¤¾åŒºä¸­äº§ç”Ÿäº†è¾ƒå¤§çš„å½±å“ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è°ƒç”¨æ™ºè°± GLM API**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ™ºè°± AI æä¾›äº† SDK å’ŒåŸç”Ÿ HTTP æ¥å®ç°æ¨¡å‹ API çš„è°ƒç”¨ï¼Œå»ºè®®ä½¿ç”¨ SDK è¿›è¡Œè°ƒç”¨ä»¥è·å¾—æ›´å¥½çš„ç¼–ç¨‹ä½“éªŒã€‚\n",
    "\n",
    "é¦–å…ˆæˆ‘ä»¬éœ€è¦é…ç½®å¯†é’¥ä¿¡æ¯ï¼Œå°†å‰é¢è·å–åˆ°çš„ `API key` è®¾ç½®åˆ° `.env` æ–‡ä»¶ä¸­çš„ `ZHIPUAI_API_KEY` å‚æ•°ï¼Œç„¶åè¿è¡Œä»¥ä¸‹ä»£ç åŠ è½½é…ç½®ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# è¯»å–æœ¬åœ°/é¡¹ç›®çš„ç¯å¢ƒå˜é‡ã€‚\n",
    "\n",
    "# find_dotenv() å¯»æ‰¾å¹¶å®šä½ .env æ–‡ä»¶çš„è·¯å¾„\n",
    "# load_dotenv() è¯»å–è¯¥ .env æ–‡ä»¶ï¼Œå¹¶å°†å…¶ä¸­çš„ç¯å¢ƒå˜é‡åŠ è½½åˆ°å½“å‰çš„è¿è¡Œç¯å¢ƒä¸­  \n",
    "# å¦‚æœä½ è®¾ç½®çš„æ˜¯å…¨å±€çš„ç¯å¢ƒå˜é‡ï¼Œè¿™è¡Œä»£ç åˆ™æ²¡æœ‰ä»»ä½•ä½œç”¨ã€‚\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\learn-llm-rag-easily\\\\.env'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"ZHIPUAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ™ºè°±çš„è°ƒç”¨ä¼ å‚å’Œå…¶ä»–ç±»ä¼¼ï¼Œä¹Ÿéœ€è¦ä¼ å…¥ä¸€ä¸ª messages åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­åŒ…æ‹¬ role å’Œ promptã€‚æˆ‘ä»¬å°è£…å¦‚ä¸‹çš„ `get_completion` å‡½æ•°ï¼Œä¾›åç»­ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai import ZhipuAI\n",
    "\n",
    "client = ZhipuAI(\n",
    "    api_key=os.environ[\"ZHIPUAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "def gen_glm_params(prompt):\n",
    "    '''\n",
    "    æ„é€  GLM æ¨¡å‹è¯·æ±‚å‚æ•° messages\n",
    "\n",
    "    è¯·æ±‚å‚æ•°ï¼š\n",
    "        prompt: å¯¹åº”çš„ç”¨æˆ·æç¤ºè¯\n",
    "    '''\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def get_completion(prompt, model=\"glm-4\", temperature=0.95):\n",
    "    '''\n",
    "    è·å– GLM æ¨¡å‹è°ƒç”¨ç»“æœ\n",
    "\n",
    "    è¯·æ±‚å‚æ•°ï¼š\n",
    "        prompt: å¯¹åº”çš„æç¤ºè¯\n",
    "        model: è°ƒç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ä¸º glm-4ï¼Œä¹Ÿå¯ä»¥æŒ‰éœ€é€‰æ‹© glm-3-turbo ç­‰å…¶ä»–æ¨¡å‹\n",
    "        temperature: æ¨¡å‹è¾“å‡ºçš„æ¸©åº¦ç³»æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºç¨‹åº¦ï¼Œå–å€¼èŒƒå›´æ˜¯ 0~1.0ï¼Œä¸”ä¸èƒ½è®¾ç½®ä¸º 0ã€‚æ¸©åº¦ç³»æ•°è¶Šä½ï¼Œè¾“å‡ºå†…å®¹è¶Šä¸€è‡´ã€‚\n",
    "    '''\n",
    "\n",
    "    messages = gen_glm_params(prompt)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    if len(response.choices) > 0:\n",
    "        return response.choices[0].message.content\n",
    "    return \"generate answer error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹æ™ºè°±æ¸…è¨€ï¼Œå¯ä»¥å«æˆ‘å°æ™ºğŸ¤–ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"ä½ å¥½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œå¯¹ä¼ å…¥ zhipuai çš„å‚æ•°è¿›è¡Œç®€å•ä»‹ç»ï¼š\n",
    "\n",
    "- `messages (list)`ï¼Œè°ƒç”¨å¯¹è¯æ¨¡å‹æ—¶ï¼Œå°†å½“å‰å¯¹è¯ä¿¡æ¯åˆ—è¡¨ä½œä¸ºæç¤ºè¾“å…¥ç»™æ¨¡å‹ï¼›æŒ‰ç…§ {\"role\": \"user\", \"content\": \"ä½ å¥½\"} çš„é”®å€¼å¯¹å½¢å¼è¿›è¡Œä¼ å‚ï¼›æ€»é•¿åº¦è¶…è¿‡æ¨¡å‹æœ€é•¿è¾“å…¥é™åˆ¶åä¼šè‡ªåŠ¨æˆªæ–­ï¼Œéœ€æŒ‰æ—¶é—´ç”±æ—§åˆ°æ–°æ’åº\n",
    "\n",
    "- `temperature (float)`ï¼Œé‡‡æ ·æ¸©åº¦ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼Œå¿…é¡»ä¸ºæ­£æ•°å–å€¼èŒƒå›´æ˜¯ï¼š(0.0, 1.0)ï¼Œä¸èƒ½ç­‰äº 0ï¼Œé»˜è®¤å€¼ä¸º 0.95ã€‚å€¼è¶Šå¤§ï¼Œä¼šä½¿è¾“å‡ºæ›´éšæœºï¼Œæ›´å…·åˆ›é€ æ€§ï¼›å€¼è¶Šå°ï¼Œè¾“å‡ºä¼šæ›´åŠ ç¨³å®šæˆ–ç¡®å®š\n",
    "  \n",
    "- `top_p (float)`ï¼Œç”¨æ¸©åº¦å–æ ·çš„å¦ä¸€ç§æ–¹æ³•ï¼Œç§°ä¸ºæ ¸å–æ ·ã€‚å–å€¼èŒƒå›´æ˜¯ï¼š(0.0, 1.0) å¼€åŒºé—´ï¼Œä¸èƒ½ç­‰äº 0 æˆ– 1ï¼Œé»˜è®¤å€¼ä¸º 0.7ã€‚æ¨¡å‹è€ƒè™‘å…·æœ‰ top_p æ¦‚ç‡è´¨é‡ tokens çš„ç»“æœã€‚ä¾‹å¦‚ï¼š0.1 æ„å‘³ç€æ¨¡å‹è§£ç å™¨åªè€ƒè™‘ä»å‰ 10% çš„æ¦‚ç‡çš„å€™é€‰é›†ä¸­å– tokens\n",
    "\n",
    "- `request_id (string)`ï¼Œç”±ç”¨æˆ·ç«¯ä¼ å‚ï¼Œéœ€ä¿è¯å”¯ä¸€æ€§ï¼›ç”¨äºåŒºåˆ†æ¯æ¬¡è¯·æ±‚çš„å”¯ä¸€æ ‡è¯†ï¼Œç”¨æˆ·ç«¯ä¸ä¼ æ—¶å¹³å°ä¼šé»˜è®¤ç”Ÿæˆ\n",
    "\n",
    "- **å»ºè®®æ‚¨æ ¹æ®åº”ç”¨åœºæ™¯è°ƒæ•´ top_p æˆ– temperature å‚æ•°ï¼Œä½†ä¸è¦åŒæ—¶è°ƒæ•´ä¸¤ä¸ªå‚æ•°**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai import ZhipuAI\n",
    "client = ZhipuAI(api_key=os.environ[\"ZHIPUAI_API_KEY\"]) \n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4\",  # å¡«å†™éœ€è¦è°ƒç”¨çš„æ¨¡å‹ç¼–ç \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªä¹äºè§£ç­”å„ç§é—®é¢˜çš„åŠ©æ‰‹ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä¸ºç”¨æˆ·æä¾›ä¸“ä¸šã€å‡†ç¡®ã€æœ‰è§åœ°çš„å»ºè®®ã€‚\"},\n",
    "        {\"role\": \"user\", \"content\": \"å†œå¤«éœ€è¦æŠŠç‹¼ã€ç¾Šå’Œç™½èœéƒ½å¸¦è¿‡æ²³ï¼Œä½†æ¯æ¬¡åªèƒ½å¸¦ä¸€æ ·ç‰©å“ï¼Œè€Œä¸”ç‹¼å’Œç¾Šä¸èƒ½å•ç‹¬ç›¸å¤„ï¼Œç¾Šå’Œç™½èœä¹Ÿä¸èƒ½å•ç‹¬ç›¸å¤„ï¼Œé—®å†œå¤«è¯¥å¦‚ä½•è¿‡æ²³ã€‚\"}\n",
    "    ],\n",
    ")\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "myllm = ChatOpenAI(\n",
    "    temperature=0.95,\n",
    "    model=\"glm-4\",\n",
    "    openai_api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\",\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "myllm.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯DeepSeek Chatï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ çš„æ™ºèƒ½åŠ©æ‰‹ï¼ğŸ¤–âœ¨ æˆ‘å¯ä»¥å¸®ä½ è§£ç­”å„ç§é—®é¢˜ã€æä¾›ä¿¡æ¯ã€é™ªä½ èŠå¤©ï¼Œç”šè‡³å¸®ä½ å¤„ç†æ–‡æ¡£ã€‚å¦‚æœæœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œå°½ç®¡é—®æˆ‘å§ï¼ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# Please install OpenAI SDK first: `pip3 install openai`\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"sk-c93efe07b0ef4445baca2cd28f54cb78\", base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"ä½ æ˜¯è°ï¼Ÿ\"},\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_deepseek'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv, find_dotenv\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_deepseek\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatDeepSeek\n\u001b[32m      5\u001b[39m _ = load_dotenv(find_dotenv())    \u001b[38;5;66;03m# read local .env file\u001b[39;00m\n\u001b[32m      6\u001b[39m deepseek_api_key = os.environ[\u001b[33m'\u001b[39m\u001b[33mDEEPSEEK_API_KEY\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_deepseek'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "_ = load_dotenv(find_dotenv())    # read local .env file\n",
    "deepseek_api_key = os.environ['DEEPSEEK_API_KEY']\n",
    "llm_deepseek = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    # max_retries=2,\n",
    "    api_key=deepseek_api_key,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "llm_deepseek.invoke(\"ä½ æ˜¯è°ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ æœ¬åœ°å¼€æºLLM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è°ƒç”¨ollamaéƒ¨ç½²çš„å¤§æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è°ƒç”¨æœ¬åœ°å¤§æ¨¡å‹ï¼ŒåŒæ ·å¯ä»¥ä½¿ç”¨ [ChatCompletion API](https://platform.openai.com/docs/api-reference/chat)ï¼Œè¯¥ API æä¾›äº† ChatGPT ç³»åˆ—æ¨¡å‹çš„è°ƒç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nå¥½ï¼Œç”¨æˆ·é—®çš„æ˜¯â€œå¤§è¯­è¨€æ¨¡å‹â€æ˜¯ä»€ä¹ˆã€‚æˆ‘å¾—å…ˆè§£é‡Šä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Œä»¥åŠå®ƒä»¬çš„ä½œç”¨å’Œç‰¹ç‚¹ã€‚\\n\\né¦–å…ˆï¼Œå¤§è¯­è¨€æ¨¡å‹åº”è¯¥æ˜¯æŒ‡èƒ½å¤Ÿç†è§£æˆ–ç”Ÿæˆäººç±»è¯­è¨€çš„æ™ºèƒ½æœºå™¨ã€‚è¿™äº›æ¨¡å‹é€šå¸¸ç”¨ç®—æ³•æ¥æ¨¡æ‹Ÿäººç±»çš„æ€ç»´æ¨¡å¼ï¼Œå¤„ç†å„ç§è¯­è¨€ä»»åŠ¡ã€‚\\n\\nç„¶åï¼Œæˆ‘éœ€è¦åˆ—å‡ºä¸€äº›ä¸»è¦çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œæ¯”å¦‚Googleçš„DeepMindã€ChatGPTå’Œå…¶ä»–ä¼ä¸šå¸¸ç”¨çš„å¦‚Anthropicçš„Bardç­‰ã€‚æåˆ°è¿™äº›æœ‰åŠ©äºå±•ç¤ºæ¦‚å¿µçš„å¹¿æ³›æ€§ã€‚\\n\\næ¥ä¸‹æ¥è¦è¯´æ˜å®ƒä»¬çš„åŸºæœ¬å·¥ä½œåŸç†ï¼ŒåŒ…æ‹¬æ•°æ®è®­ç»ƒã€æ¶æ„è®¾è®¡ç­‰ç­‰ã€‚è¿™èƒ½å¸®åŠ©ç”¨æˆ·ç†è§£ä»–ä»¬çš„è¿ä½œæœºåˆ¶ã€‚\\n\\nè¿˜è¦æåˆ°å¤§è¯­è¨€æ¨¡å‹åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨ï¼Œåˆ†ä¸ºç«¯åˆ°ç«¯å¤„ç†å’Œå¯¹è¯è¾…åŠ©ä¸¤å¤§ç±»ï¼Œè¿™æ ·ç”¨æˆ·å¯ä»¥æ›´å…¨é¢åœ°äº†è§£åˆ°å®ƒä»¬çš„ä»·å€¼ã€‚\\n\\næœ€åï¼Œæ€»ç»“ä¸€ä¸‹ä¸ºä»€ä¹ˆç”¨æˆ·ä¼šå¯¹å¤§è¯­è¨€æ¨¡å‹æ„Ÿå…´è¶£ï¼Œæ¯”å¦‚å®ƒå¦‚ä½•æé«˜æ•ˆç‡ã€å¢å¼ºAIèƒ½åŠ›ï¼Œç”šè‡³è§£å†³å¾ˆå¤šå®é™…é—®é¢˜ã€‚è¿™èƒ½æ‹‰è¿‘ç”¨æˆ·ä¸æˆ‘ä¹‹é—´çš„è”ç³»ï¼Œæ¿€å‘è¿›ä¸€æ­¥æ¢è®¨çš„å…´è¶£ã€‚\\n</think>\\n\\nå¤§è¯­è¨€æ¨¡å‹ï¼ˆLSTMï¼‰æ˜¯ä¸€ç§èƒ½å¤Ÿç†è§£å’Œæ¨¡æ‹Ÿäººç±» mind çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œç”¨äºå¤„ç†è¯­è¨€ç›¸å…³çš„ä»»åŠ¡ã€‚è¿™äº›æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆã€è‡ªç„¶è¯­è¨€ç†è§£ã€é—®ç­”å’Œå¯¹è¯ç­‰é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºå„ç§åº”ç”¨ä¸­ã€‚\\n\\n### å®šä¹‰\\nå¤§è¯­è¨€æ¨¡å‹æ˜¯åŸºäºç¥ç»ç½‘ç»œæŠ€æœ¯å¼€å‘çš„æ™ºèƒ½ç”Ÿç‰©ä½“ï¼Œèƒ½å¤Ÿé€šè¿‡è®¡ç®—æœºç¨‹åºæ¥æ‰§è¡Œå¤æ‚çš„è¯­è¨€æ“ä½œå’Œæ¨ç†ä»»åŠ¡ã€‚å®ƒä»¬çš„å·¥ä½œåŸç†ä¸äººç±»çš„å¤§è„‘ç¥ç»å…ƒåŠŸèƒ½ç›¸ä¼¼ï¼Œå¯ä»¥é€šè¿‡å¤§é‡è®­ç»ƒæ•°æ®å­¦ä¹ è¯­è¨€ç›¸å…³è§„åˆ™ã€‚\\n\\n### åŸºæœ¬å·¥ä½œåŸç†\\n1. **æ•°æ®è®­ç»ƒ**ï¼šå¤§è¯­è¨€æ¨¡å‹é€šå¸¸é€šè¿‡å¤§é‡çš„æ–‡æœ¬æ•°æ®æ¥è®­ç»ƒï¼Œè¿™äº›æ•°æ®å¯ä»¥åŒ…å«è‡ªç„¶è¯­è¨€ã€å­¦æœ¯è®ºæ–‡ã€æ–°é—»æŠ¥é“ç­‰ã€‚\\n2. **æƒ…æ„Ÿç†è§£**ï¼šä¸€äº›æ¨¡å‹è¿˜èƒ½å¤„ç†è·¨æ–‡åŒ–æˆ–å¤šè¯­è¨€çš„å¯¹è¯ï¼Œè¡¨ç°å‡ºå¯¹ä¸åŒè¯­è¨€çš„ç†è§£èƒ½åŠ›ã€‚\\n3. **ç”ŸæˆåŠŸèƒ½**ï¼šå®ƒä»¬èƒ½å¤ŸæŒ‰ç…§äººç±»çš„è¯­è¨€ç»“æ„è¿›è¡Œæ–‡æœ¬åˆæˆï¼Œè¾“å‡ºå®Œæ•´çš„å¥å­æˆ–æ®µè½ã€‚\\n\\n### ç‰¹å¾\\n1. **å¤„ç†å¹¿æ³›**ï¼šå¤§è¯­è¨€æ¨¡å‹æ”¯æŒå¤šç§ä¸Šä¸‹æ–‡ä¿¡æ¯å¤„ç†ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€ç¿»è¯‘ï¼ˆä¸­è‹±æ–‡ï¼‰å’Œæé—®ï¼ˆé—®ç­”ç³»ç»Ÿï¼‰ã€‚\\n2. **å¯¹è¯ç•Œé¢**ï¼šè®¸å¤šæ¨¡å‹æä¾›äº†ç«¯åˆ°ç«¯çš„å¯¹è¯ç³»ç»Ÿï¼Œç”¨æˆ·å¯ä»¥è¾“å…¥é—®é¢˜ï¼Œæ¨¡å‹è¾“å‡ºç›¸åº”çš„å›ç­”ã€‚\\n\\n### åº”ç”¨é¢†åŸŸ\\n1. ç§‘å­¦æ–‡çŒ®æœç´¢ä¸æ‘˜è¦ç”Ÿæˆï¼Œæ”¯æŒå­¦æœ¯ç ”ç©¶åŠ©æ‰‹çš„åŠŸèƒ½ã€‚\\n2. æ–‡æ¡£ç†è§£ä¸è‡ªåŠ¨å›å¤ï¼Œå¦‚å®æ—¶æ–‡æ¡£åˆ†æã€æ™ºèƒ½é—®ç­”ç­‰ã€‚\\n3. åŸºæœ¬çš„ä¸­æ–‡ç¿»è¯‘åŠŸèƒ½ï¼ˆå¦‚ä¸­è‹±ç¿»è¯‘ï¼‰ã€‚\\n\\nå¤§è¯­è¨€æ¨¡å‹ä»¥å…¶å¼ºå¤§çš„å¯¹è¯èƒ½åŠ›ä¸å¼ºå¤§çš„æ•°æ®å¤„ç†èƒ½åŠ›ï¼Œæ­£åœ¨æ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ã€‚'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1/',\n",
    "    api_key = 'ollama'\n",
    ")\n",
    "\n",
    "prompt = 'å¤§è¯­è¨€æ¨¡å‹æ˜¯ä»€ä¹ˆï¼Ÿ'\n",
    "messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "response = client.chat.completions.create(\n",
    "    model = 'deepseek-r1:1.5b',\n",
    "    messages = messages,\n",
    "    temperature=0.95,\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\n\\n</think>\\n\\nä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯é—®é¢˜ã€å»ºè®®è¿˜æ˜¯é—²èŠï¼Œæˆ‘éƒ½åœ¨è¿™å„¿ä¸ºä½ æœåŠ¡ã€‚ğŸ˜Š'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "my_llm = OllamaLLM(base_url='http://localhost:11434', model='deepseek-r1:1.5b', temperature=0.1)\n",
    "my_llm.invoke(\"ä½ å¥½\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\will\\AppData\\Local\\Temp\\ipykernel_20492\\2215453771.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(base_url='http://localhost:11434', model='deepseek-r1:1.5b')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<think>\\n\\n</think>\\n\\nä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯ç”Ÿæ´»ä¸­çš„é—®é¢˜ï¼Œéƒ½å¯ä»¥å‘Šè¯‰æˆ‘å“¦ï¼ğŸ˜Š'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "my_llm = Ollama(base_url='http://localhost:11434', model='deepseek-r1:1.5b')\n",
    "\n",
    "response = my_llm.invoke(\"ä½ å¥½\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è°ƒç”¨vllméƒ¨ç½²çš„å¤§æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response: ChatCompletion(id='chatcmpl-21421752cdcd48b7b1ed53379a4fddff', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Alright, so the user asked me to tell them a joke. I need to respond in a way that\\'s helpful and engaging. Let me think of a good joke that\\'s a bit light-hearted. Maybe something about animals since that\\'s a popular topic. \\n\\nI remember a classic joke about bees and honey. It goes like this: Why do bees build hives in the spring? Because they\\'re looking for a sweet deal! That\\'s a simple and funny one. It\\'s a play on words with \"sweet deal\" sounding like \"hives\" and \"spring\" sounding like \"hives\" too.\\n\\nI should make sure the joke is clear and the pun works well. It should be easy to understand and bring a smile. I\\'ll go ahead and provide the joke as requested. I don\\'t want to overcomplicate it. Keeping it simple is better for humor.\\n</think>\\n\\nWhy do bees build hives in the spring?  \\nBecause they\\'re looking for a sweet deal!', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None)], created=1743494974, model='deepseek-r1-distill-qwen-7b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=200, prompt_tokens=16, total_tokens=216, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"token-abc123\"\n",
    "openai_api_base = \"http://localhost:8081/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"deepseek-r1-distill-qwen-14b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "    ]\n",
    ")\n",
    "print(\"Chat response:\", chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLMOpenAI  # æ³¨æ„ç±»åä¸º VLLMOpenAI[3](@ref)\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_key=\"token-abc123\",          # vLLM æ— éœ€é‰´æƒï¼Œè®¾ä¸ºç©ºå­—ç¬¦ä¸²[3](@ref)\n",
    "    openai_api_base=\"deepseek-r1-distill-qwen-14b\",  # æœåŠ¡ç«¯åœ°å€\n",
    "    model_name=\"deepseek-r1-1.5b\",  # éœ€ä¸éƒ¨ç½²çš„æ¨¡å‹è·¯å¾„ä¸€è‡´\n",
    "    max_tokens=1024,                # æ§åˆ¶ç”Ÿæˆæ–‡æœ¬æœ€å¤§é•¿åº¦\n",
    "    temperature=0,               # ç”Ÿæˆå¤šæ ·æ€§å‚æ•°ï¼ˆ0~1ï¼‰\n",
    "    # top_p=0.9,                      # é‡‡æ ·é˜ˆå€¼\n",
    "    streaming=True                  # æ”¯æŒæµå¼è¾“å‡ºï¼ˆå¯é€‰ï¼‰\n",
    ")\n",
    "response = llm.invoke(\"ä½ æ˜¯è°ï¼Ÿ\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è°ƒç”¨xinferenceéƒ¨ç½²çš„å¤§æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Assume that the model is already launched.\n",
    "# The api_key can't be empty, any string is OK.\n",
    "client = openai.Client(api_key=\"not empty\", base_url=\"http://localhost:9997/v1\")\n",
    "client.chat.completions.create(\n",
    "    model=model_uid,\n",
    "    messages=[\n",
    "        {\n",
    "            \"content\": \"What is the largest animal?\",\n",
    "            \"role\": \"user\",\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xinference list æŸ¥çœ‹å·²å®‰è£…æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Xinference\n",
    "\n",
    "llm = Xinference(\n",
    "    server_url=\"http://localhost:9997\",\n",
    "    model_uid = {model_uid} # replace model_uid with the model UID return from launching the model\n",
    ")\n",
    "\n",
    "llm.invoke(\n",
    "    prompt=\"Q: where can we visit in the capital of France? A:\",\n",
    "    generate_config={\"max_tokens\": 1024, \"stream\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨embedding API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥zhipuä¸ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "my_emb = ZhipuAIEmbeddings(\n",
    "    model=\"embedding-2\",\n",
    "    api_key=os.environ[\"ZHIPUAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨æœ¬åœ°éƒ¨ç½²çš„embeddingæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨HuggingFaceéƒ¨ç½²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\will\\AppData\\Local\\Temp\\ipykernel_1576\\602453622.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  my_emb = HuggingFaceEmbeddings(model_name='/opt/workspace/models/BAAI/bge-small-zh-v1.5')\n",
      "E:\\ProgramData\\Python312\\venv\\langchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Path /opt/workspace/models/BAAI/bge-small-zh-v1.5 not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhuggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m my_emb = \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/opt/workspace/models/BAAI/bge-small-zh-v1.5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33må¦‚ä½•è°ƒç”¨HuggingFaceEmbeddingsï¼Ÿ\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m query_vector = my_emb.embed_query(query)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\ProgramData\\Python312\\venv\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:214\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    213\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\ProgramData\\Python312\\venv\\langchain\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:92\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import sentence_transformers python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     89\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28mself\u001b[39m.client = \u001b[43msentence_transformers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\ProgramData\\Python312\\venv\\langchain\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:296\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(model_name_or_path):\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# Not a path, load from hub\u001b[39;00m\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mor\u001b[39;00m model_name_or_path.count(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPath \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mand\u001b[39;00m model_name_or_path.lower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m basic_transformer_models:\n\u001b[32m    299\u001b[39m         \u001b[38;5;66;03m# A model from sentence-transformers\u001b[39;00m\n\u001b[32m    300\u001b[39m         model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + model_name_or_path\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Path /opt/workspace/models/BAAI/bge-small-zh-v1.5 not found"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "my_emb = HuggingFaceEmbeddings(model_name='/opt/workspace/models/BAAI/bge-small-zh-v1.5')\n",
    "\n",
    "query = \"å¦‚ä½•è°ƒç”¨HuggingFaceEmbeddingsï¼Ÿ\"\n",
    "\n",
    "query_vector = my_emb.embed_query(query) \n",
    "\n",
    "print(\"æŸ¥è¯¢å‘é‡:\", query_vector[:5], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨ollamaéƒ¨ç½²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "my_emb = OllamaEmbeddings(base_url='http://localhost:11434',model=\"bge-m3:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æŸ¥è¯¢å‘é‡: [-0.017827146, -0.009976904, -0.03116692, 0.029771997, -0.024388244] ...\n",
      "æ–‡æ¡£å‘é‡åˆ—è¡¨: [[-0.029825423, -0.0006194668, -0.031351015, 0.02808682, -0.018649695], [0.002058215, 0.013199092, -0.020036552, 0.024129553, -0.003552613]] ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "my_emb = OllamaEmbeddings(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"bge-m3:latest\"\n",
    ")\n",
    "\n",
    "# ç¤ºä¾‹æ–‡æœ¬\n",
    "query = \"å¦‚ä½•è°ƒç”¨OllamaEmbeddingsï¼Ÿ\"\n",
    "documents = [\n",
    "    \"OllamaEmbeddings ç”¨äºç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡ã€‚\",\n",
    "    \"è°ƒç”¨æ–¹æ³•åŒ…æ‹¬ embed_query å’Œ embed_documentsã€‚\"\n",
    "]\n",
    "\n",
    "# ç”ŸæˆåµŒå…¥å‘é‡\n",
    "query_vector = my_emb.embed_query(query)\n",
    "doc_vectors = my_emb.embed_documents(documents)\n",
    "\n",
    "# è¾“å‡ºç»“æœ\n",
    "print(\"æŸ¥è¯¢å‘é‡:\", query_vector[:5], \"...\")\n",
    "print(\"æ–‡æ¡£å‘é‡åˆ—è¡¨:\", [vec[:5] for vec in doc_vectors], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨xinferenceéƒ¨ç½²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import XinferenceEmbeddings\n",
    "\n",
    "# æ›¿æ¢ä¸ºä½ çš„XinferenceæœåŠ¡å™¨URLå’Œæ¨¡å‹UID\n",
    "xinference = XinferenceEmbeddings(\n",
    "    server_url=\"http://localhost:9997\",  # æ³¨æ„ï¼šåŸä»£ç ä¸­çš„\"loaclhost\"æ‹¼å†™é”™è¯¯ï¼Œåº”ä¸º\"localhost\"\n",
    "    model_uid=\"your_model_uid\"  # æ›¿æ¢ä¸ºå®é™…çš„æ¨¡å‹UID\n",
    ")\n",
    "\n",
    "# è¾“å…¥æ–‡æœ¬\n",
    "texts = [\"ä½ å¥½ï¼Œä¸–ç•Œã€‚\", \"LangChain æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ã€‚\"]\n",
    "\n",
    "# ç”ŸæˆåµŒå…¥å‘é‡\n",
    "vectors = xinference.embed_documents(texts)\n",
    "\n",
    "# æ‰“å°ç»“æœ\n",
    "for idx, vector in enumerate(vectors):\n",
    "    print(f\"æ–‡æœ¬ {idx + 1}: {texts[idx]}\")\n",
    "    print(f\"åµŒå…¥å‘é‡: {vector[:5]}... (ç»´åº¦: {len(vector)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨reranker API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== é‡æ’åºåçš„Top 5æ–‡æ¡£ =====\n",
      "Rank 1 (Score: 0.3492):\n",
      "å†…å®¹ï¼šé¦–å°”æ˜¯éŸ©å›½çš„é¦–éƒ½ï¼Œä»¥ç°ä»£åŒ–ä¸ä¼ ç»Ÿæ–‡åŒ–çš„èåˆé—»åã€‚\n",
      "å…ƒæ•°æ®ï¼š{'source': 'travel', 'relevance_score': 0.34919977}\n",
      "--------------------------------------------------\n",
      "Rank 2 (Score: 0.3157):\n",
      "å†…å®¹ï¼šWashington D.C. is the capital of the United States.\n",
      "å…ƒæ•°æ®ï¼š{'source': 'edu', 'relevance_score': 0.31573597}\n",
      "--------------------------------------------------\n",
      "Rank 3 (Score: 0.2335):\n",
      "å†…å®¹ï¼šå·´é»æ˜¯æ³•å›½çš„é¦–éƒ½ï¼Œä¹Ÿæ˜¯è‘—åçš„è‰ºæœ¯æ–‡åŒ–ä¸­å¿ƒã€‚\n",
      "å…ƒæ•°æ®ï¼š{'source': 'wiki', 'relevance_score': 0.23353152}\n",
      "--------------------------------------------------\n",
      "Rank 4 (Score: 0.1166):\n",
      "å†…å®¹ï¼šä¸œäº¬æ˜¯æ—¥æœ¬æœ€å¤§çš„åŸå¸‚ï¼Œä¹Ÿæ˜¯å…¨çƒé‡è¦çš„ç»æµä¸­å¿ƒã€‚\n",
      "å…ƒæ•°æ®ï¼š{'source': 'news', 'relevance_score': 0.11656274}\n",
      "--------------------------------------------------\n",
      "Rank 5 (Score: 0.0390):\n",
      "å†…å®¹ï¼šåŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»å’Œæ–‡åŒ–ä¸­å¿ƒï¼Œæ‹¥æœ‰ç´«ç¦åŸç­‰å†å²å»ºç­‘ã€‚\n",
      "å…ƒæ•°æ®ï¼š{'source': 'gov', 'relevance_score': 0.03904829}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "import cohere\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "cohere_client = cohere.Client(api_key=\"Tahx1eySFbKvu9sTyTXrRLf59la3ZUG9vy02stRZ\")\n",
    "\n",
    "compressor = CohereRerank(\n",
    "    client=cohere_client,\n",
    "    top_n=3,\n",
    "    model=\"rerank-multilingual-v3.0\"  # æ”¯æŒå¤šè¯­è¨€çš„æ–°ç‰ˆæœ¬\n",
    ")\n",
    "\n",
    "# æµ‹è¯•æ ·ä¾‹æ•°æ®\n",
    "documents = [\n",
    "    Document(page_content=\"å·´é»æ˜¯æ³•å›½çš„é¦–éƒ½ï¼Œä¹Ÿæ˜¯è‘—åçš„è‰ºæœ¯æ–‡åŒ–ä¸­å¿ƒã€‚\", metadata={\"source\": \"wiki\"}),\n",
    "    Document(page_content=\"åŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»å’Œæ–‡åŒ–ä¸­å¿ƒï¼Œæ‹¥æœ‰ç´«ç¦åŸç­‰å†å²å»ºç­‘ã€‚\", metadata={\"source\": \"gov\"}),\n",
    "    Document(page_content=\"Capital punishment refers to the death penalty in legal systems.\", metadata={\"source\": \"law\"}),\n",
    "    Document(page_content=\"ä¸œäº¬æ˜¯æ—¥æœ¬æœ€å¤§çš„åŸå¸‚ï¼Œä¹Ÿæ˜¯å…¨çƒé‡è¦çš„ç»æµä¸­å¿ƒã€‚\", metadata={\"source\": \"news\"}),\n",
    "    Document(page_content=\"Washington D.C. is the capital of the United States.\", metadata={\"source\": \"edu\"}),\n",
    "    Document(page_content=\"é¦–å°”æ˜¯éŸ©å›½çš„é¦–éƒ½ï¼Œä»¥ç°ä»£åŒ–ä¸ä¼ ç»Ÿæ–‡åŒ–çš„èåˆé—»åã€‚\", metadata={\"source\": \"travel\"}),\n",
    "    Document(page_content=\"Capitalization in finance refers to the total value of a company's shares.\", metadata={\"source\": \"finance\"})\n",
    "]\n",
    "\n",
    "query = \"å„å›½é¦–éƒ½åŸå¸‚çš„ä»‹ç»æœ‰å“ªäº›ï¼Ÿ\"\n",
    "\n",
    "# æ‰§è¡Œé‡æ’åº\n",
    "compressed_docs = compressor.compress_documents(documents=documents, query=query)\n",
    "\n",
    "# æ‰“å°æ’åºç»“æœ\n",
    "print(\"===== é‡æ’åºåçš„Top 5æ–‡æ¡£ =====\")\n",
    "for i, doc in enumerate(compressed_docs):\n",
    "    print(f\"Rank {i+1} (Score: {doc.metadata['relevance_score']:.4f}):\")\n",
    "    print(f\"å†…å®¹ï¼š{doc.page_content}\")\n",
    "    print(f\"å…ƒæ•°æ®ï¼š{doc.metadata}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== é‡æ’åºåçš„Top 5æ–‡æ¡£ =====\n",
      "Rank 1 (Score: 0.2323):\n",
      "å†…å®¹ï¼šWashington D.C. is the capital of the United States.\n",
      "å…ƒæ•°æ®ï¼š{'source': 'edu', 'relevance_score': 0.23231014609336853}\n",
      "--------------------------------------------------\n",
      "Rank 2 (Score: 0.1541):\n",
      "å†…å®¹ï¼šå·´é»æ˜¯æ³•å›½çš„é¦–éƒ½ï¼Œä¹Ÿæ˜¯è‘—åçš„è‰ºæœ¯æ–‡åŒ–ä¸­å¿ƒã€‚\n",
      "å…ƒæ•°æ®ï¼š{'source': 'wiki', 'relevance_score': 0.154057577252388}\n",
      "--------------------------------------------------\n",
      "Rank 3 (Score: 0.1432):\n",
      "å†…å®¹ï¼šé¦–å°”æ˜¯éŸ©å›½çš„é¦–éƒ½ï¼Œä»¥ç°ä»£åŒ–ä¸ä¼ ç»Ÿæ–‡åŒ–çš„èåˆé—»åã€‚\n",
      "å…ƒæ•°æ®ï¼š{'source': 'travel', 'relevance_score': 0.14318770170211792}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_compressors import JinaRerank  # ä½¿ç”¨Jinaçš„rerankç»„ä»¶\n",
    "\n",
    "# Jina Reranké…ç½®\n",
    "JINA_API_KEY = \"jina_63bb115e2d5f42d581f42643294792b5CE4nrEINMDcT4vJZJaSLcr5tkbIB\"  # æ›¿æ¢ä¸ºä½ çš„Jina APIå¯†é’¥\n",
    "\n",
    "compressor = JinaRerank(\n",
    "    jina_api_key=JINA_API_KEY,\n",
    "    top_n=3,\n",
    "    model=\"jina-reranker-v2-base-multilingual\"  # Jinaçš„å¤šè¯­è¨€rerankæ¨¡å‹[5](@ref)\n",
    ")\n",
    "\n",
    "# æµ‹è¯•æ ·ä¾‹æ•°æ®\n",
    "documents = [\n",
    "    Document(page_content=\"å·´é»æ˜¯æ³•å›½çš„é¦–éƒ½ï¼Œä¹Ÿæ˜¯è‘—åçš„è‰ºæœ¯æ–‡åŒ–ä¸­å¿ƒã€‚\", metadata={\"source\": \"wiki\"}),\n",
    "    Document(page_content=\"åŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»å’Œæ–‡åŒ–ä¸­å¿ƒï¼Œæ‹¥æœ‰ç´«ç¦åŸç­‰å†å²å»ºç­‘ã€‚\", metadata={\"source\": \"gov\"}),\n",
    "    Document(page_content=\"Capital punishment refers to the death penalty in legal systems.\", metadata={\"source\": \"law\"}),\n",
    "    Document(page_content=\"ä¸œäº¬æ˜¯æ—¥æœ¬æœ€å¤§çš„åŸå¸‚ï¼Œä¹Ÿæ˜¯å…¨çƒé‡è¦çš„ç»æµä¸­å¿ƒã€‚\", metadata={\"source\": \"news\"}),\n",
    "    Document(page_content=\"Washington D.C. is the capital of the United States.\", metadata={\"source\": \"edu\"}),\n",
    "    Document(page_content=\"é¦–å°”æ˜¯éŸ©å›½çš„é¦–éƒ½ï¼Œä»¥ç°ä»£åŒ–ä¸ä¼ ç»Ÿæ–‡åŒ–çš„èåˆé—»åã€‚\", metadata={\"source\": \"travel\"}),\n",
    "    Document(page_content=\"Capitalization in finance refers to the total value of a company's shares.\", metadata={\"source\": \"finance\"})\n",
    "]\n",
    "\n",
    "query = \"å„å›½é¦–éƒ½åŸå¸‚çš„ä»‹ç»æœ‰å“ªäº›ï¼Ÿ\"\n",
    "\n",
    "# æ‰§è¡Œé‡æ’åº\n",
    "compressed_docs = compressor.compress_documents(documents=documents, query=query)\n",
    "\n",
    "# æ‰“å°æ’åºç»“æœ\n",
    "print(\"===== é‡æ’åºåçš„Top 3æ–‡æ¡£ =====\")\n",
    "for i, doc in enumerate(compressed_docs):\n",
    "    print(f\"Rank {i+1} (Score: {doc.metadata['relevance_score']:.4f}):\")\n",
    "    print(f\"å†…å®¹ï¼š{doc.page_content}\")\n",
    "    print(f\"å…ƒæ•°æ®ï¼š{doc.metadata}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨å¼€æºçš„Rerankeræ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ï¼modelscope download --model BAAI/bge-reranker-base --cache_dir /opt/workspace/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨¡å‹è·¯å¾„ï¼š  /opt/workspace/models/BAAI/bge-reranker-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…ˆå°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "model = HuggingFaceCrossEncoder(model_name=\"/opt/workspace/models/BAAI/bge-reranker-base\")\n",
    "compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\"What is the plan for the economy?\")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\")\n",
    "compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\"What is the plan for the economy?\")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
