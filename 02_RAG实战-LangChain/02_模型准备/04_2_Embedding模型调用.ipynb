{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的部署参考： [learn-llm-deploy-easily](https://gitee.com/coderwillyan/learn-llm-deploy-easily) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里主要介绍如何调用已部署的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用embedding API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以zhipu为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "my_emb = ZhipuAIEmbeddings(\n",
    "    model=\"embedding-2\",\n",
    "    api_key=os.environ[\"ZHIPUAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用本地部署的embedding模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "​​特点​​：底层灵活，支持BERT、RoBERTa等Transformer架构的Embedding模型，需自定义向量提取逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel, AutoTokenizer\n",
    "# model = AutoModel.from_pretrained(\"BAAI/bge-large-zh\")\n",
    "# embeddings = model(inputs).last_hidden_state.mean(dim=1)  # 提取句向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Transformers部署"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "​​特点​​：基于Transformers的封装，内置池化层，一键生成句子级嵌入，支持语义相似度计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "项目地址： https://github.com/UKPLab/sentence-transformers\n",
    "\n",
    "参考文档： https://sbert.net.cn/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires transformers>=4.51.0\n",
    "# Requires sentence-transformers>=2.7.0\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer(\"/opt/workspace/models/Qwen/Qwen3-Embedding-0.6B\",  device=\"cuda:3\")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving,\n",
    "# together with setting `padding_side` to \"left\":\n",
    "# model = SentenceTransformer(\n",
    "#     \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "#     model_kwargs={\"attn_implementation\": \"flash_attention_2\", \"device_map\": \"auto\"},\n",
    "#     tokenizer_kwargs={\"padding_side\": \"left\"},\n",
    "# )\n",
    "\n",
    "# The queries and documents to embed\n",
    "queries = [\n",
    "    \"What is the capital of China?\",\n",
    "    \"Explain gravity\",\n",
    "]\n",
    "documents = [\n",
    "    \"The capital of China is Beijing.\",\n",
    "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n",
    "]\n",
    "\n",
    "# Encode the queries and documents. Note that queries benefit from using a prompt\n",
    "# Here we use the prompt called \"query\" stored under `model.prompts`, but you can\n",
    "# also pass your own prompt via the `prompt` argument\n",
    "query_embeddings = model.encode(queries, prompt_name=\"query\")\n",
    "document_embeddings = model.encode(documents)\n",
    "\n",
    "# Compute the (cosine) similarity between the query and document embeddings\n",
    "similarity = model.similarity(query_embeddings, document_embeddings)\n",
    "print(similarity)\n",
    "# tensor([[0.7646, 0.1414],\n",
    "#         [0.1355, 0.6000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 指定 GPU 设备（例如 GPU 0）\n",
    "# device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Load the model\n",
    "model = SentenceTransformer(\"/opt/workspace/models/Qwen/Qwen3-Embedding-0.6B\", device=\"cuda:3\")\n",
    "result = model.encode(\"这是一个测试\")\n",
    "result.tolist()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFaceEmbeddings部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "my_emb = HuggingFaceEmbeddings(model_name='/opt/workspace/models/Qwen/Qwen3-Embedding-0.6B', model_kwargs={\"device\": \"cuda:3\"})\n",
    "query = \"如何调用HuggingFaceEmbeddings？\"\n",
    "query_vector = my_emb.embed_query(query) \n",
    "print(\"查询向量:\", query_vector[:5], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ollama部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "my_emb = OllamaEmbeddings(base_url='http://localhost:11434',model=\"dengcao/Qwen3-Embedding-0.6B:F16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查询向量: [-0.01782775, -0.010210706, -0.031165373, 0.029789878, -0.024330704] ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "# 初始化嵌入模型\n",
    "my_emb = OllamaEmbeddings(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"bge-m3:latest\"\n",
    ")\n",
    "\n",
    "# 示例文本\n",
    "query = \"如何调用OllamaEmbeddings？\"\n",
    "\n",
    "# 生成嵌入向量\n",
    "query_vector = my_emb.embed_query(query)\n",
    "\n",
    "# 输出结果\n",
    "print(\"查询向量:\", query_vector[:5], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vllm部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 见qwen模型发布文档"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xinference部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import XinferenceEmbeddings\n",
    "\n",
    "# 替换为你的Xinference服务器URL和模型UID\n",
    "xinference = XinferenceEmbeddings(\n",
    "    server_url=\"http://localhost:9997\",  # 注意：原代码中的\"loaclhost\"拼写错误，应为\"localhost\"\n",
    "    model_uid=\"your_model_uid\"  # 替换为实际的模型UID\n",
    ")\n",
    "\n",
    "# 输入文本\n",
    "texts = [\"你好，世界。\", \"LangChain 是一个强大的工具。\"]\n",
    "\n",
    "# 生成嵌入向量\n",
    "vectors = xinference.embed_documents(texts)\n",
    "\n",
    "# 打印结果\n",
    "for idx, vector in enumerate(vectors):\n",
    "    print(f\"文本 {idx + 1}: {texts[idx]}\")\n",
    "    print(f\"嵌入向量: {vector[:5]}... (维度: {len(vector)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (langchain-yw)",
   "language": "python",
   "name": "langchain-yw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
