{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建并使用向量数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前序数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# 批量处理文件夹中所有文件\n",
    "import os\n",
    "\n",
    "# 获取folder_path下所有文件路径，储存在file_paths里\n",
    "file_paths = []\n",
    "folder_path = '../data_base/knowledge_path/VMAX-S'\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_paths.append(file_path)\n",
    "print(file_paths)\n",
    "\n",
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "# from langchain.document_loaders.markdown import UnstructuredMarkdownLoader\n",
    "\n",
    "# 遍历文件路径并把实例化的loader存放在loaders里\n",
    "loaders = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    file_type = file_path.split('.')[-1]\n",
    "    if file_type == 'pdf':\n",
    "        loaders.append(PyMuPDFLoader(file_path))\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {file_type} for file {file_path}\")\n",
    "\n",
    "# 下载文件并存储到text\n",
    "# 加载所有文档内容到 texts\n",
    "texts = []\n",
    "for loader in loaders:\n",
    "    texts.extend(loader.load())  # 关键步骤：初始化 texts\n",
    "\n",
    "    \n",
    "# 作数据清洗\n",
    "# 修改后的数据清洗部分（替换原始代码中对应段落）\n",
    "import re\n",
    "\n",
    "# 预编译正则表达式（提升效率）\n",
    "linebreak_pattern = re.compile(\n",
    "    r'(?<![\\\\u4e00-\\\\u9fff])\\n(?![\\\\u4e00-\\\\u9fff])',  # 负向断言匹配非中文环境换行\n",
    "    flags=re.DOTALL\n",
    ")\n",
    "space_pattern = re.compile(r'[ 　]+')  # 匹配半角/全角空格\n",
    "special_chars = ['•', '▪', '▫', '▶', '®', '©']  # 可扩展的干扰符号列表\n",
    "\n",
    "# 替换原始代码中的清洗循环\n",
    "for text in texts:\n",
    "    # 1. 清理非中文环境换行\n",
    "    text.page_content = re.sub(\n",
    "        linebreak_pattern,\n",
    "        lambda m: m.group().replace('\\n', ''),\n",
    "        text.page_content\n",
    "    )\n",
    "\n",
    "    # 2. 批量清理特殊符号\n",
    "    for char in special_chars:\n",
    "        text.page_content = text.page_content.replace(char, '')\n",
    "\n",
    "    # 3. 安全删除空格（保留URL等特殊场景）\n",
    "    text.page_content = space_pattern.sub('', text.page_content)\n",
    "\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv \n",
    "# pip install python-dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "zhipu_embed = ZhipuAIEmbeddings(\n",
    "    model=\"embedding-2\",\n",
    "    api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\",\n",
    ")\n",
    "\n",
    "\n",
    "''' \n",
    "* RecursiveCharacterTextSplitter 递归字符文本分割\n",
    "RecursiveCharacterTextSplitter 将按不同的字符递归地分割(按照这个优先级[\"\\n\\n\", \"\\n\", \" \", \"\"])，\n",
    "    这样就能尽量把所有和语义相关的内容尽可能长时间地保留在同一位置\n",
    "RecursiveCharacterTextSplitter需要关注的是4个参数：\n",
    "\n",
    "* separators - 分隔符字符串数组\n",
    "* chunk_size - 每个文档的字符数量限制\n",
    "* chunk_overlap - 两份文档重叠区域的长度\n",
    "* length_function - 长度计算函数\n",
    "'''\n",
    "#导入文本分割器\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "# 知识库中单段文本长度\n",
    "CHUNK_SIZE = 512\n",
    "\n",
    "# 知识库中相邻文本重合长度\n",
    "OVERLAP_SIZE = 50\n",
    "\n",
    "\n",
    "# 使用递归字符文本分割器\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=OVERLAP_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "split_docs = text_splitter.split_documents(texts)\n",
    "print(f\"切分后的文件数量：{len(split_docs)}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"切分后的字符数（可以用来大致评估 token 数）：{sum([len(doc.page_content) for doc in split_docs])}\")\n",
    "\n",
    "\n",
    "\n",
    "split_docs[90].page_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前序embedding模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZhipuAIEmbeddings模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding选型\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "emb_bgem3 = OllamaEmbeddings(base_url='http://localhost:11434',model=\"bge-m3:latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量处理文件夹中所有文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Chroma向量库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "涉及到的API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接调用Chroma.from_documents (推荐)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_milvus import Milvus\n",
    "from langchain_community.vectorstores import Milvus\n",
    "\n",
    "URI = \"../milvus_example.db\"\n",
    "\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"uri\": URI},\n",
    "    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory=\"../milvus_example.db\"\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "        documents=split_docs,  # 为了速度，只选择前 20 个切分的 doc 进行生成\n",
    "        embedding=emb_bgem3,\n",
    "        collection_name=\"vmax-s\", # 如果不指定默认为langchain\n",
    "        persist_directory=persist_directory, # 允许我们将persist_directory目录保存到磁盘上\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "服务端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_milvus import Milvus\n",
    "from langchain_community.vectorstores import Milvus\n",
    "from langchain_core.documents import Document  # 导入 Document 类\n",
    "\n",
    "\n",
    "# 向量库创建\n",
    "connection_args = {\n",
    "    \"host\": \"192.168.0.188\",\n",
    "    \"port\": \"19530\",\n",
    "}\n",
    "\n",
    "split_docs = [\n",
    "    Document(page_content=\"你好\", metadata={\"source\": \"example\"}),\n",
    "    # 可以添加更多文档...\n",
    "]\n",
    "\n",
    "\n",
    "vectordb = Milvus.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=emb_bgem3,\n",
    "    collection_name=\"vmaxs\",\n",
    "    drop_old=False,\n",
    "    connection_args=connection_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 汇总\n",
    "\n",
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "# from langchain_chroma.vectorstores import Chroma\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "# 定义持久化目录\n",
    "# persist_directory = '../milvus-vmax'\n",
    "\n",
    "# 向量库创建\n",
    "connection_args = {\n",
    "    \"host\": \"192.168.0.188\",\n",
    "    \"port\": \"19530\",\n",
    "}\n",
    "\n",
    "\n",
    "# 创建嵌入模型\n",
    "# embedding = ZhipuAIEmbeddings(model=\"embedding-2\", api_key = os.environ['ZHIPUAI_API_KEY'])\n",
    "# embedding选型\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "emb_bgem3 = OllamaEmbeddings(base_url='http://localhost:11434',model=\"bge-m3:latest\")\n",
    "\n",
    "try:\n",
    "    # 初始化 Chroma 向量数据库\n",
    "    # vectordb = Chroma.from_documents(\n",
    "    #     documents=split_docs[0:3],  # 为了速度，只选择前 20 个切分的 doc 进行生成\n",
    "    #     embedding=emb_bgem3,\n",
    "    #     collection_name=\"vmax-s\", # 如果不指定默认为langchain\n",
    "    #     persist_directory=persist_directory, # 允许我们将persist_directory目录保存到磁盘上\n",
    "    # )\n",
    "    vectordb = Milvus.from_documents(\n",
    "    documents=split_docs[0:3],\n",
    "    embedding=emb_bgem3,\n",
    "    collection_name=\"vmaxs\",\n",
    "    drop_old=False,\n",
    "    connection_args=connection_args,\n",
    "    )\n",
    "    \n",
    "    # 持久化向量数据库\n",
    "    # vectordb.persist()\n",
    "    print(\"向量数据库已成功持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码会报错： 持久化过程中发生错误: Error code: 400, with error text {\"error\":{\"code\":\"1214\",\"message\":\"input数组最大不得超过64条\"}}\n",
    "\n",
    "建议分批次导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "# 定义持久化目录\n",
    "# persist_directory = '../chroma-vmax'\n",
    "# 向量库创建\n",
    "connection_args = {\n",
    "    \"host\": \"192.168.0.188\",\n",
    "    \"port\": \"19530\",\n",
    "}\n",
    "\n",
    "\n",
    "# 创建嵌入模型\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "emb_bgem3 = OllamaEmbeddings(base_url='http://localhost:11434',model=\"bge-m3:latest\")\n",
    "\n",
    "# 定义每批处理的文档数量\n",
    "batch_size = 30\n",
    "\n",
    "try:\n",
    "    # 计算总批次数\n",
    "    total_batches = (len(split_docs) + batch_size - 1) // batch_size\n",
    "    \n",
    "    # 初始化向量数据库（如果是第一次创建）\n",
    "    vectordb = None\n",
    "    \n",
    "    for batch_num in range(total_batches):\n",
    "        # 计算当前批次的起始和结束索引\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min((batch_num + 1) * batch_size, len(split_docs))\n",
    "        \n",
    "        # 获取当前批次的文档\n",
    "        batch_docs = split_docs[start_idx:end_idx]\n",
    "        \n",
    "        print(f\"正在处理第 {batch_num + 1}/{total_batches} 批文档 (文档 {start_idx}-{end_idx-1})\")\n",
    "        \n",
    "        if batch_num == 0:\n",
    "            # 第一次创建向量数据库\n",
    "            vectordb = Milvus.from_documents(\n",
    "            documents=split_docs[0:3],\n",
    "            embedding=emb_bgem3,\n",
    "            collection_name=\"vmaxs\",\n",
    "            drop_old=False,\n",
    "            connection_args=connection_args,\n",
    "            )\n",
    "    \n",
    "        else:\n",
    "            # 后续批次添加到现有集合\n",
    "            vectordb.add_documents(batch_docs)     \n",
    "        # 每批处理后持久化\n",
    "        # vectordb.persist()\n",
    "        print(f\"第 {batch_num + 1} 批文档已成功导入并持久化\")\n",
    "    \n",
    "    print(\"所有文档已成功导入并持久化到向量数据库。\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"处理过程中发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "charma服务器版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key = os.environ['ZHIPUAI_API_KEY'],\n",
    "                model_name=\"embedding-2\"\n",
    "            )\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"test4\",\n",
    "    embedding_function=openai_ef,  # 使用openai\n",
    "    metadata={\n",
    "        \"hnsw:space\": \"cosine\", \n",
    "        \"hnsw:construction_ef\": 150,\n",
    "        \"hnsw:M\": 24,\n",
    "        \"hnsw:num_threads\": 8\n",
    "    }\n",
    ")\n",
    "ids_list = [f\"doc_{i+1}\" for i in range(len(split_docs[:20]))]\n",
    "print(ids_list)\n",
    "\n",
    "try:\n",
    "    # 添加文档到集合\n",
    "    collection.add(documents=split_docs[:20], ids =ids_list )\n",
    "    # 持久化向量数据库\n",
    "    vectordb.persist()\n",
    "    print(\"向量数据库已成功持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过chroma数据库add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按官网文档进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list = ['本发布日期南书', \n",
    "                '前言周志华老师的《机器学习》西瓜书是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读者通过西瓜书对机器学习有所了解所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推导细节的读者来说可能不太友好，本书旨在对西瓜书里比较难理解的公式加以解析，以及对部分公式补充具体的推导细节。读到这里，大家可能会疑问为啥前面这段话加了引号，因为这只是我们最初的遐想，后来我们了解到，周老师之所以省去这些推导细节的真实原因是，他本尊认为理工科数学基础扎实点的大二下学生应该对西瓜书中的推导细节无困难吧，要点在书里都有了，略去的细节应能脑补或做练习。所以本南瓜书只能算是我等数学渣渣在自学的时候记下来的笔记，希望能够帮助大家都成为一名合格的理工科数学基础扎实点的大二下学生。使用说明南瓜书的所有内容都是以西瓜书的内容为前置知识进行表述的，所以南瓜书的最佳使用方法是以西瓜书为主线，遇到自己推导不出来或者看不懂的公式时再来查阅南瓜书；对于初学机器学习的小白，西瓜书第章和第章的公式强烈不建议深究，简单过一下即可，等你学得', \n",
    "                '有点飘的时候再回来啃都来得及；每个公式的解析和推导我们都力争以本科数学基础的视角进行讲解，所以超纲的数学知识我们通常都会以附录和参考文献的形式给出，感兴趣的同学可以继续沿着我们给的资料进行深入学习；若南瓜书里没有你想要查阅的公式，或者你发现南瓜书哪个地方有错误，请毫不犹豫地去我们的地址：进行反馈，在对应版块提交你希望补充的公式编号或者勘误信息，我们通常会在小时以内给您回复，超过小时未回复的话可以微信联系我们微信号：；配套视频教程：在线阅读地址：仅供第版最新版获取地址：编委会', \n",
    "                '编委会主编：、、编委：、、、、封面设计：构思、创作林王茂盛致谢特别感谢、、、、、、、、、、、在最早期的时候对南瓜书所做的贡献。扫描下方二维码，然后回复关键词南瓜书，即可加入南瓜书读者交流群版权声明本作品采用知识共享署名非商业性使用相同方式共享国际许可协议进行许可。'\n",
    "               ]\n",
    "print(len(content_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=[\"doc1\", \"doc2\", \"doc3\", ...],\n",
    "    embeddings=[[1.1, 2.3, 3.2], [4.5, 6.9, 4.4], [1.1, 2.3, 3.2], ...],\n",
    "    metadatas=[{\"chapter\": \"3\", \"verse\": \"16\"}, {\"chapter\": \"3\", \"verse\": \"5\"}, {\"chapter\": \"29\", \"verse\": \"11\"}, ...],\n",
    "    ids=[\"id1\", \"id2\", \"id3\", ...]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "import os\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "import chromadb\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "from chromadb import PersistentClient\n",
    "# 定义持久化目录\n",
    "persist_directory = '../chroma-vmax'\n",
    "# 初始化客户端（持久化模式）\n",
    "client = PersistentClient(path=persist_directory)\n",
    "\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key = os.environ['ZHIPUAI_API_KEY'],\n",
    "                model_name=\"embedding-2\"\n",
    "            )\n",
    "# collection = client.get_or_create_collection(\n",
    "#     name=\"test\",\n",
    "#     embedding_function=openai_ef,  # 使用openai\n",
    "#     metadata={\n",
    "#         \"hnsw:space\": \"cosine\", \n",
    "#         \"hnsw:construction_ef\": 150,\n",
    "#         \"hnsw:M\": 24,\n",
    "#         \"hnsw:num_threads\": 8\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# HNSW索引构建参数优化\n",
    "collection = client.create_collection(\n",
    "    name=\"high_perf\",\n",
    "    metadata={\n",
    "        \"hnsw:construction_ef\": 200,  # 构建阶段搜索范围（默认100）\n",
    "        \"hnsw:search_ef\": 320,        # 查询阶段搜索范围 \n",
    "        \"hnsw:M\": 48                  # 图节点最大连接数\n",
    "    }\n",
    ")\n",
    "\n",
    "content_list = ['本发布日期南书', \n",
    "                '前言周志华老师的《机器学习》西瓜书是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读者通过西瓜书对机器学习有所了解所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推导细节的读者来说可能不太友好，本书旨在对西瓜书里比较难理解的公式加以解析，以及对部分公式补充具体的推导细节。读到这里，大家可能会疑问为啥前面这段话加了引号，因为这只是我们最初的遐想，后来我们了解到，周老师之所以省去这些推导细节的真实原因是，他本尊认为理工科数学基础扎实点的大二下学生应该对西瓜书中的推导细节无困难吧，要点在书里都有了，略去的细节应能脑补或做练习。所以本南瓜书只能算是我等数学渣渣在自学的时候记下来的笔记，希望能够帮助大家都成为一名合格的理工科数学基础扎实点的大二下学生。使用说明南瓜书的所有内容都是以西瓜书的内容为前置知识进行表述的，所以南瓜书的最佳使用方法是以西瓜书为主线，遇到自己推导不出来或者看不懂的公式时再来查阅南瓜书；对于初学机器学习的小白，西瓜书第章和第章的公式强烈不建议深究，简单过一下即可，等你学得', \n",
    "                '有点飘的时候再回来啃都来得及；每个公式的解析和推导我们都力争以本科数学基础的视角进行讲解，所以超纲的数学知识我们通常都会以附录和参考文献的形式给出，感兴趣的同学可以继续沿着我们给的资料进行深入学习；若南瓜书里没有你想要查阅的公式，或者你发现南瓜书哪个地方有错误，请毫不犹豫地去我们的地址：进行反馈，在对应版块提交你希望补充的公式编号或者勘误信息，我们通常会在小时以内给您回复，超过小时未回复的话可以微信联系我们微信号：；配套视频教程：在线阅读地址：仅供第版最新版获取地址：编委会', \n",
    "                '编委会主编：、、编委：、、、、封面设计：构思、创作林王茂盛致谢特别感谢、、、、、、、、、、、在最早期的时候对南瓜书所做的贡献。扫描下方二维码，然后回复关键词南瓜书，即可加入南瓜书读者交流群版权声明本作品采用知识共享署名非商业性使用相同方式共享国际许可协议进行许可。'\n",
    "               ]\n",
    "# print(len(content_list))\n",
    "\n",
    "# 如果每个文档块的元数据需要个性化（例如记录分块编号），可以生成与文档数量匹配的元数据列表：\n",
    "# metadatas = [{\"source\": \"权威文档.pdf\", \"chunk_id\": i} for i in range(len(split_docs[:20]))]\n",
    "\n",
    "# 如果所有文档块的元数据相同（例如来源一致），可通过列表推导式生成等长元数据列表：\n",
    "metadatas = [{\"source\": \"权威文档.pdf\"} for _ in range(len(split_docs[:20]))]\n",
    "\n",
    "ids_list = [f\"doc_{i+1}\" for i in range(len(content_list))]\n",
    "# print(ids_list)\n",
    "\n",
    "\n",
    "# print(metadatas)\n",
    "\n",
    "try:\n",
    "    # 添加文档到集合（修正后的参数）\n",
    "    collection.add(\n",
    "        documents=content_list,  # 传入纯文本列表\n",
    "        metadatas=[{\"source\": \"权威文档.pdf\"},{\"source\": \"权威文档.pdf\"},{\"source\": \"权威文档.pdf\"},{\"source\": \"权威文档.pdf\"} ],\n",
    "        ids=ids_list\n",
    "    )\n",
    "    print(\"数据写入成功，已自动持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")\n",
    "except chromadb.errors.ChromaError as ce:\n",
    "    print(f\"数据库错误: {ce}\")\n",
    "except ValueError as ve:\n",
    "    print(f\"数据格式错误: {ve}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正式方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "ollama_emb = OllamaEmbeddings(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"nomic-embed-text:latest\"\n",
    ")\n",
    "r1 = ollama_emb.embed_documents(\n",
    "    [\n",
    "        \"Alpha is the first letter of Greek alphabet\",\n",
    "        \"Beta is the second letter of Greek alphabet\",\n",
    "    ]\n",
    ")\n",
    "r2 = ollama_emb.embed_query(\n",
    "    \"What is the second letter of Greek alphabet\"\n",
    ")\n",
    "\n",
    "# 打印结果\n",
    "print(\"Document embeddings:\")\n",
    "for i, embedding in enumerate(r1):\n",
    "    print(f\"Document {i+1} embedding: {embedding[:10]}\")\n",
    "\n",
    "print(\"\\nQuery embedding:\")\n",
    "print(f\"Query embedding: {r2[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils.embedding_functions.ollama_embedding_function import OllamaEmbeddingFunction\n",
    "import requests\n",
    "\n",
    "ollama_ef = OllamaEmbeddingFunction(\n",
    "    url=\"http://localhost:11434\",\n",
    "    model_name=\"qwen2.5:0.5b \"\n",
    ")\n",
    "\n",
    "try:\n",
    "    response = ollama_ef._session.post(\n",
    "        ollama_ef._api_url,\n",
    "        json={\"model\": \"nnomic-embed-text:latest\", \"prompt\": \"Test\"}\n",
    "    )\n",
    "    print(\"原始响应内容:\", response.text)  # 打印原始响应\n",
    "    embeddings = ollama_ef([\"Test\"])\n",
    "except Exception as e:\n",
    "    print(f\"错误详情: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils.embedding_functions.ollama_embedding_function import OllamaEmbeddingFunction\n",
    "ollama_ef = OllamaEmbeddingFunction(url=\"http://localhost:11434\",model_name=\"nomic-embed-text:latest\")\n",
    "embeddings = ollama_ef(\"This is my first text to embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "import os\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "import chromadb\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "from chromadb import PersistentClient\n",
    "# 定义持久化目录\n",
    "persist_directory = '../chroma-vmax-3'\n",
    "# 初始化客户端（持久化模式）\n",
    "client = PersistentClient(path=persist_directory)\n",
    "\n",
    "from chromadb.utils.embedding_functions.ollama_embedding_function import OllamaEmbeddingFunction\n",
    "ollama_ef = OllamaEmbeddingFunction(url=\"http://localhost:11434\",model_name=\"nomic-embed-text:latest\")\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"vmax-s\",\n",
    "    embedding_function=ollama_ef,  # 可选，但建议保持一致性\n",
    "    metadata={\n",
    "        \"hnsw:construction_ef\": 200,  # 构建阶段搜索范围（默认100）\n",
    "        \"hnsw:search_ef\": 320,        # 查询阶段搜索范围 \n",
    "        \"hnsw:M\": 48                  # 图节点最大连接数\n",
    "    }\n",
    ")\n",
    "\n",
    "# content_list = [doc.page_content for doc in split_docs[:20]]\n",
    "content_list = [doc.page_content for doc in split_docs]\n",
    "\n",
    "# 如果每个文档块的元数据需要个性化（例如记录分块编号），可以生成与文档数量匹配的元数据列表：\n",
    "# metadatas = [{\"source\": \"权威文档.pdf\", \"chunk_id\": i} for i in range(len(split_docs[:20]))]\n",
    "\n",
    "# 如果所有文档块的元数据相同（例如来源一致），可通过列表推导式生成等长元数据列表：\n",
    "# metadatas = [{\"source\": \"VMAX安装系列.pdf\"} for _ in range(len(split_docs[:20]))]\n",
    "metadatas = [{\"source\": \"VMAX安装系列.pdf\"} for _ in range(len(split_docs))]\n",
    "\n",
    "# ids_list = [f\"doc_{i+1}\" for i in range(len(split_docs[:20]))]\n",
    "ids_list = [f\"doc_{i+1}\" for i in range(len(split_docs))]\n",
    "# print(ids_list)\n",
    "\n",
    "\n",
    "# print(metadatas)\n",
    "\n",
    "try:\n",
    "    # 添加文档到集合（修正后的参数）\n",
    "    collection.add(\n",
    "        documents=content_list,  # 传入纯文本列表\n",
    "        metadatas=metadatas, \n",
    "        ids=ids_list\n",
    "    )\n",
    "    print(\"数据写入成功，已自动持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")\n",
    "except chromadb.errors.ChromaError as ce:\n",
    "    print(f\"数据库错误: {ce}\")\n",
    "except ValueError as ve:\n",
    "    print(f\"数据格式错误: {ve}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过chroma数据库add-改良版 \n",
    "每次三个doc，分批次导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 汇总\n",
    "\n",
    "import os\n",
    "# from langchain.vectordbs.chroma import Chroma\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "from chromadb import PersistentClient\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "# 定义持久化目录\n",
    "persist_directory = '../chroma-vmax-batch'\n",
    "# 初始化客户端（持久化模式）\n",
    "client = PersistentClient(path=persist_directory)\n",
    "\n",
    "# 创建嵌入模型\n",
    "# import chromadb.utils.embedding_functions as embedding_functions\n",
    "# openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "#                 api_key = os.environ['ZHIPUAI_API_KEY'],\n",
    "#                 model_name=\"embedding-2\"\n",
    "#             )\n",
    "\n",
    "# import chromadb.utils.embedding_functions as embedding_functions\n",
    "# cohere_ef  = embedding_functions.CohereEmbeddingFunction(api_key=\"Tahx1eySFbKvu9sTyTXrRLf59la3ZUG9vy02stRZ\",  model_name=\"embed-multilingual-v3.0\")\n",
    "\n",
    "zhipu_ef = ZhipuAIAdapter(model_name=\"embedding-2\", api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\")\n",
    "\n",
    "# 创建集合并配置HNSW参数\n",
    "# collection = client.create_collection( \n",
    "# collection = client.get_or_create_collection(\n",
    "#     name=\"test4\",\n",
    "#     embedding_function=openai_ef,\n",
    "#     metadata={\n",
    "#         \"hnsw:space\": \"cosine\", \n",
    "#         \"hnsw:construction_ef\": 150,\n",
    "#         \"hnsw:M\": 24,\n",
    "#         \"hnsw:num_threads\": 8\n",
    "#     }\n",
    "# )\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"vmax-s\",\n",
    "    embedding_function=cohere_ef,  # 可选，但建议保持一致性\n",
    "    metadata={\n",
    "        \"hnsw:construction_ef\": 200,  # 构建阶段搜索范围（默认100）\n",
    "        \"hnsw:search_ef\": 320,        # 查询阶段搜索范围 \n",
    "        \"hnsw:M\": 48                  # 图节点最大连接数\n",
    "    }\n",
    ")\n",
    "# content_list = [doc.page_content for doc in split_docs[:20]]\n",
    "content_list = [doc.page_content for doc in split_docs]\n",
    "\n",
    "# 如果每个文档块的元数据需要个性化（例如记录分块编号），可以生成与文档数量匹配的元数据列表：\n",
    "# metadatas = [{\"source\": \"权威文档.pdf\", \"chunk_id\": i} for i in range(len(split_docs[:20]))]\n",
    "\n",
    "# 如果所有文档块的元数据相同（例如来源一致），可通过列表推导式生成等长元数据列表：\n",
    "# metadatas = [{\"source\": \"VMAX安装系列.pdf\"} for _ in range(len(split_docs[:20]))]\n",
    "metadatas = [{\"source\": \"VMAX安装系列.pdf\"} for _ in range(len(split_docs))]\n",
    "\n",
    "# ids_list = [f\"doc_{i+1}\" for i in range(len(split_docs[:20]))]\n",
    "ids_list = [f\"doc_{i+1}\" for i in range(len(split_docs))]\n",
    "# print(ids_list)\n",
    "\n",
    "try:\n",
    "    # 添加文档到集合\n",
    "    # collection.add(documents=split_docs)\n",
    "\n",
    "    from itertools import batched\n",
    "    docs_batches = batched(content_list, 3)  # 分3个批次\n",
    "    ids_batches = batched(ids_list, 3)\n",
    "\n",
    "    for docs, ids in zip(docs_batches, ids_batches):\n",
    "        collection.add(\n",
    "            documents=content_list,  # 传入纯文本列表\n",
    "            metadatas=metadatas, \n",
    "            ids=ids_list\n",
    "        )\n",
    "    \n",
    "    # 持久化向量数据库\n",
    "    vectordb.persist()\n",
    "    print(\"向量数据库已成功持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 连接chroma数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据库路径 ./chroma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client  = chromadb.PersistentClient(path=\"../chroma-vmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取一个存在的Collection对象\n",
    "collection = chroma_client.get_collection(\"vmax-s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.count()  #  returns the number of items in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.peek()  # returns a list of the first 10 items in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取所有数据（默认不返回嵌入向量）\n",
    "all_data = collection.get()\n",
    "print(all_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量检索\n",
    "### 3.1 相似度检索\n",
    "Chroma的相似度搜索使用的是余弦距离，即：\n",
    "$$\n",
    "similarity = cos(A, B) = \\frac{A \\cdot B}{\\parallel A \\parallel \\parallel B \\parallel} = \\frac{\\sum_1^n a_i b_i}{\\sqrt{\\sum_1^n a_i^2}\\sqrt{\\sum_1^n b_i^2}}\n",
    "$$\n",
    "其中$a_i$、$b_i$分别是向量$A$、$B$的分量。\n",
    "\n",
    "当你需要数据库返回严谨的按余弦相似度排序的结果时可以使用`similarity_search`函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Chroma.from_documents时，用这个方法： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"vmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_docs = vectordb.similarity_search(question,k=3)\n",
    "print(f\"检索到的内容数：{len(sim_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sim_doc in enumerate(sim_docs):\n",
    "    print(f\"检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 MMR检索\n",
    "如果只考虑检索出内容的相关性会导致内容过于单一，可能丢失重要信息。\n",
    "\n",
    "最大边际相关性 (`MMR, Maximum marginal relevance`) 可以帮助我们在保持相关性的同时，增加内容的丰富度。\n",
    "\n",
    "核心思想是在已经选择了一个相关性高的文档之后，再选择一个与已选文档相关性较低但是信息丰富的文档。这样可以在保持相关性的同时，增加内容的多样性，避免过于单一的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr_docs = vectordb.max_marginal_relevance_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sim_doc in enumerate(mmr_docs):\n",
    "    print(f\"MMR 检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
