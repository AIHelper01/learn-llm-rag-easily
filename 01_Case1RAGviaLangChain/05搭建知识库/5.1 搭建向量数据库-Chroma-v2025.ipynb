{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建并使用向量数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前序embedding模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZhipuAIEmbeddings模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "zhipu_embed = ZhipuAIEmbeddings(\n",
    "    model=\"embedding-2\",\n",
    "    api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前序数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量处理文件夹中所有文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data_base/knowledge_path\\\\pumkin_book\\\\pumpkin_book.pdf']\n",
      "---\n",
      "文档split成功\n",
      "切分后的文件数量：614\n",
      "切分后的字符数（可以用来大致评估 token 数）：252076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'有点飘的时候再回来啃都来得及；每个公式的解析和推导我们都力(zhi)争(neng)以本科数学基础的视角进行讲解，所以超纲的数学知识\\n我们通常都会以附录和参考文献的形式给出，感兴趣的同学可以继续沿着我们给的资料进行深入学习；若南瓜书里没有你想要查阅的公式，或者你发现南瓜书哪个地方有错误，请毫不犹豫地去我们GitHub的\\nIssues（地址：https://github.com/datawhalechina/pumpkin-book/issues）进行反馈，在对应版块\\n提交你希望补充的公式编号或者勘误信息，我们通常会在24小时以内给您回复，超过24小时未回复的\\n话可以微信联系我们（微信号：at-Sm1les）；\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n在线阅读地址：https://datawhalechina.github.io/pumpkin-book（仅供第1版）\\n最新版PDF获取地址：https://github.com/datawhalechina/pumpkin-book/releases\\n编委会'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# from dotenv import load_dotenv, find_dotenv \n",
    "# pip install python-dotenv\n",
    "\n",
    "# _ = load_dotenv(find_dotenv())\n",
    "\n",
    "\n",
    "# 获取folder_path下所有文件路径，储存在file_paths里\n",
    "file_paths = []\n",
    "folder_path = '../data_base/knowledge_path'\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_paths.append(file_path)\n",
    "print(file_paths)\n",
    "\n",
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "# from langchain.document_loaders.markdown import UnstructuredMarkdownLoader\n",
    "\n",
    "# 遍历文件路径并把实例化的loader存放在loaders里\n",
    "loaders = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    file_type = file_path.split('.')[-1]\n",
    "    if file_type == 'pdf':\n",
    "        loaders.append(PyMuPDFLoader(file_path))\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {file_type} for file {file_path}\")\n",
    "\n",
    "# 下载文件并存储到text\n",
    "# 作数据清洗\n",
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "import re\n",
    "# 下载文件并存储到text\n",
    "texts = []\n",
    "for loader in loaders:\n",
    "    texts.extend(loader.load())   \n",
    "\n",
    "for text in texts:\n",
    "    pattern = re.compile(r'[^\\u4e00-\\u9fff](\\n)[^\\u4e00-\\u9fff]', re.DOTALL)\n",
    "    text.page_content = re.sub(pattern, lambda match: match.group(0).replace('\\n', ''), text.page_content)\n",
    "    text.page_content = text.page_content.replace('•', '')\n",
    "    text.page_content = text.page_content.replace(' ', '')\n",
    "\n",
    "\n",
    "\n",
    "''' \n",
    "* RecursiveCharacterTextSplitter 递归字符文本分割\n",
    "RecursiveCharacterTextSplitter 将按不同的字符递归地分割(按照这个优先级[\"\\n\\n\", \"\\n\", \" \", \"\"])，\n",
    "    这样就能尽量把所有和语义相关的内容尽可能长时间地保留在同一位置\n",
    "RecursiveCharacterTextSplitter需要关注的是4个参数：\n",
    "\n",
    "* separators - 分隔符字符串数组\n",
    "* chunk_size - 每个文档的字符数量限制\n",
    "* chunk_overlap - 两份文档重叠区域的长度\n",
    "* length_function - 长度计算函数\n",
    "'''\n",
    "#导入文本分割器\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 知识库中单段文本长度\n",
    "CHUNK_SIZE = 512\n",
    "# 知识库中相邻文本重合长度\n",
    "OVERLAP_SIZE = 50\n",
    "\n",
    "# 使用递归字符文本分割器\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=OVERLAP_SIZE\n",
    ")\n",
    "# text_splitter.split_text(text.page_content[0:1000])\n",
    "split_docs = text_splitter.split_documents(texts)\n",
    "\n",
    "print(f\"---\\n文档split成功\")\n",
    "\n",
    "\n",
    "print(f\"切分后的文件数量：{len(split_docs)}\")\n",
    "\n",
    "print(f\"切分后的字符数（可以用来大致评估 token 数）：{sum([len(doc.page_content) for doc in split_docs])}\")\n",
    "\n",
    "\n",
    "split_docs[2].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Chroma向量库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "涉及到的API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain_chroma.vectordbs.Chroma\n",
    "# class langchain_chroma.vectordbs.Chroma(\n",
    "#     collection_name: str = 'langchain', \n",
    "# embedding_function: Embeddings | None = None, \n",
    "# persist_directory: str | None = None, \n",
    "# client_settings: Settings | None = None, \n",
    "# collection_metadata: Dict | None = None, \n",
    "# client: ClientAPI | None = None, \n",
    "# relevance_score_fn: Callable[[float], float] | None = None, \n",
    "# create_collection_if_not_exists: bool | None = True)\n",
    "\n",
    "\n",
    "# classmethod from_documents(\n",
    "#     documents: List[Document], \n",
    "# embedding: Embeddings | None = None, \n",
    "# ids: List[str] | None = None, \n",
    "# collection_name: str = 'langchain', \n",
    "# persist_directory: str | None = None, \n",
    "# client_settings: Settings | None = None, \n",
    "# client: ClientAPI | None = None, \n",
    "# collection_metadata: Dict | None = None, \n",
    "# **kwargs: Any)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接调用Chroma.from_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量数据库已成功持久化到磁盘。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\will\\AppData\\Local\\Temp\\ipykernel_16272\\3345607360.py:23: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "# 汇总\n",
    "\n",
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "# 定义持久化目录\n",
    "persist_directory = '../chromatest'\n",
    "\n",
    "# 创建嵌入模型\n",
    "embedding = ZhipuAIEmbeddings(model=\"embedding-2\", api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\")\n",
    "\n",
    "try:\n",
    "    # 初始化 Chroma 向量数据库\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=split_docs[:20],  # 为了速度，只选择前 20 个切分的 doc 进行生成\n",
    "        embedding=embedding,\n",
    "        # collection_name=\"test1\", # 如果不指定默认为langchain\n",
    "        persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上\n",
    "    )\n",
    "    \n",
    "    # 持久化向量数据库\n",
    "    vectordb.persist()\n",
    "    print(\"向量数据库已成功持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "charma服务器版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\",\n",
    "                model_name=\"embedding-2\"\n",
    "            )\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"test4\",\n",
    "    embedding_function=openai_ef,  # 使用openai\n",
    "    metadata={\n",
    "        \"hnsw:space\": \"cosine\", \n",
    "        \"hnsw:construction_ef\": 150,\n",
    "        \"hnsw:M\": 24,\n",
    "        \"hnsw:num_threads\": 8\n",
    "    }\n",
    ")\n",
    "ids_list = [f\"doc_{i+1}\" for i in range(len(split_docs[:20]))]\n",
    "print(ids_list)\n",
    "\n",
    "try:\n",
    "    # 添加文档到集合\n",
    "    collection.add(documents=split_docs[:20], ids =ids_list )\n",
    "    # 持久化向量数据库\n",
    "    vectordb.persist()\n",
    "    print(\"向量数据库已成功持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过chroma数据库add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "import os\n",
    "from langchain.vectordbs.chroma import Chroma\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "from chromadb import PersistentClient\n",
    "# 定义持久化目录\n",
    "persist_directory = './chroma6'\n",
    "# 初始化客户端（持久化模式）\n",
    "client = PersistentClient(path=persist_directory)\n",
    "\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\",\n",
    "                model_name=\"embedding-2\"\n",
    "            )\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"test4\",\n",
    "    embedding_function=openai_ef,  # 使用openai\n",
    "    metadata={\n",
    "        \"hnsw:space\": \"cosine\", \n",
    "        \"hnsw:construction_ef\": 150,\n",
    "        \"hnsw:M\": 24,\n",
    "        \"hnsw:num_threads\": 8\n",
    "    }\n",
    ")\n",
    "ids_list = [f\"doc_{i+1}\" for i in range(len(split_docs[:20]))]\n",
    "print(ids_list)\n",
    "\n",
    "try:\n",
    "    # 添加文档到集合\n",
    "    collection.add(documents=split_docs[:20], ids =ids_list )\n",
    "    # 持久化向量数据库\n",
    "    vectordb.persist()\n",
    "    print(\"向量数据库已成功持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过chroma数据库add-改良版 每次三个doc，分批次导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 汇总\n",
    "\n",
    "import os\n",
    "from langchain.vectordbs.chroma import Chroma\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "from chromadb import PersistentClient\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "# 定义持久化目录\n",
    "persist_directory = './chroma6'\n",
    "# 初始化客户端（持久化模式）\n",
    "client = PersistentClient(path=persist_directory)\n",
    "\n",
    "# 创建嵌入模型\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\",\n",
    "                model_name=\"embedding-2\"\n",
    "            )\n",
    "\n",
    "# zhipu_ef = ZhipuAIAdapter(model_name=\"embedding-2\", api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\")\n",
    "\n",
    "# 创建集合并配置HNSW参数\n",
    "# collection = client.create_collection( \n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"test3\",\n",
    "    embedding_function=openai_ef,\n",
    "    metadata={\n",
    "        \"hnsw:space\": \"cosine\", \n",
    "        \"hnsw:construction_ef\": 150,\n",
    "        \"hnsw:M\": 24,\n",
    "        \"hnsw:num_threads\": 8\n",
    "    }\n",
    ")\n",
    "\n",
    "ids_list = [f\"doc_{i+1}\" for i in range(len(split_docs[:20]))]\n",
    "print(ids_list)\n",
    "\n",
    "try:\n",
    "    # 添加文档到集合\n",
    "    # collection.add(documents=split_docs)\n",
    "\n",
    "    from itertools import batched\n",
    "    docs_batches = batched(split_docs[:20], 3)  # 分3个批次\n",
    "    ids_batches = batched(ids_list, 3)\n",
    "\n",
    "    for docs, ids in zip(docs_batches, ids_batches):\n",
    "        collection.add(documents=docs, ids=ids)\n",
    "    \n",
    "    # 持久化向量数据库\n",
    "    vectordb.persist()\n",
    "    print(\"向量数据库已成功持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量库中存储的数量：80\n"
     ]
    }
   ],
   "source": [
    "print(f\"向量库中存储的数量：{vectordb._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "连接chroma数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据库路径 ./chroma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client  = chromadb.PersistentClient(path=\"../chromatest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['langchain']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取一个存在的Collection对象\n",
    "collection = chroma_client.get_collection(\"langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count()  #  returns the number of items in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.peek()  # returns a list of the first 10 items in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取所有数据（默认不返回嵌入向量）\n",
    "all_data = collection.get()\n",
    "# print(all_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、向量检索\n",
    "### 3.1 相似度检索\n",
    "Chroma的相似度搜索使用的是余弦距离，即：\n",
    "$$\n",
    "similarity = cos(A, B) = \\frac{A \\cdot B}{\\parallel A \\parallel \\parallel B \\parallel} = \\frac{\\sum_1^n a_i b_i}{\\sqrt{\\sum_1^n a_i^2}\\sqrt{\\sum_1^n b_i^2}}\n",
    "$$\n",
    "其中$a_i$、$b_i$分别是向量$A$、$B$的分量。\n",
    "\n",
    "当你需要数据库返回严谨的按余弦相似度排序的结果时可以使用`similarity_search`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"什么是南瓜书\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的内容数：3\n"
     ]
    }
   ],
   "source": [
    "sim_docs = vectordb.similarity_search(question,k=3)\n",
    "print(f\"检索到的内容数：{len(sim_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的第0个内容: \n",
      "编委会\n",
      "主编：Sm1les、archwalker、jbb0523\n",
      "编委：juxiao、Majingmin、MrBigFan、shanry、Ye980226\n",
      "封面设计：构思-Sm1les、创作-林王茂盛\n",
      "致谢\n",
      "特别感谢awyd234、feijuan、Ggmatch、Heitao5200、huaqing89、LongJH、LilRachel、LeoLRH、Nono17、spareribs、suncha\n",
      "--------------\n",
      "检索到的第1个内容: \n",
      "编委会\n",
      "主编：Sm1les、archwalker、jbb0523\n",
      "编委：juxiao、Majingmin、MrBigFan、shanry、Ye980226\n",
      "封面设计：构思-Sm1les、创作-林王茂盛\n",
      "致谢\n",
      "特别感谢awyd234、feijuan、Ggmatch、Heitao5200、huaqing89、LongJH、LilRachel、LeoLRH、Nono17、spareribs、suncha\n",
      "--------------\n",
      "检索到的第2个内容: \n",
      "编委会\n",
      "主编：Sm1les、archwalker、jbb0523\n",
      "编委：juxiao、Majingmin、MrBigFan、shanry、Ye980226\n",
      "封面设计：构思-Sm1les、创作-林王茂盛\n",
      "致谢\n",
      "特别感谢awyd234、feijuan、Ggmatch、Heitao5200、huaqing89、LongJH、LilRachel、LeoLRH、Nono17、spareribs、suncha\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for i, sim_doc in enumerate(sim_docs):\n",
    "    print(f\"检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 MMR检索\n",
    "如果只考虑检索出内容的相关性会导致内容过于单一，可能丢失重要信息。\n",
    "\n",
    "最大边际相关性 (`MMR, Maximum marginal relevance`) 可以帮助我们在保持相关性的同时，增加内容的丰富度。\n",
    "\n",
    "核心思想是在已经选择了一个相关性高的文档之后，再选择一个与已选文档相关性较低但是信息丰富的文档。这样可以在保持相关性的同时，增加内容的多样性，避免过于单一的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr_docs = vectordb.max_marginal_relevance_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMR 检索到的第0个内容: \n",
      "编委会\n",
      "主编：Sm1les、archwalker、jbb0523\n",
      "编委：juxiao、Majingmin、MrBigFan、shanry、Ye980226\n",
      "封面设计：构思-Sm1les、创作-林王茂盛\n",
      "致谢\n",
      "特别感谢awyd234、feijuan、Ggmatch、Heitao5200、huaqing89、LongJH、LilRachel、LeoLRH、Nono17、spareribs、suncha\n",
      "--------------\n",
      "MMR 检索到的第1个内容: \n",
      "\u0001本\u0003:1.9.9\n",
      "发布日期:2023.03\n",
      "南⽠书\n",
      "PUMPKINBOOKDatawhale\n",
      "--------------\n",
      "MMR 检索到的第2个内容: \n",
      "前言\n",
      "“周志华老师的《机器学习》（西瓜书）是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读\n",
      "者通过西瓜书对机器学习有所了解,所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推\n",
      "导细节的读者来说可能“不太友好”，本书旨在对西瓜书里比较难理解的公式加以解析，以及对部分公式补充\n",
      "具体的推导细节。”\n",
      "读到这里，大家可能会疑问为啥前面这段话加了引号，因为这只是我们最初的遐想，后来我们\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for i, sim_doc in enumerate(mmr_docs):\n",
    "    print(f\"MMR 检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "# Cohere Rerank配置\n",
    "# https://docs.cohere.com/v2/reference/rerank\n",
    "import cohere\n",
    "cohere_client = cohere.Client(api_key=\"Tahx1eySFbKvu9sTyTXrRLf59la3ZUG9vy02stRZ\")\n",
    "compressor = CohereRerank(\n",
    "    client=cohere_client,\n",
    "    top_n=5  # 保留Top5相关文档\n",
    ")\n",
    "\n",
    "# 创建基础的检索器\n",
    "base_retriever = vectordb.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# 创建带重排序的检索器\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "# 5. LLM模型初始化\n",
    "# from openai import OpenAI \n",
    "\n",
    "# llm = OpenAI(\n",
    "#     api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\",\n",
    "#     base_url=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    "# ) \n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.95,\n",
    "    model=\"glm-4\",\n",
    "    openai_api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\",\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "\n",
    "# 6. 构建RAG链\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 提示模板设计\n",
    "template = \"\"\"基于以下上下文，用中文回答用户问题：\n",
    "上下文：{context}\n",
    "问题：{question}\n",
    "要求：答案需包含引用来源的页码（示例：[P3]）\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 组装RAG链\n",
    "rag_chain = (\n",
    "    {\"context\": compression_retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# 7. 使用示例\n",
    "response = rag_chain.invoke(\"机器学习\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# no rerank 和 rerank 的回答结果对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础检索器（无重排序）\n",
    "base_retriever = vectordb.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# 构建RAG链\n",
    "basic_rag_chain = (\n",
    "    {\"context\": base_retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 带重排序的检索器\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=CohereRerank(client=cohere_client, top_n=5),\n",
    "    base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "# 构建RAG链\n",
    "rerank_rag_chain = (\n",
    "    {\"context\": compression_retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试同一问题\n",
    "question = \"机器学习的主要应用场景有哪些？\"\n",
    "\n",
    "# 无Rerank的结果\n",
    "basic_response = basic_rag_chain.invoke(question)\n",
    "print(\"=== 无Rerank的答案 ===\")\n",
    "print(basic_response)\n",
    "\n",
    "# 带Rerank的结果\n",
    "rerank_response = rerank_rag_chain.invoke(question)\n",
    "print(\"\\n=== 带Rerank的答案 ===\")\n",
    "print(rerank_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# no rerank 和 rerank 的检索结果对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在原有代码基础上新增以下对比逻辑\n",
    "\n",
    "# 执行原始检索（无rerank）\n",
    "base_docs = base_retriever.invoke(\"机器学习\")\n",
    "print(\"=== Rerank前检索结果 ===\")\n",
    "for i, doc in enumerate(base_docs):\n",
    "    print(f\"[文档{i+1}] 相似度分：{doc.metadata.get('score', 'N/A')}\")\n",
    "    print(f\"内容摘要：{doc.page_content[:100]}...\")\n",
    "    print(f\"元数据：页码={doc.metadata.get('page', 'N/A')}, 来源={doc.metadata.get('source', 'N/A')}\\n\")\n",
    "\n",
    "# 执行带rerank的检索\n",
    "reranked_docs = compression_retriever.invoke(\"机器学习\")\n",
    "print(\"\\n=== Rerank后检索结果 ===\")\n",
    "for i, doc in enumerate(reranked_docs):\n",
    "    print(f\"[文档{i+1}] 重排序分：{doc.metadata.get('relevance_score', 'N/A')}\")\n",
    "    print(f\"内容摘要：{doc.page_content[:100]}...\")\n",
    "    print(f\"元数据：页码={doc.metadata.get('page', 'N/A')}, 来源={doc.metadata.get('source', 'N/A')}\\n\")\n",
    "\n",
    "# 原有RAG流程保持不变\n",
    "response = rag_chain.invoke(\"机器学习\")\n",
    "print(\"\\n=== 最终答案 ===\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
