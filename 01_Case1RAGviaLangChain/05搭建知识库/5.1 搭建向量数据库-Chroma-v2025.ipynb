{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建并使用向量数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前序embedding模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZhipuAIEmbeddings模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "zhipu_embed = ZhipuAIEmbeddings(\n",
    "    model=\"embedding-2\",\n",
    "    api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前序数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量处理文件夹中所有文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data_base/knowledge_path\\\\pumkin_book\\\\pumpkin_book.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# from dotenv import load_dotenv, find_dotenv \n",
    "# pip install python-dotenv\n",
    "\n",
    "# _ = load_dotenv(find_dotenv())\n",
    "\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "zhipu_embed = ZhipuAIEmbeddings(\n",
    "    model=\"embedding-2\",\n",
    "    api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\",\n",
    ")\n",
    "# 获取folder_path下所有文件路径，储存在file_paths里\n",
    "file_paths = []\n",
    "folder_path = '../data_base/knowledge_path'\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_paths.append(file_path)\n",
    "print(file_paths)\n",
    "\n",
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "# from langchain.document_loaders.markdown import UnstructuredMarkdownLoader\n",
    "\n",
    "# 遍历文件路径并把实例化的loader存放在loaders里\n",
    "loaders = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    file_type = file_path.split('.')[-1]\n",
    "    if file_type == 'pdf':\n",
    "        loaders.append(PyMuPDFLoader(file_path))\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {file_type} for file {file_path}\")\n",
    "\n",
    "# 下载文件并存储到text\n",
    "# 作数据清洗\n",
    "# 修改后的数据清洗部分（替换原始代码中对应段落）\n",
    "import re\n",
    "\n",
    "# 预编译正则表达式（提升效率）\n",
    "linebreak_pattern = re.compile(\n",
    "    r'(?<![\\\\u4e00-\\\\u9fff])\\n(?![\\\\u4e00-\\\\u9fff])',  # 负向断言匹配非中文环境换行\n",
    "    flags=re.DOTALL\n",
    ")\n",
    "space_pattern = re.compile(r'[ 　]+')  # 匹配半角/全角空格\n",
    "special_chars = ['•', '▪', '▫', '▶', '®', '©']  # 可扩展的干扰符号列表\n",
    "\n",
    "# 替换原始代码中的清洗循环\n",
    "for text in texts:\n",
    "    # 1. 清理非中文环境换行\n",
    "    text.page_content = re.sub(\n",
    "        linebreak_pattern,\n",
    "        lambda m: m.group().replace('\\n', ''),\n",
    "        text.page_content\n",
    "    )\n",
    "    \n",
    "    # 2. 批量清理特殊符号\n",
    "    for char in special_chars:\n",
    "        text.page_content = text.page_content.replace(char, '')\n",
    "    \n",
    "    # 3. 安全删除空格（保留URL等特殊场景）\n",
    "    text.page_content = space_pattern.sub('', text.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'→_→欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》←_←Vθ(x)是一个标量，θ是一个向量，∂Vθ(x)∂θ属于矩阵微积分中的标量对向量求偏导，因此∂Vθ(x)∂θ=∂θTx∂θ=\\n\"∂θTx∂θ1,∂θTx∂θ2,···,∂θTx∂θn#T=[x1,x2,···,xm]T=x故−∂Eθ∂θ=Ex∼π\\x14\\n2(Vπ(x)−Vθ(x))∂Vθ(x)∂θ\\x15\\n=Ex∼π[2(Vπ(x)−Vθ(x))x]16.6\\n模仿学习本节公式无复杂推导，在此不做赘述。参考文献\\n[1]王琦，杨毅远，江季.EasyRL：强化学习教程.人民邮电出版社,2022.→_→配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU←_←'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# # from dotenv import load_dotenv, find_dotenv \n",
    "# # pip install python-dotenv\n",
    "\n",
    "# # _ = load_dotenv(find_dotenv())\n",
    "\n",
    "# from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "# zhipu_embed = ZhipuAIEmbeddings(\n",
    "#     model=\"embedding-2\",\n",
    "#     api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\",\n",
    "# )\n",
    "# # 获取folder_path下所有文件路径，储存在file_paths里\n",
    "# file_paths = []\n",
    "# folder_path = '../data_base/knowledge_path'\n",
    "# for root, dirs, files in os.walk(folder_path):\n",
    "#     for file in files:\n",
    "#         file_path = os.path.join(root, file)\n",
    "#         file_paths.append(file_path)\n",
    "# print(file_paths)\n",
    "\n",
    "# from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "# # from langchain.document_loaders.markdown import UnstructuredMarkdownLoader\n",
    "\n",
    "# # 遍历文件路径并把实例化的loader存放在loaders里\n",
    "# loaders = []\n",
    "\n",
    "# for file_path in file_paths:\n",
    "#     file_type = file_path.split('.')[-1]\n",
    "#     if file_type == 'pdf':\n",
    "#         loaders.append(PyMuPDFLoader(file_path))\n",
    "#     else:\n",
    "#         print(f\"Unsupported file type: {file_type} for file {file_path}\")\n",
    "\n",
    "# # 下载文件并存储到text\n",
    "# # 作数据清洗\n",
    "# from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "# import re\n",
    "# # 下载文件并存储到text\n",
    "# texts = []\n",
    "# for loader in loaders:\n",
    "#     texts.extend(loader.load())   \n",
    "\n",
    "# for text in texts:\n",
    "#     pattern = re.compile(r'[^\\u4e00-\\u9fff](\\n)[^\\u4e00-\\u9fff]', re.DOTALL)\n",
    "#     text.page_content = re.sub(pattern, lambda match: match.group(0).replace('\\n', ''), text.page_content)\n",
    "#     text.page_content = text.page_content.replace('•', '')\n",
    "#     text.page_content = text.page_content.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "文档split成功\n",
      "切分后的文件数量：640\n",
      "切分后的字符数（可以用来大致评估 token 数）：243866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'式(2.20)的推导在推导式(2.20)之前，需要先弄清楚ROC曲线的具体绘制过程。下面我们就举个例子，按照“西瓜书”图2.4下方给出的绘制方法来讲解一下ROC曲线的具体绘制过程。假设我们已经训练得到一个学习器f(s)，现在用该学习器来对8个测试样本（4个正例，4个反例，即m+=m−=4）进行预测，预测结果为（此处用s表示样本，以和坐标(x,y)作出区分）：(s1,0.77,+),(s2,0.62,−),(s3,0.58,+),(s4,0.47,+),(s5,0.47,−),(s6,0.33,−),(s7,0.23,+),(s8,0.15,−)其中，+和−分别表示样本为正例和为反例，数字表示学习器f预测该样本为正例的概率，例如对于反例s2来说，当前学习器f(s)预测它是正例的概率为0.62。根据“西瓜书”上给出的绘制方法，首先需要对所有测试样本按照学习器给出的预测结果进行排序（上面给出的预测结果已经按照预测值从大到小排序），接着将分类阈值设为一个不可能取到的超大值，例如设为1。显然，此时所有样本预测为正例的概率都一定小于分类阈值，那么预测为正例的样本个数为0，相应的真正例率和假正例率也都为0，所以我们可以在'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "''' \n",
    "* RecursiveCharacterTextSplitter 递归字符文本分割\n",
    "RecursiveCharacterTextSplitter 将按不同的字符递归地分割(按照这个优先级[\"\\n\\n\", \"\\n\", \" \", \"\"])，\n",
    "    这样就能尽量把所有和语义相关的内容尽可能长时间地保留在同一位置\n",
    "RecursiveCharacterTextSplitter需要关注的是4个参数：\n",
    "\n",
    "* separators - 分隔符字符串数组\n",
    "* chunk_size - 每个文档的字符数量限制\n",
    "* chunk_overlap - 两份文档重叠区域的长度\n",
    "* length_function - 长度计算函数\n",
    "'''\n",
    "#导入文本分割器\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 知识库中单段文本长度\n",
    "CHUNK_SIZE = 512\n",
    "# 知识库中相邻文本重合长度\n",
    "OVERLAP_SIZE = 50\n",
    "\n",
    "# 使用递归字符文本分割器\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=OVERLAP_SIZE\n",
    ")\n",
    "# text_splitter.split_text(text.page_content[0:1000])\n",
    "split_docs = text_splitter.split_documents(texts)\n",
    "\n",
    "print(f\"---\\n文档split成功\")\n",
    "\n",
    "\n",
    "print(f\"切分后的文件数量：{len(split_docs)}\")\n",
    "\n",
    "print(f\"切分后的字符数（可以用来大致评估 token 数）：{sum([len(doc.page_content) for doc in split_docs])}\")\n",
    "\n",
    "\n",
    "split_docs[100].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'xdvipdfmx (20200315)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-03T17:07:09+00:00', 'source': '../data_base/knowledge_path\\\\pumkin_book\\\\pumpkin_book.pdf', 'file_path': '../data_base/knowledge_path\\\\pumkin_book\\\\pumpkin_book.pdf', 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20230303170709-00'00'\", 'page': 20}, page_content='式的推导在推导式之前，需要先弄清楚曲线的具体绘制过程。下面我们就举个例子，按照西瓜书图下方给出的绘制方法来讲解一下曲线的具体绘制过程。假设我们已经训练得到一个学习器，现在用该学习器来对个测试样本个正例，个反例，即进行预测，预测结果为此处用表示样本，以和坐标作出区分：其中，和分别表示样本为正例和为反例，数字表示学习器预测该样本为正例的概率，例如对于反例来说，当前学习器预测它是正例的概率为。根据西瓜书上给出的绘制方法，首先需要对所有测试样本按照学习器给出的预测结果进行排序上面给出的预测结果已经按照预测值从大到小排序，接着将分类阈值设为一个不可能取到的超大值，例如设为。显然，此时所有样本预测为正例的概率都一定小于分类阈值，那么预测为正例的样本个数为，相应的真正例率和假正例率也都为，所以我们可以在')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Chroma向量库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "涉及到的API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain_chroma.vectordbs.Chroma\n",
    "# class langchain_chroma.vectordbs.Chroma(\n",
    "#     collection_name: str = 'langchain', \n",
    "# embedding_function: Embeddings | None = None, \n",
    "# persist_directory: str | None = None, \n",
    "# client_settings: Settings | None = None, \n",
    "# collection_metadata: Dict | None = None, \n",
    "# client: ClientAPI | None = None, \n",
    "# relevance_score_fn: Callable[[float], float] | None = None, \n",
    "# create_collection_if_not_exists: bool | None = True)\n",
    "\n",
    "\n",
    "# classmethod from_documents(\n",
    "#     documents: List[Document], \n",
    "# embedding: Embeddings | None = None, \n",
    "# ids: List[str] | None = None, \n",
    "# collection_name: str = 'langchain', \n",
    "# persist_directory: str | None = None, \n",
    "# client_settings: Settings | None = None, \n",
    "# client: ClientAPI | None = None, \n",
    "# collection_metadata: Dict | None = None, \n",
    "# **kwargs: Any)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接调用Chroma.from_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量数据库已成功持久化到磁盘。\n"
     ]
    }
   ],
   "source": [
    "# 汇总\n",
    "\n",
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "# 定义持久化目录\n",
    "persist_directory = '../chroma-will'\n",
    "\n",
    "# 创建嵌入模型\n",
    "embedding = ZhipuAIEmbeddings(model=\"embedding-2\", api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\")\n",
    "\n",
    "try:\n",
    "    # 初始化 Chroma 向量数据库\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=split_docs[:20],  # 为了速度，只选择前 20 个切分的 doc 进行生成\n",
    "        embedding=embedding,\n",
    "        # collection_name=\"test1\", # 如果不指定默认为langchain\n",
    "        persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上\n",
    "    )\n",
    "    \n",
    "    # 持久化向量数据库\n",
    "    vectordb.persist()\n",
    "    print(\"向量数据库已成功持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "charma服务器版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\",\n",
    "                model_name=\"embedding-2\"\n",
    "            )\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"test4\",\n",
    "    embedding_function=openai_ef,  # 使用openai\n",
    "    metadata={\n",
    "        \"hnsw:space\": \"cosine\", \n",
    "        \"hnsw:construction_ef\": 150,\n",
    "        \"hnsw:M\": 24,\n",
    "        \"hnsw:num_threads\": 8\n",
    "    }\n",
    ")\n",
    "ids_list = [f\"doc_{i+1}\" for i in range(len(split_docs[:20]))]\n",
    "print(ids_list)\n",
    "\n",
    "try:\n",
    "    # 添加文档到集合\n",
    "    collection.add(documents=split_docs[:20], ids =ids_list )\n",
    "    # 持久化向量数据库\n",
    "    vectordb.persist()\n",
    "    print(\"向量数据库已成功持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过chroma数据库add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "content_list = ['本发布日期南书', \n",
    "                '前言周志华老师的《机器学习》西瓜书是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读者通过西瓜书对机器学习有所了解所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推导细节的读者来说可能不太友好，本书旨在对西瓜书里比较难理解的公式加以解析，以及对部分公式补充具体的推导细节。读到这里，大家可能会疑问为啥前面这段话加了引号，因为这只是我们最初的遐想，后来我们了解到，周老师之所以省去这些推导细节的真实原因是，他本尊认为理工科数学基础扎实点的大二下学生应该对西瓜书中的推导细节无困难吧，要点在书里都有了，略去的细节应能脑补或做练习。所以本南瓜书只能算是我等数学渣渣在自学的时候记下来的笔记，希望能够帮助大家都成为一名合格的理工科数学基础扎实点的大二下学生。使用说明南瓜书的所有内容都是以西瓜书的内容为前置知识进行表述的，所以南瓜书的最佳使用方法是以西瓜书为主线，遇到自己推导不出来或者看不懂的公式时再来查阅南瓜书；对于初学机器学习的小白，西瓜书第章和第章的公式强烈不建议深究，简单过一下即可，等你学得', \n",
    "                '有点飘的时候再回来啃都来得及；每个公式的解析和推导我们都力争以本科数学基础的视角进行讲解，所以超纲的数学知识我们通常都会以附录和参考文献的形式给出，感兴趣的同学可以继续沿着我们给的资料进行深入学习；若南瓜书里没有你想要查阅的公式，或者你发现南瓜书哪个地方有错误，请毫不犹豫地去我们的地址：进行反馈，在对应版块提交你希望补充的公式编号或者勘误信息，我们通常会在小时以内给您回复，超过小时未回复的话可以微信联系我们微信号：；配套视频教程：在线阅读地址：仅供第版最新版获取地址：编委会', \n",
    "                '编委会主编：、、编委：、、、、封面设计：构思、创作林王茂盛致谢特别感谢、、、、、、、、、、、在最早期的时候对南瓜书所做的贡献。扫描下方二维码，然后回复关键词南瓜书，即可加入南瓜书读者交流群版权声明本作品采用知识共享署名非商业性使用相同方式共享国际许可协议进行许可。'\n",
    "               ]\n",
    "print(len(content_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=[\"doc1\", \"doc2\", \"doc3\", ...],\n",
    "    embeddings=[[1.1, 2.3, 3.2], [4.5, 6.9, 4.4], [1.1, 2.3, 3.2], ...],\n",
    "    metadatas=[{\"chapter\": \"3\", \"verse\": \"16\"}, {\"chapter\": \"3\", \"verse\": \"5\"}, {\"chapter\": \"29\", \"verse\": \"11\"}, ...],\n",
    "    ids=[\"id1\", \"id2\", \"id3\", ...]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\will\\.cache\\chroma\\onnx_models\\all-MiniLM-L6-v2\\onnx.tar.gz: 100%|███████| 79.3M/79.3M [05:29<00:00, 253kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据写入成功，已自动持久化到磁盘。\n"
     ]
    }
   ],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "import os\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "import chromadb\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "from chromadb import PersistentClient\n",
    "# 定义持久化目录\n",
    "persist_directory = '../chroma0407'\n",
    "# 初始化客户端（持久化模式）\n",
    "client = PersistentClient(path=persist_directory)\n",
    "\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\",\n",
    "                model_name=\"embedding-2\"\n",
    "            )\n",
    "# collection = client.get_or_create_collection(\n",
    "#     name=\"test\",\n",
    "#     embedding_function=openai_ef,  # 使用openai\n",
    "#     metadata={\n",
    "#         \"hnsw:space\": \"cosine\", \n",
    "#         \"hnsw:construction_ef\": 150,\n",
    "#         \"hnsw:M\": 24,\n",
    "#         \"hnsw:num_threads\": 8\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# HNSW索引构建参数优化\n",
    "collection = client.create_collection(\n",
    "    name=\"high_perf\",\n",
    "    metadata={\n",
    "        \"hnsw:construction_ef\": 200,  # 构建阶段搜索范围（默认100）\n",
    "        \"hnsw:search_ef\": 320,        # 查询阶段搜索范围 \n",
    "        \"hnsw:M\": 48                  # 图节点最大连接数\n",
    "    }\n",
    ")\n",
    "\n",
    "content_list = ['本发布日期南书', \n",
    "                '前言周志华老师的《机器学习》西瓜书是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读者通过西瓜书对机器学习有所了解所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推导细节的读者来说可能不太友好，本书旨在对西瓜书里比较难理解的公式加以解析，以及对部分公式补充具体的推导细节。读到这里，大家可能会疑问为啥前面这段话加了引号，因为这只是我们最初的遐想，后来我们了解到，周老师之所以省去这些推导细节的真实原因是，他本尊认为理工科数学基础扎实点的大二下学生应该对西瓜书中的推导细节无困难吧，要点在书里都有了，略去的细节应能脑补或做练习。所以本南瓜书只能算是我等数学渣渣在自学的时候记下来的笔记，希望能够帮助大家都成为一名合格的理工科数学基础扎实点的大二下学生。使用说明南瓜书的所有内容都是以西瓜书的内容为前置知识进行表述的，所以南瓜书的最佳使用方法是以西瓜书为主线，遇到自己推导不出来或者看不懂的公式时再来查阅南瓜书；对于初学机器学习的小白，西瓜书第章和第章的公式强烈不建议深究，简单过一下即可，等你学得', \n",
    "                '有点飘的时候再回来啃都来得及；每个公式的解析和推导我们都力争以本科数学基础的视角进行讲解，所以超纲的数学知识我们通常都会以附录和参考文献的形式给出，感兴趣的同学可以继续沿着我们给的资料进行深入学习；若南瓜书里没有你想要查阅的公式，或者你发现南瓜书哪个地方有错误，请毫不犹豫地去我们的地址：进行反馈，在对应版块提交你希望补充的公式编号或者勘误信息，我们通常会在小时以内给您回复，超过小时未回复的话可以微信联系我们微信号：；配套视频教程：在线阅读地址：仅供第版最新版获取地址：编委会', \n",
    "                '编委会主编：、、编委：、、、、封面设计：构思、创作林王茂盛致谢特别感谢、、、、、、、、、、、在最早期的时候对南瓜书所做的贡献。扫描下方二维码，然后回复关键词南瓜书，即可加入南瓜书读者交流群版权声明本作品采用知识共享署名非商业性使用相同方式共享国际许可协议进行许可。'\n",
    "               ]\n",
    "# print(len(content_list))\n",
    "\n",
    "# 如果每个文档块的元数据需要个性化（例如记录分块编号），可以生成与文档数量匹配的元数据列表：\n",
    "# metadatas = [{\"source\": \"权威文档.pdf\", \"chunk_id\": i} for i in range(len(split_docs[:20]))]\n",
    "\n",
    "# 如果所有文档块的元数据相同（例如来源一致），可通过列表推导式生成等长元数据列表：\n",
    "metadatas = [{\"source\": \"权威文档.pdf\"} for _ in range(len(split_docs[:20]))]\n",
    "\n",
    "ids_list = [f\"doc_{i+1}\" for i in range(len(content_list))]\n",
    "# print(ids_list)\n",
    "\n",
    "\n",
    "# print(metadatas)\n",
    "\n",
    "try:\n",
    "    # 添加文档到集合（修正后的参数）\n",
    "    collection.add(\n",
    "        documents=content_list,  # 传入纯文本列表\n",
    "        metadatas=[{\"source\": \"权威文档.pdf\"},{\"source\": \"权威文档.pdf\"},{\"source\": \"权威文档.pdf\"},{\"source\": \"权威文档.pdf\"} ],\n",
    "        ids=ids_list\n",
    "    )\n",
    "    print(\"数据写入成功，已自动持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")\n",
    "except chromadb.errors.ChromaError as ce:\n",
    "    print(f\"数据库错误: {ce}\")\n",
    "except ValueError as ve:\n",
    "    print(f\"数据格式错误: {ve}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: doc_1\n",
      "Insert of existing embedding ID: doc_2\n",
      "Insert of existing embedding ID: doc_3\n",
      "Insert of existing embedding ID: doc_4\n",
      "Insert of existing embedding ID: doc_5\n",
      "Insert of existing embedding ID: doc_6\n",
      "Insert of existing embedding ID: doc_7\n",
      "Insert of existing embedding ID: doc_8\n",
      "Insert of existing embedding ID: doc_9\n",
      "Insert of existing embedding ID: doc_10\n",
      "Insert of existing embedding ID: doc_11\n",
      "Insert of existing embedding ID: doc_12\n",
      "Insert of existing embedding ID: doc_13\n",
      "Insert of existing embedding ID: doc_14\n",
      "Insert of existing embedding ID: doc_15\n",
      "Insert of existing embedding ID: doc_16\n",
      "Insert of existing embedding ID: doc_17\n",
      "Insert of existing embedding ID: doc_18\n",
      "Insert of existing embedding ID: doc_19\n",
      "Insert of existing embedding ID: doc_20\n",
      "Add of existing embedding ID: doc_1\n",
      "Add of existing embedding ID: doc_2\n",
      "Add of existing embedding ID: doc_3\n",
      "Add of existing embedding ID: doc_4\n",
      "Add of existing embedding ID: doc_5\n",
      "Add of existing embedding ID: doc_6\n",
      "Add of existing embedding ID: doc_7\n",
      "Add of existing embedding ID: doc_8\n",
      "Add of existing embedding ID: doc_9\n",
      "Add of existing embedding ID: doc_10\n",
      "Add of existing embedding ID: doc_11\n",
      "Add of existing embedding ID: doc_12\n",
      "Add of existing embedding ID: doc_13\n",
      "Add of existing embedding ID: doc_14\n",
      "Add of existing embedding ID: doc_15\n",
      "Add of existing embedding ID: doc_16\n",
      "Add of existing embedding ID: doc_17\n",
      "Add of existing embedding ID: doc_18\n",
      "Add of existing embedding ID: doc_19\n",
      "Add of existing embedding ID: doc_20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据写入成功，已自动持久化到磁盘。\n"
     ]
    }
   ],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "import os\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "import chromadb\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "from chromadb import PersistentClient\n",
    "# 定义持久化目录\n",
    "persist_directory = '../chroma0406'\n",
    "# 初始化客户端（持久化模式）\n",
    "client = PersistentClient(path=persist_directory)\n",
    "\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\",\n",
    "                model_name=\"embedding-2\"\n",
    "            )\n",
    "# collection = client.get_or_create_collection(\n",
    "#     name=\"test_1\",\n",
    "#     embedding_function=openai_ef,  # 使用openai\n",
    "#     metadata={\n",
    "#         \"hnsw:space\": \"cosine\", \n",
    "#         \"hnsw:construction_ef\": 150,\n",
    "#         \"hnsw:M\": 24,\n",
    "#         \"hnsw:num_threads\": 8\n",
    "#     }\n",
    "# )\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"high_perf_1\",\n",
    "    metadata={\n",
    "        \"hnsw:construction_ef\": 200,  # 构建阶段搜索范围（默认100）\n",
    "        \"hnsw:search_ef\": 320,        # 查询阶段搜索范围 \n",
    "        \"hnsw:M\": 48                  # 图节点最大连接数\n",
    "    }\n",
    ")\n",
    "\n",
    "content_list = [doc.page_content for doc in split_docs[:20]]\n",
    "\n",
    "# 如果每个文档块的元数据需要个性化（例如记录分块编号），可以生成与文档数量匹配的元数据列表：\n",
    "# metadatas = [{\"source\": \"权威文档.pdf\", \"chunk_id\": i} for i in range(len(split_docs[:20]))]\n",
    "\n",
    "# 如果所有文档块的元数据相同（例如来源一致），可通过列表推导式生成等长元数据列表：\n",
    "metadatas = [{\"source\": \"权威文档.pdf\"} for _ in range(len(split_docs[:20]))]\n",
    "\n",
    "ids_list = [f\"doc_{i+1}\" for i in range(len(split_docs[:20]))]\n",
    "# print(ids_list)\n",
    "\n",
    "\n",
    "# print(metadatas)\n",
    "\n",
    "try:\n",
    "    # 添加文档到集合（修正后的参数）\n",
    "    collection.add(\n",
    "        documents=content_list,  # 传入纯文本列表\n",
    "        metadatas=metadatas, \n",
    "        ids=ids_list\n",
    "    )\n",
    "    print(\"数据写入成功，已自动持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")\n",
    "except chromadb.errors.ChromaError as ce:\n",
    "    print(f\"数据库错误: {ce}\")\n",
    "except ValueError as ve:\n",
    "    print(f\"数据格式错误: {ve}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过chroma数据库add-改良版 每次三个doc，分批次导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 汇总\n",
    "\n",
    "import os\n",
    "# from langchain.vectordbs.chroma import Chroma\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "from chromadb import PersistentClient\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "# 定义持久化目录\n",
    "persist_directory = './chroma7'\n",
    "# 初始化客户端（持久化模式）\n",
    "client = PersistentClient(path=persist_directory)\n",
    "\n",
    "# 创建嵌入模型\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\",\n",
    "                model_name=\"embedding-2\"\n",
    "            )\n",
    "\n",
    "# zhipu_ef = ZhipuAIAdapter(model_name=\"embedding-2\", api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\")\n",
    "\n",
    "# 创建集合并配置HNSW参数\n",
    "# collection = client.create_collection( \n",
    "# collection = client.get_or_create_collection(\n",
    "#     name=\"test4\",\n",
    "#     embedding_function=openai_ef,\n",
    "#     metadata={\n",
    "#         \"hnsw:space\": \"cosine\", \n",
    "#         \"hnsw:construction_ef\": 150,\n",
    "#         \"hnsw:M\": 24,\n",
    "#         \"hnsw:num_threads\": 8\n",
    "#     }\n",
    "# )\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name=\"high_perf_3\",\n",
    "    metadata={\n",
    "        \"hnsw:construction_ef\": 200,  # 构建阶段搜索范围（默认100）\n",
    "        \"hnsw:search_ef\": 320,        # 查询阶段搜索范围 \n",
    "        \"hnsw:M\": 48                  # 图节点最大连接数\n",
    "    }\n",
    ")\n",
    "content_list = [doc.page_content for doc in split_docs[:20]]\n",
    "\n",
    "# 如果每个文档块的元数据需要个性化（例如记录分块编号），可以生成与文档数量匹配的元数据列表：\n",
    "# metadatas = [{\"source\": \"权威文档.pdf\", \"chunk_id\": i} for i in range(len(split_docs[:20]))]\n",
    "\n",
    "# 如果所有文档块的元数据相同（例如来源一致），可通过列表推导式生成等长元数据列表：\n",
    "metadatas = [{\"source\": \"权威文档.pdf\"} for _ in range(len(split_docs[:20]))]\n",
    "\n",
    "ids_list = [f\"doc_{i+1}\" for i in range(len(split_docs[:20]))]\n",
    "# print(ids_list)\n",
    "\n",
    "\n",
    "try:\n",
    "    # 添加文档到集合\n",
    "    # collection.add(documents=split_docs)\n",
    "\n",
    "    from itertools import batched\n",
    "    docs_batches = batched(content_list, 3)  # 分3个批次\n",
    "    ids_batches = batched(ids_list, 3)\n",
    "\n",
    "    for docs, ids in zip(docs_batches, ids_batches):\n",
    "        collection.add(\n",
    "            documents=docs, \n",
    "            ids=ids\n",
    "        )\n",
    "    \n",
    "    # 持久化向量数据库\n",
    "    vectordb.persist()\n",
    "    print(\"向量数据库已成功持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"向量库中存储的数量：{vectordb._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "连接chroma数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据库路径 ./chroma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client  = chromadb.PersistentClient(path=\"../chroma-will\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['langchain']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取一个存在的Collection对象\n",
    "collection = chroma_client.get_collection(\"langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count()  #  returns the number of items in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.peek()  # returns a list of the first 10 items in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取所有数据（默认不返回嵌入向量）\n",
    "all_data = collection.get()\n",
    "# print(all_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、向量检索\n",
    "### 3.1 相似度检索\n",
    "Chroma的相似度搜索使用的是余弦距离，即：\n",
    "$$\n",
    "similarity = cos(A, B) = \\frac{A \\cdot B}{\\parallel A \\parallel \\parallel B \\parallel} = \\frac{\\sum_1^n a_i b_i}{\\sqrt{\\sum_1^n a_i^2}\\sqrt{\\sum_1^n b_i^2}}\n",
    "$$\n",
    "其中$a_i$、$b_i$分别是向量$A$、$B$的分量。\n",
    "\n",
    "当你需要数据库返回严谨的按余弦相似度排序的结果时可以使用`similarity_search`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"什么是南瓜书\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的内容数：3\n"
     ]
    }
   ],
   "source": [
    "sim_docs = vectordb.similarity_search(question,k=3)\n",
    "print(f\"检索到的内容数：{len(sim_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的第0个内容: \n",
      "编委：、、、、封面设计：构思、创作林王茂盛致谢特别感谢、、、、、、、、、、、在最早期的时候对南瓜书所做的贡献。扫描下方二维码，然后回复关键词南瓜书，即可加入南瓜书读者交流群版权声明本作品采用知识共享署名非商业性使用相同方式共享国际许可协议进行许可。\n",
      "--------------\n",
      "检索到的第1个内容: \n",
      "本发布日期南书\n",
      "--------------\n",
      "检索到的第2个内容: \n",
      "前言周志华老师的《机器学习》西瓜书是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读者通过西瓜书对机器学习有所了解所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推导细节的读者来说可能不太友好，本书旨在对西瓜书里比较难理解的公式加以解析，以及对部分公式补充具体的推导细节。读到这里，大家可能会疑问为啥前面这段话加了引号，因为这只是我们最初的遐想，后来我们了解到，周老师之所以省去\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for i, sim_doc in enumerate(sim_docs):\n",
    "    print(f\"检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 MMR检索\n",
    "如果只考虑检索出内容的相关性会导致内容过于单一，可能丢失重要信息。\n",
    "\n",
    "最大边际相关性 (`MMR, Maximum marginal relevance`) 可以帮助我们在保持相关性的同时，增加内容的丰富度。\n",
    "\n",
    "核心思想是在已经选择了一个相关性高的文档之后，再选择一个与已选文档相关性较低但是信息丰富的文档。这样可以在保持相关性的同时，增加内容的多样性，避免过于单一的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr_docs = vectordb.max_marginal_relevance_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMR 检索到的第0个内容: \n",
      "编委：、、、、封面设计：构思、创作林王茂盛致谢特别感谢、、、、、、、、、、、在最早期的时候对南瓜书所做的贡献。扫描下方二维码，然后回复关键词南瓜书，即可加入南瓜书读者交流群版权声明本作品采用知识共享署名非商业性使用相同方式共享国际许可协议进行许可。\n",
      "--------------\n",
      "MMR 检索到的第1个内容: \n",
      "本发布日期南书\n",
      "--------------\n",
      "MMR 检索到的第2个内容: \n",
      "凸优化问题条件拉格朗日对偶函数拉格朗日对偶问题式和式的推导式的推导式的解释核函数式的解释\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for i, sim_doc in enumerate(mmr_docs):\n",
    "    print(f\"MMR 检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
