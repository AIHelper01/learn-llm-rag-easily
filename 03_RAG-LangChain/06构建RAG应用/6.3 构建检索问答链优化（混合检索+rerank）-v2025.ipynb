{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac93052e-aa6c-4591-bd81-725585592866",
   "metadata": {},
   "source": [
    "# 混合检索（Vilnvsembdedding + BM25）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299cfc86-b632-4fc9-bd77-f2cb9478a260",
   "metadata": {},
   "source": [
    "## 固定权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c31bec-ebdd-427c-9da5-aa8d5f42d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, Collection\n",
    "\n",
    "\n",
    "def get_text_list_from_milvus(\n",
    "        collection_name: str,\n",
    "        host: str = \"192.168.0.188\",\n",
    "        port: str = \"19530\",\n",
    "        expr: str = \"\",\n",
    "        limit: int = 1000,\n",
    "        output_fields: list = [\"text\"],\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    从 Milvus 集合中读取指定字段（默认是 text）并返回列表\n",
    "\n",
    "    Args:\n",
    "        collection_name: Milvus 集合名称\n",
    "        host: Milvus 服务器地址（默认 localhost）\n",
    "        port: Milvus 端口（默认 19530）\n",
    "        expr: 过滤条件表达式（默认无过滤）\n",
    "        limit: 返回数据条数上限（默认 1000）\n",
    "        output_fields: 要提取的字段列表（默认 [\"text\"]）\n",
    "\n",
    "    Returns:\n",
    "        list: 包含目标字段值的列表\n",
    "    \"\"\"\n",
    "    # 1. 连接 Milvus\n",
    "    connections.connect(alias=\"default\", host=host, port=port)\n",
    "\n",
    "    # 2. 加载集合\n",
    "    collection = Collection(name=collection_name)\n",
    "    collection.load()\n",
    "\n",
    "    # 3. 查询数据\n",
    "    results = collection.query(\n",
    "            expr=expr,\n",
    "            output_fields=output_fields,\n",
    "            limit=limit\n",
    "        )\n",
    "\n",
    "    # 4. 提取目标字段为列表\n",
    "    if not output_fields:\n",
    "        raise ValueError(\"output_fields 不能为空\")\n",
    "\n",
    "    field_name = output_fields[0]  # 默认取第一个字段\n",
    "    data_list = [item[field_name] for item in results]\n",
    "    return data_list\n",
    "\n",
    "\n",
    "# 示例调用\n",
    "if __name__ == \"__main__\":\n",
    "    # 示例1：读取默认的 text 字段\n",
    "    texts = get_text_list_from_milvus(collection_name=\"Vmaxs\")\n",
    "    print(f\"获取 {len(texts)} 条文本，前5条: {texts[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef7095-536d-4309-9948-61f4290ddfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores import Milvus\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ContextualCompressionRetriever, EnsembleRetriever\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from pymilvus import Collection, connections\n",
    "\n",
    "# Initialize memory outside the function so it persists across questions\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# 初始化 Milvus 向量数据库\n",
    "def get_vectordb():\n",
    "    emb_bgem3 = OllamaEmbeddings(base_url='http://localhost:11434', model=\"bge-m3:latest\")\n",
    "\n",
    "    # Milvus 连接参数\n",
    "    vectordb = Milvus(\n",
    "        embedding_function=emb_bgem3,\n",
    "        collection_name=\"Vmaxs\",  # Milvus 集合名称\n",
    "        connection_args={\n",
    "            \"host\": \"192.168.0.188\",  # Milvus 服务器地址\n",
    "            \"port\": \"19530\",  # Milvus 默认端口\n",
    "        },\n",
    "    )\n",
    "    return vectordb\n",
    "\n",
    "def get_llm():\n",
    "    return OllamaLLM(base_url='http://localhost:11434', model='deepseek-r1:1.5b', temperature=0.1, streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "def get_text_list_from_milvus(\n",
    "        collection_name: str,\n",
    "        host: str = \"192.168.0.188\",\n",
    "        port: str = \"19530\",\n",
    "        expr: str = \"\",\n",
    "        limit: int = 1000,\n",
    "        output_fields: list = [\"text\"],\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    从 Milvus 集合中读取指定字段（默认是 text）并返回列表\n",
    "    \"\"\"\n",
    "    # 1. 连接 Milvus\n",
    "    connections.connect(alias=\"default\", host=host, port=port)\n",
    "\n",
    "    # 2. 加载集合\n",
    "    collection = Collection(name=collection_name)\n",
    "    collection.load()\n",
    "\n",
    "    # 3. 查询数据\n",
    "    results = collection.query(\n",
    "            expr=expr,\n",
    "            output_fields=output_fields,\n",
    "            limit=limit\n",
    "        )\n",
    "\n",
    "    # 4. 提取目标字段为列表\n",
    "    if not output_fields:\n",
    "        raise ValueError(\"output_fields 不能为空\")\n",
    "\n",
    "    field_name = output_fields[0]  # 默认取第一个字段\n",
    "    data_list = [item[field_name] for item in results]\n",
    "    return data_list\n",
    "\n",
    "def get_qa_chain_with_memory(question: str):\n",
    "    vectordb = get_vectordb()\n",
    "\n",
    "    # 1. 初始化 BM25 检索器（关键词检索）\n",
    "    documents = get_text_list_from_milvus(collection_name=\"Vmaxs\")\n",
    "    bm25_retriever = BM25Retriever.from_texts(documents)\n",
    "    bm25_retriever.k = 10  # 返回前10个BM25检索结果\n",
    "\n",
    "    # 2. 初始化向量检索器\n",
    "    vector_retriever = vectordb.as_retriever(\n",
    "        search_kwargs={\"k\": 10},  # 返回前10个向量检索结果\n",
    "        search_type=\"mmr\",  # 多样性检索\n",
    "    )\n",
    "\n",
    "    # 3. 混合检索（EnsembleRetriever）作为最终检索器\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, vector_retriever],\n",
    "        weights=[0.5, 0.5],  # 调整BM25和向量检索的权重\n",
    "    )\n",
    "\n",
    "    # 4. 定义提示模板\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"chat_history\", \"question\", \"context\"],\n",
    "        template=\"\"\"\n",
    "    你是一位专业的VMAX-S技术专家助手，负责回答关于VMAX-S产品的技术问题。请根据以下规则回答问题：\n",
    "\n",
    "    1. 严格基于提供的上下文信息回答，不要编造或假设\n",
    "    2. 如果上下文不包含答案，明确表示\"根据现有资料，我无法回答这个问题\"\n",
    "    3. 回答要专业、准确、简洁\n",
    "    4. 对于操作类问题，提供分步骤说明\n",
    "    5. 对于需要比较或列举的问题，使用表格或列表形式\n",
    "    6. 保持友好专业的语气\n",
    "\n",
    "    当前对话历史：\n",
    "    {chat_history}\n",
    "\n",
    "    相关技术资料：\n",
    "    {context}\n",
    "\n",
    "    用户问题：{question}\n",
    "\n",
    "    请按照以下格式回答：\n",
    "    [专业回答]\n",
    "    (你的回答内容)\n",
    "\n",
    "    [补充说明]\n",
    "    (如有需要，可添加额外说明或建议)\n",
    "\n",
    "    感谢您咨询VMAX-S相关问题！\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # 5. 构建对话式检索链\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=get_llm(),\n",
    "        retriever=ensemble_retriever,  # 直接使用混合检索器\n",
    "        memory=memory,\n",
    "        output_key=\"answer\",\n",
    "        combine_docs_chain_kwargs={\n",
    "            \"prompt\": QA_CHAIN_PROMPT\n",
    "        },\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    result = qa_chain({\"question\": question})\n",
    "    return result\n",
    "\n",
    "# 测试问题\n",
    "questions = [\n",
    "    \"什么是VMAX的上网日志业务？\",\n",
    "    \"上网日志业务包含哪些功能？\",\n",
    "    \"整理成excel表格\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = get_qa_chain_with_memory(question)\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df084a1b-cf9f-418f-b432-c51675812ead",
   "metadata": {},
   "source": [
    "## 动态权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded9e68-6164-41ce-8392-3e9a72f79753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "问题: 什么是VMAX的上网日志业务？\n",
      "问题类型: balanced, 权重设置: BM25=0.5, Vector=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\will\\AppData\\Local\\Temp\\ipykernel_24180\\1639405118.py:215: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"question\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "嗯，用户的问题是关于ZXVMAX-S的上网日志业务。我得先理解什么是上网日志业务。根据产品描述，ZXVMAX-S主要用于网络运维和运营分析，特别是多维价值分析。上网日志可能是指设备包装箱中存储的数据记录。\n",
      "\n",
      "首先，我需要明确“上网”在这里"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores import Milvus\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ContextualCompressionRetriever, EnsembleRetriever\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from pymilvus import Collection, connections\n",
    "import re\n",
    "\n",
    "# Initialize memory outside the function so it persists across questions\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# 初始化 Milvus 向量数据库\n",
    "def get_vectordb():\n",
    "    emb_bgem3 = OllamaEmbeddings(base_url='http://localhost:11434', model=\"bge-m3:latest\")\n",
    "\n",
    "    # Milvus 连接参数\n",
    "    vectordb = Milvus(\n",
    "        embedding_function=emb_bgem3,\n",
    "        collection_name=\"Vmaxs\",  # Milvus 集合名称\n",
    "        connection_args={\n",
    "            \"host\": \"192.168.0.188\",  # Milvus 服务器地址\n",
    "            \"port\": \"19530\",  # Milvus 默认端口\n",
    "        },\n",
    "    )\n",
    "    return vectordb\n",
    "\n",
    "def get_llm():\n",
    "    return OllamaLLM(base_url='http://localhost:11434', model='deepseek-r1:1.5b', temperature=0.1, streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "def get_text_list_from_milvus(\n",
    "        collection_name: str,\n",
    "        host: str = \"192.168.0.188\",\n",
    "        port: str = \"19530\",\n",
    "        expr: str = \"\",\n",
    "        limit: int = 1000,\n",
    "        output_fields: list = [\"text\"],\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    从 Milvus 集合中读取指定字段（默认是 text）并返回列表\n",
    "    \"\"\"\n",
    "    # 1. 连接 Milvus\n",
    "    connections.connect(alias=\"default\", host=host, port=port)\n",
    "\n",
    "    # 2. 加载集合\n",
    "    collection = Collection(name=collection_name)\n",
    "    collection.load()\n",
    "\n",
    "    # 3. 查询数据\n",
    "    results = collection.query(\n",
    "            expr=expr,\n",
    "            output_fields=output_fields,\n",
    "            limit=limit\n",
    "        )\n",
    "\n",
    "    # 4. 提取目标字段为列表\n",
    "    if not output_fields:\n",
    "        raise ValueError(\"output_fields 不能为空\")\n",
    "\n",
    "    field_name = output_fields[0]  # 默认取第一个字段\n",
    "    data_list = [item[field_name] for item in results]\n",
    "    return data_list\n",
    "\n",
    "def determine_query_type(question: str) -> str:\n",
    "    \"\"\"\n",
    "    根据问题内容判断查询类型，返回权重调整策略\n",
    "\n",
    "    返回:\n",
    "        \"keyword\" - 更适合关键词检索的问题\n",
    "        \"semantic\" - 更适合语义检索的问题\n",
    "        \"balanced\" - 平衡型问题\n",
    "    \"\"\"\n",
    "    # 关键词型问题特征\n",
    "    keyword_patterns = [\n",
    "        r\"什么是.*\\?\",  # 定义类问题\n",
    "        r\".*包括哪些.*\",  # 列举类问题\n",
    "        r\".*有哪些.*\",  # 列举类问题\n",
    "        r\".*多少种.*\",  # 数量类问题\n",
    "        r\".*步骤.*\",  # 流程类问题\n",
    "        r\".*如何.*\",  # 方法类问题\n",
    "        r\".*怎样.*\",  # 方法类问题\n",
    "        r\".*整理.*表格\",  # 结构化输出要求\n",
    "        r\".*列出.*\",  # 列举要求\n",
    "        r\".*对比.*\",  # 比较类问题\n",
    "    ]\n",
    "\n",
    "    # 语义型问题特征\n",
    "    semantic_patterns = [\n",
    "        r\".*解决.*问题\",  # 解决方案类\n",
    "        r\".*原因.*\",  # 原因分析类\n",
    "        r\".*为什么.*\",  # 原因分析类\n",
    "        r\".*建议.*\",  # 建议类\n",
    "        r\".*优缺点.*\",  # 分析类\n",
    "        r\".*影响.*\",  # 影响分析类\n",
    "        r\".*解释.*\",  # 解释说明类\n",
    "        r\".*理解.*\",  # 理解类\n",
    "        r\".*意味着什么\",  # 含义类\n",
    "    ]\n",
    "\n",
    "    # 检查是否是关键词型问题\n",
    "    for pattern in keyword_patterns:\n",
    "        if re.search(pattern, question):\n",
    "            return \"keyword\"\n",
    "\n",
    "    # 检查是否是语义型问题\n",
    "    for pattern in semantic_patterns:\n",
    "        if re.search(pattern, question):\n",
    "            return \"semantic\"\n",
    "\n",
    "    # 默认平衡型\n",
    "    return \"balanced\"\n",
    "\n",
    "\n",
    "def get_dynamic_weights(query_type: str) -> tuple:\n",
    "    \"\"\"\n",
    "    根据查询类型返回动态权重\n",
    "\n",
    "    返回:\n",
    "        tuple: (bm25_weight, vector_weight)\n",
    "    \"\"\"\n",
    "    if query_type == \"keyword\":\n",
    "        return (0.7, 0.3)  # 更侧重关键词检索\n",
    "    elif query_type == \"semantic\":\n",
    "        return (0.3, 0.7)  # 更侧语义检索\n",
    "    else:\n",
    "        return (0.5, 0.5)  # 平衡权重\n",
    "\n",
    "def get_qa_chain_with_memory(question: str):\n",
    "    vectordb = get_vectordb()\n",
    "\n",
    "    # 1. 确定查询类型和动态权重\n",
    "    query_type = determine_query_type(question)\n",
    "    bm25_weight, vector_weight = get_dynamic_weights(query_type)\n",
    "    print(f\"问题类型: {query_type}, 权重设置: BM25={bm25_weight}, Vector={vector_weight}\")\n",
    "\n",
    "    # 2. 初始化 BM25 检索器\n",
    "    documents = get_text_list_from_milvus(collection_name=\"Vmaxs\")\n",
    "    bm25_retriever = BM25Retriever.from_texts(documents)\n",
    "    bm25_retriever.k = 10\n",
    "\n",
    "    # 3. 初始化向量检索器\n",
    "    vector_retriever = vectordb.as_retriever(\n",
    "        search_kwargs={\"k\": 10},\n",
    "        search_type=\"mmr\",\n",
    "    )\n",
    "\n",
    "    # 4. 使用动态权重的混合检索\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, vector_retriever],\n",
    "        weights=[bm25_weight, vector_weight],  # 使用动态权重\n",
    "    )\n",
    "    # 5. 混合检索（EnsembleRetriever）作为最终检索器\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, vector_retriever],\n",
    "        weights=[0.5, 0.5],  # 调整BM25和向量检索的权重\n",
    "    )\n",
    "\n",
    "   # 6. 定义提示模板（加入权重信息）\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"chat_history\", \"question\", \"context\"],\n",
    "        template=f\"\"\"\n",
    "    你是一位专业的VMAX-S技术专家助手，负责回答关于VMAX-S产品的技术问题。请根据以下规则回答问题：\n",
    "\n",
    "    1. 严格基于提供的上下文信息回答，不要编造或假设\n",
    "    2. 如果上下文不包含答案，明确表示\"根据现有资料，我无法回答这个问题\"\n",
    "    3. 回答要专业、准确、简洁\n",
    "    4. 对于操作类问题，提供分步骤说明\n",
    "    5. 对于需要比较或列举的问题，使用表格或列表形式\n",
    "    6. 保持友好专业的语气\n",
    "\n",
    "    当前问题类型: {query_type} (检索权重: BM25={bm25_weight}, Vector={vector_weight})\n",
    "\n",
    "    当前对话历史：\n",
    "    {{chat_history}}\n",
    "\n",
    "    相关技术资料：\n",
    "    {{context}}\n",
    "\n",
    "    用户问题：{{question}}\n",
    "\n",
    "    请按照以下格式回答：\n",
    "    [专业回答]\n",
    "    (你的回答内容)\n",
    "\n",
    "    [补充说明]\n",
    "    (如有需要，可添加额外说明或建议)\n",
    "\n",
    "    感谢您咨询VMAX-S相关问题！\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # 7. 构建对话式检索链\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=get_llm(),\n",
    "        retriever=ensemble_retriever,  # 直接使用混合检索器\n",
    "        memory=memory,\n",
    "        output_key=\"answer\",\n",
    "        combine_docs_chain_kwargs={\n",
    "            \"prompt\": QA_CHAIN_PROMPT\n",
    "        },\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    result = qa_chain({\"question\": question})\n",
    "    return result\n",
    "\n",
    "# 测试问题\n",
    "questions = [\n",
    "    \"什么是VMAX的上网日志业务？\",\n",
    "    \"上网日志业务包含哪些功能？\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n问题: {question}\")\n",
    "    result = get_qa_chain_with_memory(question)\n",
    "    print(f\"\\n回答: {result['answer']}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e24c3cd-2804-41d2-93af-17d48c53168b",
   "metadata": {},
   "source": [
    "# 混合检索 + rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a194f6e-c331-48f1-9a2a-d5df93999e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores import Milvus\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ContextualCompressionRetriever, EnsembleRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "import cohere\n",
    "\n",
    "# Initialize memory outside the function so it persists across questions\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "\n",
    "# 初始化 Milvus 向量数据库\n",
    "def get_vectordb():\n",
    "    emb_bgem3 = OllamaEmbeddings(base_url='http://localhost:11434', model=\"bge-m3:latest\")\n",
    "\n",
    "    # Milvus 连接参数\n",
    "    vectordb = Milvus(\n",
    "        embedding_function=emb_bgem3,\n",
    "        collection_name=\"Vmaxs\",  # Milvus 集合名称\n",
    "        connection_args={\n",
    "            \"host\": \"192.168.0.188\",  # Milvus 服务器地址\n",
    "            \"port\": \"19530\",  # Milvus 默认端口\n",
    "        },\n",
    "    )\n",
    "    return vectordb\n",
    "\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "def get_llm():\n",
    "    return OllamaLLM(base_url='http://localhost:11434', model='deepseek-r1:1.5b', temperature=0.1, streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "\n",
    "from pymilvus import Collection\n",
    "\n",
    "\n",
    "# 从milvus获取完整文档\n",
    "# def get_documents_from_milvus():\n",
    "#     # 连接 Milvus 集合\n",
    "#     collection = Collection(\"Vmaxs\")  # 替换为你的集合名\n",
    "#     collection.load()\n",
    "#\n",
    "#     # 获取所有文档的 text 字段\n",
    "#     documents = []\n",
    "#     res = collection.query(\n",
    "#         expr=\"\",  # 空表达式表示获取所有\n",
    "#         output_fields=[\"text\"],  # 假设你的文本存储在 \"text\" 字段\n",
    "#         limit=10000  # 限制最大数量（根据实际情况调整）\n",
    "#     )\n",
    "#     documents = [item[\"text\"] for item in res]\n",
    "#     return documents\n",
    "\n",
    "\n",
    "def get_qa_chain_with_memory(question: str):\n",
    "    vectordb = get_vectordb()\n",
    "\n",
    "    # 1. 初始化 BM25 检索器（关键词检索）\n",
    "    # 假设你已经有一个文档列表 `documents`（如果没有，可以从 vectordb 获取）\n",
    "    # 示例：documents = vectordb.get_all_documents()\n",
    "    # 这里仅作演示，实际使用时需要替换成你的文档数据\n",
    "    documents = [\"doc1\", \"doc2\", \"doc3\"]  # 替换成你的文档\n",
    "    # 从 Milvus 获取所有原始文本\n",
    "    # documents = get_documents_from_milvus()\n",
    "    bm25_retriever = BM25Retriever.from_texts(documents)\n",
    "    bm25_retriever.k = 10  # 返回前10个BM25检索结果\n",
    "\n",
    "    # 2. 初始化向量检索器\n",
    "    vector_retriever = vectordb.as_retriever(\n",
    "        search_kwargs={\"k\": 10},  # 返回前10个向量检索结果\n",
    "        search_type=\"mmr\",  # 多样性检索\n",
    "    )\n",
    "\n",
    "    # 3. 混合检索（EnsembleRetriever）\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, vector_retriever],\n",
    "        weights=[0.5, 0.5],  # 调整BM25和向量检索的权重\n",
    "    )\n",
    "\n",
    "    # 4. 使用 Jina Rerank 优化结果\n",
    "    # Jina Rerank配置\n",
    "    JINA_API_KEY = \"jina_63bb115e2d5f42d581f42643294792b5CE4nrEINMDcT4vJZJaSLcr5tkbIB\"  # 替换为你的Jina API密钥\n",
    "    \n",
    "    compressor = JinaRerank(\n",
    "        jina_api_key=JINA_API_KEY,\n",
    "        top_n=3,\n",
    "        model=\"jina-reranker-v2-base-multilingual\"  # Jina的多语言rerank模型[5](@ref)\n",
    "    )\n",
    "\n",
    "\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=ensemble_retriever  # 使用混合检索作为基础检索器\n",
    "    )\n",
    "\n",
    "    # 5. 定义提示模板\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"chat_history\", \"question\", \"context\"],\n",
    "        template=\"\"\"\n",
    "    你是一位专业的VMAX-S技术专家助手，负责回答关于VMAX-S产品的技术问题。请根据以下规则回答问题：\n",
    "\n",
    "    1. 严格基于提供的上下文信息回答，不要编造或假设\n",
    "    2. 如果上下文不包含答案，明确表示\"根据现有资料，我无法回答这个问题\"\n",
    "    3. 回答要专业、准确、简洁\n",
    "    4. 对于操作类问题，提供分步骤说明\n",
    "    5. 对于需要比较或列举的问题，使用表格或列表形式\n",
    "    6. 保持友好专业的语气\n",
    "\n",
    "    当前对话历史：\n",
    "    {chat_history}\n",
    "\n",
    "    相关技术资料：\n",
    "    {context}\n",
    "\n",
    "    用户问题：{question}\n",
    "\n",
    "    请按照以下格式回答：\n",
    "    [专业回答]\n",
    "    (你的回答内容)\n",
    "\n",
    "    [补充说明]\n",
    "    (如有需要，可添加额外说明或建议)\n",
    "\n",
    "    感谢您咨询VMAX-S相关问题！\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # 6. 构建对话式检索链\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=get_llm(),\n",
    "        retriever=compression_retriever,  # 使用混合检索 + Cohere Rerank\n",
    "        memory=memory,\n",
    "        output_key=\"answer\",\n",
    "        combine_docs_chain_kwargs={\n",
    "            \"prompt\": QA_CHAIN_PROMPT\n",
    "        },\n",
    "        # verbose=True, # 这个参数会开启调试模式，输出链的详细执行过程，包括传递给模型的完整提示词\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    result = qa_chain({\"question\": question})\n",
    "    return result\n",
    "\n",
    "\n",
    "# 测试问题\n",
    "questions = [\n",
    "    \"什么是VMAX的上网日志业务？\",\n",
    "    \"上网日志业务包含哪些功能？\",\n",
    "    \"整理成excel表格\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = get_qa_chain_with_memory(question)\n",
    "    # print(f\"问题：{question}\")\n",
    "    # print(f\"回答：{result['answer']}\")\n",
    "    # print(\"对话历史：\", memory.load_memory_variables({}))\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08518a5b-7ae7-4d26-a238-b33c26b2d1cb",
   "metadata": {},
   "source": [
    "获取milvus数据库中数据text字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f221421-3012-4771-9eea-90730e258fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, Collection\n",
    "\n",
    "\n",
    "def get_text_list_from_milvus(\n",
    "        collection_name: str,\n",
    "        host: str = \"129.201.70.35\",\n",
    "        port: str = \"19530\",\n",
    "        expr: str = \"\",\n",
    "        limit: int = 1000,\n",
    "        output_fields: list = [\"text\"],\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    从 Milvus 集合中读取指定字段（默认是 text）并返回列表\n",
    "\n",
    "    Args:\n",
    "        collection_name: Milvus 集合名称\n",
    "        host: Milvus 服务器地址（默认 localhost）\n",
    "        port: Milvus 端口（默认 19530）\n",
    "        expr: 过滤条件表达式（默认无过滤）\n",
    "        limit: 返回数据条数上限（默认 1000）\n",
    "        output_fields: 要提取的字段列表（默认 [\"text\"]）\n",
    "\n",
    "    Returns:\n",
    "        list: 包含目标字段值的列表\n",
    "    \"\"\"\n",
    "    # 1. 连接 Milvus\n",
    "    connections.connect(alias=\"default\", host=host, port=port)\n",
    "\n",
    "    # 2. 加载集合\n",
    "    collection = Collection(name=collection_name)\n",
    "    collection.load()\n",
    "\n",
    "    # 3. 查询数据\n",
    "    results = collection.query(\n",
    "            expr=expr,\n",
    "            output_fields=output_fields,\n",
    "            limit=limit\n",
    "        )\n",
    "\n",
    "    # 4. 提取目标字段为列表\n",
    "    if not output_fields:\n",
    "        raise ValueError(\"output_fields 不能为空\")\n",
    "\n",
    "    field_name = output_fields[0]  # 默认取第一个字段\n",
    "    data_list = [item[field_name] for item in results]\n",
    "    return data_list\n",
    "\n",
    "\n",
    "# 示例调用\n",
    "if __name__ == \"__main__\":\n",
    "    # 示例1：读取默认的 text 字段\n",
    "    texts = get_text_list_from_milvus(collection_name=\"Vmaxs\")\n",
    "    print(f\"获取 {len(texts)} 条文本，前5条: {texts[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878c584-ee3a-43fa-9299-2a413360c35e",
   "metadata": {},
   "source": [
    "将本地文件替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a34f1-ad7b-47f1-9dbd-2c09862f9131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores import Milvus\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ContextualCompressionRetriever, EnsembleRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import cohere\n",
    "\n",
    "# Initialize memory outside the function so it persists across questions\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "\n",
    "# 初始化 Milvus 向量数据库\n",
    "def get_vectordb():\n",
    "    emb_bgem3 = OllamaEmbeddings(base_url='http://localhost:11434', model=\"bge-m3:latest\")\n",
    "\n",
    "    # Milvus 连接参数\n",
    "    vectordb = Milvus(\n",
    "        embedding_function=emb_bgem3,\n",
    "        collection_name=\"Vmaxs\",  # Milvus 集合名称\n",
    "        connection_args={\n",
    "            \"host\": \"192.168.0.188\",  # Milvus 服务器地址\n",
    "            \"port\": \"19530\",  # Milvus 默认端口\n",
    "        },\n",
    "    )\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "def get_llm():\n",
    "    return OllamaLLM(base_url='http://localhost:11434', model='deepseek-r1:1.5b', temperature=0.1, streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "\n",
    "from pymilvus import Collection\n",
    "\n",
    "def get_text_list_from_milvus(\n",
    "        collection_name: str,\n",
    "        host: str = \"192.168.0.188\",\n",
    "        port: str = \"19530\",\n",
    "        expr: str = \"\",\n",
    "        limit: int = 1000,\n",
    "        output_fields: list = [\"text\"],\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    从 Milvus 集合中读取指定字段（默认是 text）并返回列表\n",
    "\n",
    "    Args:\n",
    "        collection_name: Milvus 集合名称\n",
    "        host: Milvus 服务器地址（默认 localhost）\n",
    "        port: Milvus 端口（默认 19530）\n",
    "        expr: 过滤条件表达式（默认无过滤）\n",
    "        limit: 返回数据条数上限（默认 1000）\n",
    "        output_fields: 要提取的字段列表（默认 [\"text\"]）\n",
    "\n",
    "    Returns:\n",
    "        list: 包含目标字段值的列表\n",
    "    \"\"\"\n",
    "    # 1. 连接 Milvus\n",
    "    connections.connect(alias=\"default\", host=host, port=port)\n",
    "\n",
    "    # 2. 加载集合\n",
    "    collection = Collection(name=collection_name)\n",
    "    collection.load()\n",
    "\n",
    "    # 3. 查询数据\n",
    "    results = collection.query(\n",
    "            expr=expr,\n",
    "            output_fields=output_fields,\n",
    "            limit=limit\n",
    "        )\n",
    "\n",
    "    # 4. 提取目标字段为列表\n",
    "    if not output_fields:\n",
    "        raise ValueError(\"output_fields 不能为空\")\n",
    "\n",
    "    field_name = output_fields[0]  # 默认取第一个字段\n",
    "    data_list = [item[field_name] for item in results]\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def get_qa_chain_with_memory(question: str):\n",
    "    vectordb = get_vectordb()\n",
    "\n",
    "    # 1. 初始化 BM25 检索器（关键词检索）\n",
    "    # 假设你已经有一个文档列表 `documents`（如果没有，可以从 vectordb 获取）\n",
    "    # 示例：documents = vectordb.get_all_documents()\n",
    "    # 这里仅作演示，实际使用时需要替换成你的文档数据\n",
    "    # documents = [\"doc1\", \"doc2\", \"doc3\"]  # 替换成你的文档\n",
    "    # 从 Milvus 获取所有原始文本\n",
    "    documents = get_text_list_from_milvus(collection_name=\"Vmaxs\")\n",
    "    bm25_retriever = BM25Retriever.from_texts(documents)\n",
    "    bm25_retriever.k = 10  # 返回前10个BM25检索结果\n",
    "\n",
    "    # 2. 初始化向量检索器\n",
    "    vector_retriever = vectordb.as_retriever(\n",
    "        search_kwargs={\"k\": 10},  # 返回前10个向量检索结果\n",
    "        search_type=\"mmr\",  # 多样性检索\n",
    "    )\n",
    "\n",
    "    # 3. 混合检索（EnsembleRetriever）\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, vector_retriever],\n",
    "        weights=[0.5, 0.5],  # 调整BM25和向量检索的权重\n",
    "    )\n",
    "\n",
    "    # 4. 使用 Cohere Rerank 优化结果\n",
    "    cohere_client = cohere.Client(api_key=\"Tahx1eySFbKvu9sTyTXrRLf59la3ZUG9vy02stRZ\")\n",
    "    compressor = CohereRerank(\n",
    "        client=cohere_client,\n",
    "        top_n=5,  # 最终保留5个最相关的文档\n",
    "        model=\"rerank-multilingual-v3.0\"\n",
    "    )\n",
    "\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_retriever=ensemble_retriever,  # 使用混合检索作为基础检索器\n",
    "        base_compressor=compressor # 进行rerank\n",
    "\n",
    "    )\n",
    "\n",
    "    # 5. 定义提示模板\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"chat_history\", \"question\", \"context\"],\n",
    "        template=\"\"\"\n",
    "    你是一位专业的VMAX-S技术专家助手，负责回答关于VMAX-S产品的技术问题。请根据以下规则回答问题：\n",
    "\n",
    "    1. 严格基于提供的上下文信息回答，不要编造或假设\n",
    "    2. 如果上下文不包含答案，明确表示\"根据现有资料，我无法回答这个问题\"\n",
    "    3. 回答要专业、准确、简洁\n",
    "    4. 对于操作类问题，提供分步骤说明\n",
    "    5. 对于需要比较或列举的问题，使用表格或列表形式\n",
    "    6. 保持友好专业的语气\n",
    "\n",
    "    当前对话历史：\n",
    "    {chat_history}\n",
    "\n",
    "    相关技术资料：\n",
    "    {context}\n",
    "\n",
    "    用户问题：{question}\n",
    "\n",
    "    请按照以下格式回答：\n",
    "    [专业回答]\n",
    "    (你的回答内容)\n",
    "\n",
    "    [补充说明]\n",
    "    (如有需要，可添加额外说明或建议)\n",
    "\n",
    "    感谢您咨询VMAX-S相关问题！\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # 6. 构建对话式检索链\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=get_llm(),\n",
    "        retriever=compression_retriever,  # 使用混合检索 + Cohere Rerank\n",
    "        memory=memory,\n",
    "        output_key=\"answer\",\n",
    "        combine_docs_chain_kwargs={\n",
    "            \"prompt\": QA_CHAIN_PROMPT\n",
    "        },\n",
    "        # verbose=True, # 这个参数会开启调试模式，输出链的详细执行过程，包括传递给模型的完整提示词\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    result = qa_chain({\"question\": question})\n",
    "    return result\n",
    "\n",
    "\n",
    "# 测试问题\n",
    "questions = [\n",
    "    \"什么是VMAX的上网日志业务？\",\n",
    "    \"上网日志业务包含哪些功能？\",\n",
    "    \"整理成excel表格\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = get_qa_chain_with_memory(question)\n",
    "    # print(f\"问题：{question}\")\n",
    "    # print(f\"回答：{result['answer']}\")\n",
    "    # print(\"对话历史：\", memory.load_memory_variables({}))\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ab52f3-6c48-465b-82ec-f174e2793240",
   "metadata": {},
   "source": [
    "# 混合检索动态权重 + rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dda7a5b-dc6a-4e0d-b7af-8547b8a0ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores import Milvus\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ContextualCompressionRetriever, EnsembleRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import cohere\n",
    "from pymilvus import connections, Collection\n",
    "import re\n",
    "\n",
    "# Initialize memory outside the function so it persists across questions\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "\n",
    "# 初始化 Milvus 向量数据库\n",
    "def get_vectordb():\n",
    "    emb_bgem3 = OllamaEmbeddings(base_url='http://localhost:11434', model=\"bge-m3:latest\")\n",
    "\n",
    "    # Milvus 连接参数\n",
    "    vectordb = Milvus(\n",
    "        embedding_function=emb_bgem3,\n",
    "        collection_name=\"Vmaxs\",  # Milvus 集合名称\n",
    "        connection_args={\n",
    "            \"host\": \"192.168.0.188\",  # Milvus 服务器地址\n",
    "            \"port\": \"19530\",  # Milvus 默认端口\n",
    "        },\n",
    "    )\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "def get_llm():\n",
    "    return OllamaLLM(base_url='http://localhost:11434', model='deepseek-r1:1.5b', temperature=0.1, streaming=True,\n",
    "                     callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "\n",
    "def get_text_list_from_milvus(\n",
    "        collection_name: str,\n",
    "        host: str = \"192.168.0.188\",\n",
    "        port: str = \"19530\",\n",
    "        expr: str = \"\",\n",
    "        limit: int = 1000,\n",
    "        output_fields: list = [\"text\"],\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    从 Milvus 集合中读取指定字段（默认是 text）并返回列表\n",
    "    \"\"\"\n",
    "    connections.connect(alias=\"default\", host=host, port=port)\n",
    "    collection = Collection(name=collection_name)\n",
    "    collection.load()\n",
    "    results = collection.query(\n",
    "        expr=expr,\n",
    "        output_fields=output_fields,\n",
    "        limit=limit\n",
    "    )\n",
    "    field_name = output_fields[0]\n",
    "    data_list = [item[field_name] for item in results]\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def determine_query_type(question: str) -> str:\n",
    "    \"\"\"\n",
    "    根据问题内容判断查询类型，返回权重调整策略\n",
    "\n",
    "    返回:\n",
    "        \"keyword\" - 更适合关键词检索的问题\n",
    "        \"semantic\" - 更适合语义检索的问题\n",
    "        \"balanced\" - 平衡型问题\n",
    "    \"\"\"\n",
    "    # 关键词型问题特征\n",
    "    keyword_patterns = [\n",
    "        r\"什么是.*\\?\",  # 定义类问题\n",
    "        r\".*包括哪些.*\",  # 列举类问题\n",
    "        r\".*有哪些.*\",  # 列举类问题\n",
    "        r\".*多少种.*\",  # 数量类问题\n",
    "        r\".*步骤.*\",  # 流程类问题\n",
    "        r\".*如何.*\",  # 方法类问题\n",
    "        r\".*怎样.*\",  # 方法类问题\n",
    "        r\".*整理.*表格\",  # 结构化输出要求\n",
    "        r\".*列出.*\",  # 列举要求\n",
    "        r\".*对比.*\",  # 比较类问题\n",
    "    ]\n",
    "\n",
    "    # 语义型问题特征\n",
    "    semantic_patterns = [\n",
    "        r\".*解决.*问题\",  # 解决方案类\n",
    "        r\".*原因.*\",  # 原因分析类\n",
    "        r\".*为什么.*\",  # 原因分析类\n",
    "        r\".*建议.*\",  # 建议类\n",
    "        r\".*优缺点.*\",  # 分析类\n",
    "        r\".*影响.*\",  # 影响分析类\n",
    "        r\".*解释.*\",  # 解释说明类\n",
    "        r\".*理解.*\",  # 理解类\n",
    "        r\".*意味着什么\",  # 含义类\n",
    "    ]\n",
    "\n",
    "    # 检查是否是关键词型问题\n",
    "    for pattern in keyword_patterns:\n",
    "        if re.search(pattern, question):\n",
    "            return \"keyword\"\n",
    "\n",
    "    # 检查是否是语义型问题\n",
    "    for pattern in semantic_patterns:\n",
    "        if re.search(pattern, question):\n",
    "            return \"semantic\"\n",
    "\n",
    "    # 默认平衡型\n",
    "    return \"balanced\"\n",
    "\n",
    "\n",
    "def get_dynamic_weights(query_type: str) -> tuple:\n",
    "    \"\"\"\n",
    "    根据查询类型返回动态权重\n",
    "\n",
    "    返回:\n",
    "        tuple: (bm25_weight, vector_weight)\n",
    "    \"\"\"\n",
    "    if query_type == \"keyword\":\n",
    "        return (0.7, 0.3)  # 更侧重关键词检索\n",
    "    elif query_type == \"semantic\":\n",
    "        return (0.3, 0.7)  # 更侧语义检索\n",
    "    else:\n",
    "        return (0.5, 0.5)  # 平衡权重\n",
    "\n",
    "\n",
    "def get_qa_chain_with_memory(question: str):\n",
    "    vectordb = get_vectordb()\n",
    "\n",
    "    # 1. 确定查询类型和动态权重\n",
    "    query_type = determine_query_type(question)\n",
    "    bm25_weight, vector_weight = get_dynamic_weights(query_type)\n",
    "    print(f\"问题类型: {query_type}, 权重设置: BM25={bm25_weight}, Vector={vector_weight}\")\n",
    "\n",
    "    # 2. 初始化 BM25 检索器\n",
    "    documents = get_text_list_from_milvus(collection_name=\"Vmaxs\")\n",
    "    bm25_retriever = BM25Retriever.from_texts(documents)\n",
    "    bm25_retriever.k = 10\n",
    "\n",
    "    # 3. 初始化向量检索器\n",
    "    vector_retriever = vectordb.as_retriever(\n",
    "        search_kwargs={\"k\": 10},\n",
    "        search_type=\"mmr\",\n",
    "    )\n",
    "\n",
    "    # 4. 使用动态权重的混合检索\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, vector_retriever],\n",
    "        weights=[bm25_weight, vector_weight],  # 使用动态权重\n",
    "    )\n",
    "\n",
    "    # 5. 使用 Cohere Rerank 优化结果\n",
    "    cohere_client = cohere.Client(api_key=\"Tahx1eySFbKvu9sTyTXrRLf59la3ZUG9vy02stRZ\")\n",
    "    compressor = CohereRerank(\n",
    "        client=cohere_client,\n",
    "        top_n=5,\n",
    "        model=\"rerank-multilingual-v3.0\"\n",
    "    )\n",
    "\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=ensemble_retriever\n",
    "    )\n",
    "\n",
    "    # 6. 定义提示模板（加入权重信息）\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"chat_history\", \"question\", \"context\"],\n",
    "        template=f\"\"\"\n",
    "    你是一位专业的VMAX-S技术专家助手，负责回答关于VMAX-S产品的技术问题。请根据以下规则回答问题：\n",
    "\n",
    "    1. 严格基于提供的上下文信息回答，不要编造或假设\n",
    "    2. 如果上下文不包含答案，明确表示\"根据现有资料，我无法回答这个问题\"\n",
    "    3. 回答要专业、准确、简洁\n",
    "    4. 对于操作类问题，提供分步骤说明\n",
    "    5. 对于需要比较或列举的问题，使用表格或列表形式\n",
    "    6. 保持友好专业的语气\n",
    "\n",
    "    当前问题类型: {query_type} (检索权重: BM25={bm25_weight}, Vector={vector_weight})\n",
    "\n",
    "    当前对话历史：\n",
    "    {{chat_history}}\n",
    "\n",
    "    相关技术资料：\n",
    "    {{context}}\n",
    "\n",
    "    用户问题：{{question}}\n",
    "\n",
    "    请按照以下格式回答：\n",
    "    [专业回答]\n",
    "    (你的回答内容)\n",
    "\n",
    "    [补充说明]\n",
    "    (如有需要，可添加额外说明或建议)\n",
    "\n",
    "    感谢您咨询VMAX-S相关问题！\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # 7. 构建对话式检索链\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=get_llm(),\n",
    "        retriever=compression_retriever,\n",
    "        memory=memory,\n",
    "        output_key=\"answer\",\n",
    "        combine_docs_chain_kwargs={\n",
    "            \"prompt\": QA_CHAIN_PROMPT\n",
    "        },\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    result = qa_chain({\"question\": question})\n",
    "    return result\n",
    "\n",
    "\n",
    "# 测试问题\n",
    "questions = [\n",
    "    \"什么是VMAX的上网日志业务？\",  # 定义类问题，更适合关键词检索\n",
    "    \"上网日志业务包含哪些功能？\",  # 列举类问题，更适合关键词检索\n",
    "    \"整理成excel表格\",  # 结构化输出要求，更适合关键词检索\n",
    "    \"为什么我的VMAX设备会出现日志丢失问题？\",  # 原因分析类，更适合语义检索\n",
    "    \"如何解决VMAX日志存储空间不足的问题？\",  # 解决方案类，更适合语义检索\n",
    "    \"VMAX-S与其他型号的主要区别是什么？\"  # 平衡型问题\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n问题: {question}\")\n",
    "    result = get_qa_chain_with_memory(question)\n",
    "    print(f\"\\n回答: {result['answer']}\")\n",
    "    print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
