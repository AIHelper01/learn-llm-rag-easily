{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c794bc7",
   "metadata": {},
   "source": [
    "# 构建检索问答链"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d0f2c3-3bd9-4de1-bbd2-e0e2b09161c8",
   "metadata": {},
   "source": [
    "我们已经介绍了如何根据自己的本地知识文档，搭建一个向量知识库。 在接下来的内容里，我们将使用搭建好的向量数据库，对 query 查询问题进行召回，并将召回结果和 query 结合起来构建 prompt，输入到大模型中进行问答。   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d8d968-8d98-47b9-8885-dc17d24dce76",
   "metadata": {},
   "source": [
    "## 1. 加载向量数据库\n",
    "\n",
    "首先，我们加载在前一章已经构建的向量数据库。注意，此处你需要使用和构建时相同的 Emedding。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7affbed-f36e-4700-a1d9-c5d88917fff5",
   "metadata": {},
   "source": [
    "### Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f66d376-8140-4224-bdfb-360b60aef43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量数据库已成功加载。\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"../C3 搭建知识库\") # 将父目录放入系统路径中\n",
    "\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "# # 从环境变量中加载你的 API_KEY\n",
    "# _ = load_dotenv(find_dotenv())    # read local .env file\n",
    "# zhipuai_api_key = os.environ['ZHIPUAI_API_KEY']\n",
    "\n",
    "# 定义持久化目录\n",
    "persist_directory = '../data_base/vector_db/chroma-vmax'\n",
    "\n",
    "# # 创建嵌入模型\n",
    "# from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "# zhipu_embed = ZhipuAIEmbeddings(\n",
    "#     model=\"embedding-2\",\n",
    "#     api_key=zhipuai_api_key\n",
    "# )\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "my_emb = OllamaEmbeddings(base_url='http://localhost:11434',model=\"bge-m3:latest\")\n",
    "\n",
    "try:\n",
    "    # 加载持久化的 Chroma 向量数据库\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=persist_directory,  # 允许我们将persist_directory目录保存到磁盘上\n",
    "        collection_name=\"vmax-s\",\n",
    "        embedding_function=my_emb\n",
    "    )\n",
    "    print(\"向量数据库已成功加载。\")\n",
    "except Exception as e:\n",
    "    print(f\"加载向量数据库时发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b0b1838-38a3-4666-8bc8-c4592a5a39d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量库中存储的数量：891\n"
     ]
    }
   ],
   "source": [
    "print(f\"向量库中存储的数量：{vectordb._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5912d3d-9517-465c-8b82-cdd1eab27a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量库中存储的数量：891\n"
     ]
    }
   ],
   "source": [
    "print(f\"向量库中存储的数量：{vectordb._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a68dc0-4f4c-433b-a367-44b5cffe8516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的内容数：5\n"
     ]
    }
   ],
   "source": [
    "question = \"VMAX上网日志业务是什么？\"\n",
    "docs = vectordb.similarity_search(question,k=5)\n",
    "print(f\"检索到的内容数：{len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09852d2a-aeda-4822-bf56-782a0397df3c",
   "metadata": {},
   "source": [
    "打印一下检索到的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f492bf-197b-4f94-820c-2d88c17754d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的第0个内容: \n",
      " 4 功能本章包含如下主题：上网日志保存\n",
      "10\n",
      "上网日志查询\n",
      "10\n",
      "上网日志批量导入查询\n",
      "11\n",
      "日志管理\n",
      "11\n",
      "账号管理\n",
      "11\n",
      "角色管理\n",
      "12\n",
      "资源监控\n",
      "12\n",
      "告警管理\n",
      "12\n",
      "省级网关对接\n",
      "12\n",
      "拨测结果自动比对功能\n",
      "12\n",
      "NAT日志入库功能\n",
      "13\n",
      "北向接口\n",
      "13\n",
      "上网日志历史查询\n",
      "13\n",
      "云化上网日志XDR查询\n",
      "13\n",
      "以下介绍ZXVMAX-S的主要的功能。\n",
      "4.1 上网日志保存支持使用Gbase数据库或HDFS保存上网日志。上网日志保存时间可配置，最短保存7天时间，最长可保存一年时间。支持自动清理超过保存时间的上网日志。\n",
      "4.2 上网日志查询可通过web界面指定查询条件，查询用户上网日志。支持的查询条件：时间范围+公网IP\n",
      "时间范围+目的IP\n",
      "时间范围+MSISDN\n",
      "时间范围+IMSI\n",
      "时间范围+URL\n",
      "支持组合以上五种基础条件，或在基础条件上增加指定公网端口、目的端口，做更精确的查询。\n",
      "10\n",
      "SJ-20220623151803-017|2023-03-30（R1.0）\n",
      "-----------------------------------------------------\n",
      "检索到的第1个内容: \n",
      " 4功能\n",
      "4.11 NAT日志入库功能\n",
      "ZXVMAX-S支持将NAT日志保存到GP数据库，可通过公网IP或目的IP查询NAT日志。\n",
      "4.12 北向接口支持中移规范北向接口，中移集团查询平台下发查询命令，上网日志系统将查询结果以文件方式上传至查询平台。包括云化核心网的4G/5G上网日志查询、4G/5G控制面用户面XDR查询、\n",
      "4G/5G软采XDR查询、VoLTEXDR查询、5G消息XDR查询，以及原始码流查询功能。\n",
      "4.13 上网日志历史查询当现网使用两个库分别存储2个月数据和6个月数据时，使用历史数据查询操作查询6个月库。\n",
      "4.14 云化上网日志XDR查询上网日志系统支持云化上网日志XDR查询，包括云化核心网的4G/5G控制面用户面XDR查询、\n",
      "4G/5G软采XDR查询、VoLTEXDR查询、5G消息XDR查询，以及原始码流查询功能。\n",
      "SJ-20220623151803-017|2023-03-30（R1.0）\n",
      "13\n",
      "-----------------------------------------------------\n",
      "检索到的第2个内容: \n",
      " ZXVMAX-S多维价值分析系统产品描述（上网日志业务）图2-3ZXVMAX逻辑结构图模块功能采集层：àProbe：探针，支持硬采和软采，负责进行标准接口的信令和Packet抓包。àFirewalllogProcess：防火墙日志处理单元，负责接收防火墙日志。àDataMergeProcess：数据合成单元，负责关联网元信令和防火墙日志，生成上网日志数据。存储共享层：à分布式存储集群：采用分布式存储集群，留存上网日志记录。支持MPP数据库Gbase。à本地分布式文件系统：支持HDFS。àSaturn：负责接收和解析数据合成单元发送的上网日志实时流数据，将上网日志记录存入\n",
      "MPP数据库。应用分析层：à客户端：ZXVMAX-S用户访问接口，通过Windows应用程序、Browser等形式提供用户界面和操作接口。àREST接口：以RESTful接口形式提供服务器端的服务访问能力。à服务器端：实现ZXVMAX-S系统应用分析功能的一系列服务端逻辑、算法，服务器端各模块分别对应不同的Feature。\n",
      "2.3 硬件结构\n",
      "-----------------------------------------------------\n",
      "检索到的第3个内容: \n",
      " 4.4 日志管理描述日志主要用户记录用户对IPDR系统的操作事件，是系统安全管理的重要组成部分。系统维护人员可通过web查看相关日志，对用户的操作进行追踪和审查。\n",
      "ZXVMAX-S提供对日志的查询、导出和下载功能。日志分类按照日志类型可以将日志分为三类：操作日志操作日志用户记录用户操作的详细信息，包括：操作时间、操作功能、操作结果、用户账号、主机地址、日志详细信息。操作日志包括新增、修改、删除等命令所记录下的日志信息。安全日志安全日志用来记录用户的登录和登出信息，包括：日志名称、用户账号、主机地址、操作时间、详细信息。系统日志系统日志用来记录系统运行时产生的日志，包括：主机、级别、时间、日志源、详细信息。\n",
      "4.5 账号管理账号管理是系统安全管理最重要的组成部分，提供账号的创建、查询、修改、删除操作。创建账号时，可以指定用户名、密码、用户角色、用户联系方式，并为用户分配角色。通过分配不同角色，赋予用户不同的权限。提供账号策略和密码策略，达到安全保护的目的。\n",
      "SJ-20220623151803-017|2023-03-30（R1.0）\n",
      "11\n",
      "-----------------------------------------------------\n",
      "检索到的第4个内容: \n",
      " 1 产品定位与特点本章包含如下主题：产品背景\n",
      "1\n",
      "产品定位\n",
      "1\n",
      "产品特点\n",
      "1\n",
      "1.1 产品背景随着移动通信网络的不断发展，移动智能终端用户数突飞猛进，越来越多的用户开始使用移动终端上网。为了网络安全考虑，政府要求移动运营商可以追溯用户的上网行为，这就要求移动运营商能记录下所有用户的上网日志。\n",
      "1.2 产品定位\n",
      "ZXVMAX-S系统可以长时间的保存用户的上网日志，并可通过公网IP、目的IP、IMSI、\n",
      "MSISDN、URL等参数查询用户上网日志，满足运营商追溯用户上网行为的需求。\n",
      "1.3 产品特点使用Gbase数据库或者分布式本地文件系统(HDFS)保存上网日志，支持保存上网日志1年时间。可通过硬采或软采方式采集网元信令，同时接收防火墙NAT日志，关联网元信令和NAT日志生成用户上网日志，保存在数据库中。à硬采：通过分光或镜像，采集Gn口或S11口、S1-U口信令。à软采：通过与GGSN/xGW的消息接口，采集网元信令。支持多种方式查询用户上网日志，包括：à时间范围+公网IP\n",
      "à时间范围+目的IP\n",
      "à时间范围+IMSI\n",
      "à时间范围+MSISDN\n",
      "à时间范围+URL\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs):\n",
    "    print(f\"检索到的第{i}个内容: \\n {doc.page_content}\", end=\"\\n-----------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc86262-da78-4fda-a597-600d54057062",
   "metadata": {},
   "source": [
    "### Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8349782-7ca4-4ebb-acc1-6097ff5cee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Milvus\n",
    "my_emb = OllamaEmbeddings(base_url='http://localhost:11434', model=\"bge-m3:latest\")\n",
    "\n",
    "# Milvus 连接参数\n",
    "vectordb = Milvus(\n",
    "        embedding_function=my_emb,\n",
    "        collection_name=\"Vmaxs\",  # Milvus 集合名称\n",
    "        connection_args={\n",
    "            \"host\": \"129.201.70.35\",  # Milvus 服务器地址\n",
    "            \"port\": \"19530\",  # Milvus 默认端口\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14c39416-a9b2-4de1-9fb6-5c521a7fd2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Apache FOP Version 2.6', 'creator': 'DITA Open Toolkit', 'creationdate': '2023-05-23T21:44:01+08:00', 'source': '../data_base/knowledge_path/VMAX-S/ZXVMAX-S（V6.23）产品描述（语音业务）.pdf', 'file_path': '../data_base/knowledge_path/VMAX-S/ZXVMAX-S（V6.23）产品描述（语音业务）.pdf', 'total_pages': 41, 'format': 'PDF 1.4', 'title': '目录', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20230523214401+08'00'\", 'page': 16, 'pk': 457237404597884273}, page_content='4\\xa0功能与业务本章包含如下主题：\\uf06c功能\\n13\\n\\uf06c业务\\n16\\n4.1\\xa0功能本节介绍ZXVMAX-S语音业务的主要的功能。\\n4.1.1\\xa0用户分析功能用户分析是以客户、客户群组等为维度的专项分析，可以区分业务，区分用户等级，区分时间等，进行用户级的在某一时间段的呼叫、注册记录，媒体数据、信令详情，并结合该用户在最近的呼叫、注册统计情况，分析定位语音/视频通话质量发生异常的环节。由于网元众多，运营维护人员在发生用户投诉后难以定位具体问题所在，事后拨测定位，在各个接口上同时抓包，工作量繁重。用户分析功能提供了运营维护人员以下帮助：\\uf06c问题初步定位根据用户IMSI或者号码，查询在某一时间段的呼叫、注册记录，媒体数据、信令详情，并结合该用户在最近7天的呼叫、注册统计情况，分析定位语音/视频通话质量发生异常的环节。\\uf06c详细分析可以更进一步钻取可以根据用户IMSI或者号码，回溯VMAX系统接入采集的所有信令，展现信令流程图，无需事后拨测抓包。'),\n",
       " Document(metadata={'producer': 'Apache FOP Version 2.6', 'creator': 'DITA Open Toolkit', 'creationdate': '2023-05-23T21:45:33+08:00', 'source': '../data_base/knowledge_path/VMAX-S/ZXVMAX-S（V6.23）产品描述（上网日志业务）.pdf', 'file_path': '../data_base/knowledge_path/VMAX-S/ZXVMAX-S（V6.23）产品描述（上网日志业务）.pdf', 'total_pages': 29, 'format': 'PDF 1.4', 'title': '目录', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20230523214533+08'00'\", 'page': 6, 'pk': 457237404597884943}, page_content='2产品结构图2-2ZXVMAX系统组网结构图（软采方式）表2-1ZXVMAX系统组件组网说明\\nZXVMAX产品级组件产品（组件）说明\\nProbeUnit探针单元，支持硬采或软采方式采集网元信令。\\uf06c硬采：通过分光或镜像采集Gn/S11/S1-U口信令。\\uf06c软采：通过消息方式与GGSN/xGW对接采集网元信息。\\nFirewalllogProcessUnit防火墙日志处理单元，与防火墙对接接收防火墙NAT日志。\\nDataMergeUnit数据合成单元，关联网元信令和防火墙NAT日志，生成上网日志记录。\\nDataStorageUnit数据存储单元，接收并保存上网日志数据。包括实时流数据处理节点Satu-rn、数据入库节点Dataload和MPP数据库。\\uf06cSaturn：实时接收并解析数据合成单元发送的上网日志。\\uf06cDataload：将上网日志存入集群数据库。\\uf06cMPP数据库：支持Gbase数据库。\\uf06c分布式本地文件系统：支持HDFS。'),\n",
       " Document(metadata={'producer': 'Apache FOP Version 2.3', 'creator': 'DITA Open Toolkit', 'creationdate': '2022-06-23T16:34:22+08:00', 'source': '../data_base/knowledge_path/VMAX-S/ZXVMAX-S（V6.20.80.02）告警处理.pdf', 'file_path': '../data_base/knowledge_path/VMAX-S/ZXVMAX-S（V6.20.80.02）告警处理.pdf', 'total_pages': 330, 'format': 'PDF 1.4', 'title': '目录', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20220623163422+08'00'\", 'page': 211, 'pk': 457237404597884776}, page_content='ZXVMAX-S多维价值分析系统告警处理\\uf06c4000200116S1-MME接口XDRID完整率(小时)\\n208\\n\\uf06c4000200117S1-MME接口RAT完整率(小时)\\n208\\n\\uf06c4000200118S1-MME接口IMSI完整率(小时)\\n208\\n\\uf06c4000200119S1-MME接口IMEI完整率(小时)\\n209\\n\\uf06c4000200120S1-MME接口MSISDN完整率(小时)\\n209\\n\\uf06c4000200121S1-MME接口ProcedureStartTime完整率(小时)\\n210\\n\\uf06c4000200122S1-MME接口ProcedureEndTime完整率(小时)\\n210\\n\\uf06c4000200123S1-MME接口ProcedureType完整率(小时)\\n210\\n\\uf06c4000200124S1-MME接口ProcedureStatus完整率(小时)\\n211\\n\\uf06c4000200125S1-MME接口MMEGroupID完整率(小时)\\n211\\n\\uf06c4000200126S1-MME接口MMECode完整率(小时)\\n211\\n\\uf06c4000200127S1-MME接口M-TMSI完整率(小时)\\n212'),\n",
       " Document(metadata={'producer': 'Apache FOP Version 2.6', 'creator': 'DITA Open Toolkit', 'creationdate': '2023-05-23T21:45:02+08:00', 'source': '../data_base/knowledge_path/VMAX-S/ZXVMAX-S（V6.23）产品描述（端到端业务）.pdf', 'file_path': '../data_base/knowledge_path/VMAX-S/ZXVMAX-S（V6.23）产品描述（端到端业务）.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': '目录', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20230523214502+08'00'\", 'page': 4, 'pk': 457237404597884325}, page_content='1.2\\xa0产品定位\\nZXVMAX-S（ValueMAX）端到端多维价值分析系统是面向用户的网络运维和运营分析产品，立足于从客户角度去感知和分析网络和业务信息，通过对海量数据灵活的挖掘和分析，从网元、用户、终端、业务等多个维度，对业务使用过程中的质量和特征进行全方位的挖掘。ZXVMAX-\\nS端到端支持实时分析（端到端质量等）和事后分析（用户投诉、问题分析等）两种模式，为移动通讯网络的运维和运营提供全面支撑。\\nZXVMAX-S端到端系统为运营商的网络建设、运维优化、客户服务和用户数据价值挖掘等多个环节提供支撑，重点关注八大场景，如图1-1所示，服务对象包括：\\uf06c运营商网优和运维部门，支撑网络运维；具体用户是网优工程师和维护工程师，聚焦客户是\\nCTO和运维负责人。\\uf06c运营商市场部门，为运营商市场发展提供支撑方案；具体用户是市场工程师，聚焦客户是\\nCEO/CMO以及市场负责人。\\nSJ-20220623151803-015|2023-03-30（R1.0）\\n1'),\n",
       " Document(metadata={'producer': 'Apache FOP Version 2.3', 'creator': 'DITA Open Toolkit', 'creationdate': '2022-06-23T16:34:22+08:00', 'source': '../data_base/knowledge_path/VMAX-S/ZXVMAX-S（V6.20.80.02）告警处理.pdf', 'file_path': '../data_base/knowledge_path/VMAX-S/ZXVMAX-S（V6.20.80.02）告警处理.pdf', 'total_pages': 330, 'format': 'PDF 1.4', 'title': '目录', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20220623163422+08'00'\", 'page': 199, 'pk': 457237404597884758}, page_content='ZXVMAX-S多维价值分析系统告警处理\\n5.23100100102h2内存数据库连接异常告警描述h2内存数据库连接异常告警级别严重可能原因h2进程不存在处理步骤检查XDRQuery角色是否正常\\n5.33100100103gbase数据库连接异常告警描述gbase数据库连接异常告警级别严重可能原因gbase数据库异常处理步骤检查gbasedb角色是否正常\\n5.43100100201hawk查询异常告警描述hawk查询异常告警级别严重可能原因查询sql拼写错误\\n178\\nSJ-20220623151803-011|2022-06-20（R1.0）')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = vectordb.similarity_search(query=\"VMAX\", k=5)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7f8dbd-ecd5-449d-9753-aedc2b74289c",
   "metadata": {},
   "source": [
    "## 2. 创建一个 LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026bd74f-3dd0-496e-905b-950a444bb7a7",
   "metadata": {},
   "source": [
    "在这里，我们调用 OpenAI 的 API 创建一个 LLM，当然你也可以使用其他 LLM 的 API 进行创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe25e4-e98e-4cb7-ac95-40e0e7b14ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 从环境变量中加载你的 API_KEY\n",
    "# _ = load_dotenv(find_dotenv())    # read local .env file\n",
    "# zhipuai_api_key = os.environ['ZHIPUAI_API_KEY']\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# zhipuai_llm = ChatOpenAI(\n",
    "#     temperature=0,\n",
    "#     model=\"glm-4\",\n",
    "#     openai_api_key=zhipuai_api_key,\n",
    "#     openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a89fb297-888a-4d35-b519-90eba639893c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\n\\n</think>\\n\\n你好！有什么我可以帮助你的吗？'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "my_llm = Ollama(base_url='http://localhost:11434', model='deepseek-r1:14b', temperature=0.1)\n",
    "\n",
    "my_llm.invoke(\"nihao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db394906-5379-48e5-abf4-abdf60082884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "# from langchain_ollama import OllamaLLM\n",
    "\n",
    "# my_llm = OllamaLLM(base_url='http://129.201.70.35:11434', model='deepseek-r1:14b', temperature=0.1, streaming=True,\n",
    "#     callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# my_llm.invoke(\"nihao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ad0fd45-73a0-451c-8e0e-7ea7b7365d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_deepseek import ChatDeepSeek\n",
    "# _ = load_dotenv(find_dotenv())    # read local .env file\n",
    "# deepseek_api_key = os.environ['DEEPSEEK_API_KEY']\n",
    "# llm_deepseek = ChatDeepSeek(\n",
    "#     model=\"deepseek-reasoner\",\n",
    "#     temperature=0,\n",
    "#     max_tokens=None,\n",
    "#     timeout=None,\n",
    "#     # max_retries=2,\n",
    "#     api_key=deepseek_api_key,\n",
    "#     # other params...\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ee13994-8cc6-45c0-a19a-c18dddddab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "# _ = load_dotenv(find_dotenv())    # read local .env file\n",
    "# deepseek_api_key = os.environ['DEEPSEEK_API_KEY']\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# zhipuai_llm = ChatOpenAI(\n",
    "#     temperature=0,\n",
    "#     model=\"deepseek-chat\",\n",
    "#     openai_api_key=deepseek_api_key,\n",
    "#     openai_api_base=\"https://api.deepseek.com\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f361e27-cafb-48bf-bb41-50c9cb3a4f7e",
   "metadata": {},
   "source": [
    "## 3. 构建检索问答链"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b5e3c-1bc9-40e9-83c7-0594c2e7727d",
   "metadata": {},
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91be03f4-264d-45cb-bebd-223c1c5747fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答\n",
    "案。总是在回答的最后说“谢谢你的提问！”。\n",
    "{context}\n",
    "问题: {question}\n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],\n",
    "                                 template=template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d06d7f-1dca-4d10-b5cd-3a23e9d91200",
   "metadata": {},
   "source": [
    "#### 创建一个基于模板的检索链： 基础检索版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b05eb57-edf5-4b35-9538-42c2b8f5cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 基础检索\n",
    "base_retriever = vectordb.as_retriever(search_kwargs={\"k\": 10})\n",
    "base_retriever = vectordb.as_retriever(\n",
    "    search_kwargs={\"k\": 15},  # 扩大召回池\n",
    "    search_type=\"mmr\",  # 最大边际相关性算法（网页5）\n",
    "    # metadata_filter={\"source\": \"权威文档.pdf\"}  # 元数据过滤\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(my_llm,\n",
    "                                       retriever=base_retriever,\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa1a61eb-feea-4fff-8063-a20c3b392aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大模型+知识库后回答 question_1 的结果：\n",
      "<think>\n",
      "好的，我现在要回答用户的问题：“什么是VMAX？”根据提供的上下文资料，我需要先理解相关内容，然后整理出一个清晰的解释。\n",
      "\n",
      "首先，资料中多次提到“ZXVMAX-S多维价值分析系统”。这表明VMAX可能是指这个系统。从内容来看，该系统涉及上网日志保存、查询、管理，以及使用Gbase数据库和HDFS存储数据。此外，还提到了故障处理流程和告警机制。\n",
      "\n",
      "接下来，资料中详细描述了系统的功能模块，如上网日志保存、查询、批量导入、日志管理等，说明VMAX是一个用于管理和分析网络日志的系统。它可能用于电信或互联网行业，帮助监控和维护网络性能，检测异常流量，并提供数据挖掘和统计分析功能。\n",
      "\n",
      "另外，资料中还提到了硬件结构部分，说明该系统基于标准IT部件构建，适用于处理海量数据。同时，故障处理流程显示了系统的稳定性和技术支持的重要性。\n",
      "\n",
      "综合以上信息，VMAX很可能是一个专业的网络日志管理平台，用于收集、存储、分析和监控网络活动，帮助管理员进行故障排查、性能优化和安全审计。\n",
      "</think>\n",
      "\n",
      "**VMAX** 是指 **ZXVMAX-S 多维价值分析系统**，这是一个专业的网络日志管理平台。该系统主要用于电信或互联网行业，能够高效地处理海量数据，包括上网日志的保存、查询、批量导入以及数据分析等功能。它支持使用 Gbase 数据库和 HDFS 进行存储，并提供灵活的时间配置和自动清理功能。\n",
      "\n",
      "VMAX 系统具备以下特点：\n",
      "1. **日志管理**：支持通过 Web 界面进行精确查询，可组合多种条件如时间范围、IP、MSISDN、IMSI 和 URL 等。\n",
      "2. **数据存储**：采用 Gbase 数据库和 HDFS，确保数据的高效存储与管理。\n",
      "3. **故障处理**：提供详细的故障处理流程和告警机制，帮助快速定位和解决系统问题。\n",
      "4. **硬件支持**：基于标准 IT 部件构建，适用于大规模部署。\n",
      "\n",
      "总之，VMAX 是一个功能强大的网络日志分析系统，旨在帮助管理员进行网络监控、性能优化和安全审计。\n"
     ]
    }
   ],
   "source": [
    "question_1 = \"什么是vmax？\"\n",
    "result = qa_chain({\"query\": question_1})\n",
    "print(\"大模型+知识库后回答 question_1 的结果：\")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2707095e-21d0-4e2b-8a5b-0c02258d2ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大模型+知识库后回答 question_1 的结果：\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "对不起，我还没有学会回答这个问题。如果你有其他问题，我非常乐意为你提供帮助。\n"
     ]
    }
   ],
   "source": [
    "question_1 = \"严威是谁？\"\n",
    "result = qa_chain({\"query\": question_1})\n",
    "print(\"大模型+知识库后回答 question_1 的结果：\")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ac734-6693-422d-97e1-50f140e2d358",
   "metadata": {},
   "source": [
    "### 创建一个基于模板的检索链： rerank检索版本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb7cfe-22a4-40f0-ab33-b5a856c486d5",
   "metadata": {},
   "source": [
    "#### cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c893eb-3a30-40c1-9b60-dc6d448ea1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "# rerank检索\n",
    "# Cohere Rerank配置\n",
    "import cohere\n",
    "cohere_client = cohere.Client(api_key=\"Tahx1eySFbKvu9sTyTXrRLf59la3ZUG9vy02stRZ\")\n",
    "\n",
    "compressor = CohereRerank(\n",
    "    client=cohere_client,\n",
    "    top_n=5,\n",
    "    model=\"rerank-multilingual-v3.0\"  # 支持多语言的新版本\n",
    ")\n",
    "\n",
    "base_retriever = vectordb.as_retriever(\n",
    "    search_kwargs={\"k\": 15},  # 扩大召回池\n",
    "    search_type=\"mmr\",  # 最大边际相关性算法（网页5）\n",
    "    # metadata_filter={\"source\": \"权威文档.pdf\"}  # 元数据过滤\n",
    ")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm_deepseek,\n",
    "    retriever=compression_retriever,  # 替换为压缩检索器\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": QA_CHAIN_PROMPT,\n",
    "        # \"llm_kwargs\": {\"max_length\": 300}  # 新增输出长度限制\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8902ba4-c914-423e-b3da-7552147dd132",
   "metadata": {},
   "source": [
    "#### jina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d238422-a365-495d-b2aa-bae3b40254a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.document_compressors import JinaRerank  # 使用Jina的rerank组件\n",
    "\n",
    "# Jina Rerank配置\n",
    "JINA_API_KEY = \"jina_63bb115e2d5f42d581f42643294792b5CE4nrEINMDcT4vJZJaSLcr5tkbIB\"  # 替换为你的Jina API密钥\n",
    "\n",
    "compressor = JinaRerank(\n",
    "    jina_api_key=JINA_API_KEY,\n",
    "    top_n=3,\n",
    "    model=\"jina-reranker-v2-base-multilingual\"  # Jina的多语言rerank模型[5](@ref)\n",
    ")\n",
    "\n",
    "base_retriever = vectordb.as_retriever(\n",
    "    search_kwargs={\"k\": 15},  # 扩大召回池\n",
    "    search_type=\"mmr\",  # 最大边际相关性算法（网页5）\n",
    "    # metadata_filter={\"source\": \"权威文档.pdf\"}  # 元数据过滤\n",
    ")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    my_llm,\n",
    "    retriever=compression_retriever,  # 替换为压缩检索器\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": QA_CHAIN_PROMPT,\n",
    "        # \"llm_kwargs\": {\"max_length\": 300}  # 新增输出长度限制\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4b724-0733-4382-87f9-46bd988df947",
   "metadata": {},
   "source": [
    "#### BGE版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd65a64-b115-4fdb-a3af-7928e8049252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.document_compressors import JinaRerank  # 使用Jina的rerank组件\n",
    "\n",
    "# BGE配置\n",
    "# 先将模型下载到本地\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "model = HuggingFaceCrossEncoder(model_name=\"/opt/workspace/models/BAAI/bge-reranker-base\")\n",
    "compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "\n",
    "base_retriever = vectordb.as_retriever(\n",
    "    search_kwargs={\"k\": 15},  # 扩大召回池\n",
    "    search_type=\"mmr\",  # 最大边际相关性算法（网页5）\n",
    "    # metadata_filter={\"source\": \"权威文档.pdf\"}  # 元数据过滤\n",
    ")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=base_retriever,\n",
    "    base_compressor=compressor\n",
    ")\n",
    "\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    my_llm,\n",
    "    retriever=compression_retriever,  # 替换为压缩检索器\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": QA_CHAIN_PROMPT,\n",
    "        # \"llm_kwargs\": {\"max_length\": 300}  # 新增输出长度限制\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503a7972-a673-41ca-a028-647169d19fcb",
   "metadata": {},
   "source": [
    "## 4.检索问答链效果测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a610e223-64c2-4865-b049-b47a36262a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = \"什么是VMAX-S的上网日志业务？\"\n",
    "question_2 = \"严威是谁？\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2223f-6fb5-4504-bfcd-ac74ca9ff2fa",
   "metadata": {},
   "source": [
    "### 4.1 基于召回结果和 query 结合起来构建的 prompt 效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1eb5c5c3-9958-44f5-9fbc-f867de6c5042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大模型+知识库后回答 question_1 的结果：\n",
      "<think>\n",
      "好的，我现在需要回答用户的问题：“什么是VMAX-S的上网日志业务？”首先，我得仔细阅读提供的上下文，找出相关的信息。\n",
      "\n",
      "从产品描述中可以看到，ZXVMAX-S多维价值分析系统专注于上网日志业务。它有几个主要模块：采集层、存储共享层和应用分析层。采集层负责收集信令数据和防火墙日志，并将它们合并生成上网日志。存储层使用分布式集群和MPP数据库来保存这些日志，同时还有一个Saturn模块处理实时流数据。\n",
      "\n",
      "接下来是应用分析层，它提供了客户端访问接口、RESTful API以及服务器端的逻辑和算法，用于分析这些日志数据。此外，系统还具备角色管理功能，提供四种不同的角色权限，确保系统的安全性。还有资源监控和告警管理，帮助运维人员及时发现和处理问题。\n",
      "\n",
      "总结一下，VMAX-S的上网日志业务主要是通过采集、存储和分析网络日志数据，来实现对网络行为的监控和管理，并提供了相应的安全措施和技术支持。\n",
      "</think>\n",
      "\n",
      "ZXVMAX-S多维价值分析系统的产品描述中提到，该系统专注于“上网日志业务”。具体来说，它通过采集层（包括探针、防火墙日志处理单元和数据合成单元）收集信令数据和防火墙日志，并生成上网日志数据。存储共享层使用分布式存储集群和MPP数据库Gbase来保存这些日志记录，同时Saturn模块负责实时流数据的接收和解析。\n",
      "\n",
      "应用分析层则通过客户端、REST接口和服务器端逻辑提供对日志数据的访问和分析功能。此外，系统还具备角色管理、资源监控和告警管理等功能，以确保系统的安全性和稳定性。\n",
      "\n",
      "因此，ZXVMAX-S的上网日志业务主要是指该系统在收集、存储和分析网络日志数据方面的功能，以及相关的安全管理与监控能力。\n",
      "\n",
      "谢谢你的提问！\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": question_1})\n",
    "print(\"大模型+知识库后回答 question_1 的结果：\")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37a0f3c4-4c50-4b73-a4b3-674825a366f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大模型+知识库后回答 question_2 的结果：\n",
      "<think>\n",
      "好的，我现在需要回答用户的问题：“严威是谁？”首先，我查看了提供的上下文内容。上下文主要介绍了ZXVMAX-S端到端系统及其功能、告警处理和产品描述，但没有提到任何关于“严威”的信息。\n",
      "\n",
      "接下来，我分析用户可能的意图。用户可能在寻找某个特定人物的信息，比如公司员工、公众人物或技术专家。然而，在提供的资料中，并没有任何关于个人的内容，只有产品和技术细节。\n",
      "\n",
      "考虑到上下文内容与问题无关，我无法找到任何相关的信息来回答这个问题。因此，根据指示，如果不知道答案，应该明确表示不知道，并避免编造信息。\n",
      "\n",
      "最后，按照要求，回答结束后需要加上“谢谢你的提问！”。\n",
      "</think>\n",
      "\n",
      "严威不是在提供的上下文中提到的人物，因此我不知道严威是谁。谢谢你的提问！\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": question_2})\n",
    "print(\"大模型+知识库后回答 question_2 的结果：\")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4195cfa-1fc8-41a9-8984-91f2e5fbe013",
   "metadata": {},
   "source": [
    "### 4.2 大模型自己回答的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "569fbe28-2e2d-4042-b3a1-65326842bdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41296/46941173.py:5: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  my_llm.predict(prompt_template)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<think>\\n嗯，我现在要了解什么是VMAX-S的上网日志业务。首先，我听说过VMAX这个品牌，可能是一个网络设备或者服务提供商。S可能代表的是“安全”或者其他含义，但我不确定。\\n\\n上网日志业务听起来像是记录用户上网活动的服务。那具体来说，VMAX-S的这个业务是做什么的呢？是不是用来监控用户的网络使用情况，比如访问了哪些网站，登录了什么应用，或者记录下了线时间？\\n\\n我记得以前听说过企业或学校会用类似的东西来管理内部网络，确保员工或学生遵守规定，不访问不良网站。所以，VMAX-S可能提供这样的日志记录功能，帮助机构监控和管理网络使用。\\n\\n那这个业务具体包括哪些内容呢？可能有详细的上网记录，比如IP地址、MAC地址这些信息，还有用户访问的URL，登录的应用系统，以及上下线时间等。这些数据可以帮助管理员了解用户的网络行为，进行审计或者安全分析。\\n\\n另外，日志存储和查询功能也很重要。VMAX-S可能会提供一个平台或工具，让用户可以方便地查看和管理这些日志数据，可能还会生成报告，帮助机构更好地理解和处理网络活动。\\n\\n合规性也是一个考虑因素。很多行业有法规要求记录用户的上网行为，比如金融、教育或者政府机构。VMAX-S的这个业务可能帮助企业满足这些合规要求，避免法律风险。\\n\\n总的来说，VMAX-S的上网日志业务应该是提供一个全面的日志记录和管理解决方案，帮助企业和机构监控网络使用情况，确保安全和合规性。\\n</think>\\n\\nVMAX-S的上网日志业务是一种由VMAX提供的网络管理服务，主要用于记录和分析用户的上网行为。该业务通过收集详细的网络活动数据，如IP地址、MAC地址、访问URL、应用登录信息及上下线时间等，帮助企业和机构监控内部网络使用情况，确保符合安全策略和法规要求。其主要功能包括全面的日志记录、存储与查询、实时监控告警以及生成审计报告，适用于金融、教育、政府等多个行业，以满足合规需求并保障网络安全。'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"\"\"请回答下列问题:\n",
    "                            {}\"\"\".format(question_1)\n",
    "\n",
    "### 基于大模型的问答\n",
    "my_llm.predict(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0d3a813-db19-4be5-8926-ad8298e3e2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\n\\n</think>\\n\\n对不起，我还没有学会回答这个问题。如果你有其他问题，我非常乐意为你提供帮助。'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"\"\"请回答下列问题:\n",
    "                            {}\"\"\".format(question_2)\n",
    "\n",
    "### 基于大模型的问答\n",
    "my_llm.predict(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b9ba4a-053d-409a-a632-63336c2bdf84",
   "metadata": {},
   "source": [
    "> ⭐ 通过以上两个问题，我们发现 LLM 对于一些近几年的知识以及非常识性的专业问题，回答的并不是很好。而加上我们的本地知识，就可以帮助 LLM 做出更好的回答。另外，也有助于缓解大模型的“幻觉”问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20c510-f583-42b4-ac5c-b232388e9673",
   "metadata": {},
   "source": [
    "## 5. 添加历史对话的记忆功能-v2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e43e5-262d-4358-afb4-129dbf413287",
   "metadata": {},
   "source": [
    "现在我们已经实现了通过上传本地知识文档，然后将他们保存到向量知识库，通过将查询问题与向量知识库的召回结果进行结合输入到 LLM 中，我们就得到了一个相比于直接让 LLM 回答要好得多的结果。在与语言模型交互时，你可能已经注意到一个关键问题 - **它们并不记得你之前的交流内容**。这在我们构建一些应用程序（如聊天机器人）的时候，带来了很大的挑战，使得对话似乎缺乏真正的连续性。这个问题该如何解决呢？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cadc3df-4123-4e12-9516-8632b10dc41f",
   "metadata": {},
   "source": [
    "### 5.1 记忆（Memory）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f3fd2-14de-4177-893c-53ddaf6768e9",
   "metadata": {},
   "source": [
    "在本节中我们将介绍 LangChain 中的储存模块，即如何将先前的对话嵌入到语言模型中的，使其具有连续对话的能力。我们将使用 `ConversationBufferMemory` ，它保存聊天消息历史记录的列表，这些历史记录将在回答问题时与问题一起传递给聊天机器人，从而将它们添加到上下文中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42210b88-3590-47dc-a087-ab9cef14f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",  # 与 prompt 的输入变量保持一致。\n",
    "    return_messages=True  # 将以消息列表的形式返回聊天记录，而不是单个字符串\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d580d9-b926-462f-9d6a-23135ecece37",
   "metadata": {},
   "source": [
    "关于更多的 Memory 的使用，包括保留指定对话轮数、保存指定 token 数量、保存历史对话的总结摘要等内容，请参考 langchain 的 Memory 部分的相关文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ec5f5c-5ee8-4b89-add3-25f3c864200d",
   "metadata": {},
   "source": [
    "### 5.2 对话检索链（ConversationalRetrievalChain）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f1afd4-a5e7-4eea-855b-d03055c93e2d",
   "metadata": {},
   "source": [
    "对话检索链（ConversationalRetrievalChain）在检索 QA 链的基础上，增加了处理对话历史的能力。\n",
    "\n",
    "它的工作流程是:\n",
    "1. 将之前的对话与新问题合并生成一个完整的查询语句。\n",
    "2. 在向量数据库中搜索该查询的相关文档。\n",
    "3. 获取结果后,存储所有答案到对话记忆区。\n",
    "4. 用户可在 UI 中查看完整的对话流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0305623-1a06-4bb8-b340-dde57202dd67",
   "metadata": {},
   "source": [
    "这种链式方式将新问题放在之前对话的语境中进行检索，可以处理依赖历史信息的查询。并保留所有信\n",
    "息在对话记忆中，方便追踪。\n",
    "\n",
    "接下来让我们可以测试这个对话检索链的效果："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd113e-9d21-4385-a426-7e7a8ebb887a",
   "metadata": {},
   "source": [
    "使用上一节中的向量数据库和 LLM ！首先提出一个无历史对话的问题“这门课会学习 Python 吗？”，并查看回答。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cbf223-ffb1-4220-bd16-2de3f1fbc6bc",
   "metadata": {},
   "source": [
    "v2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5dcc5f-545e-463a-9f3f-9eca90457caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "retriever=vectordb.as_retriever()\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm_deepseek,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n",
    "question = \"什么是VMAX-S的上网日志业务？\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d48806-de78-4927-82de-f3adde8833fd",
   "metadata": {},
   "source": [
    "V2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d0089-02ed-4737-b834-914eed83c5e9",
   "metadata": {},
   "source": [
    "不带memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cccd64-c744-407d-868b-e1534b272a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "# rerank检索\n",
    "\n",
    "# Jina Rerank配置\n",
    "JINA_API_KEY = \"jina_63bb115e2d5f42d581f42643294792b5CE4nrEINMDcT4vJZJaSLcr5tkbIB\"  # 替换为你的Jina API密钥\n",
    "\n",
    "compressor = JinaRerank(\n",
    "    jina_api_key=JINA_API_KEY,\n",
    "    top_n=3,\n",
    "    model=\"jina-reranker-v2-base-multilingual\"  # Jina的多语言rerank模型[5](@ref)\n",
    ")\n",
    "\n",
    "\n",
    "# # BGE配置\n",
    "# # 先将模型下载到本地\n",
    "# from langchain.retrievers import ContextualCompressionRetriever\n",
    "# from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "# from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "# model = HuggingFaceCrossEncoder(model_name=\"/opt/workspace/models/BAAI/bge-reranker-base\")\n",
    "# compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "\n",
    "base_retriever = vectordb.as_retriever(\n",
    "    search_kwargs={\"k\": 15},  # 扩大召回池\n",
    "    search_type=\"mmr\",  # 最大边际相关性算法（网页5）\n",
    "    # metadata_filter={\"source\": \"权威文档.pdf\"}  # 元数据过滤\n",
    ")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    my_llm,\n",
    "    retriever=compression_retriever,  # 替换为压缩检索器\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": QA_CHAIN_PROMPT,\n",
    "        # \"llm_kwargs\": {\"max_length\": 300}  # 新增输出长度限制\n",
    "    }\n",
    ")\n",
    "\n",
    "result = qa_chain({\"query\": question_1})\n",
    "print(\"大模型+知识库后回答 question_1 的结果：\")\n",
    "print(result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e2b197-e0bd-40ca-9351-20fd47c2991b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bf5a9be-7fa9-46e3-8966-7c7fad7f259c",
   "metadata": {},
   "source": [
    "新增MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25601979-9c1b-4d48-bceb-1cf2c979a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "    你是一个专业的问答助手。请根据对话历史和提供的上下文回答问题。\n",
    "    \n",
    "    历史对话：\n",
    "    {chat_history}\n",
    "    \n",
    "    上下文：\n",
    "    {context}\n",
    "    \n",
    "    问题：{question}\n",
    "    \n",
    "    回答：\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm_deepseek,\n",
    "    retriever=compression_retriever,\n",
    "    memory=memory,\n",
    "    # return_source_documents=True,\n",
    "    output_key=\"answer\",  # 明确指定存储到内存的键\n",
    "    combine_docs_chain_kwargs={  # 替代chain_type_kwargs\n",
    "        \"prompt\": QA_CHAIN_PROMPT\n",
    "    },\n",
    "    verbose=True, # 独立传递verbose参数\n",
    ")\n",
    "\n",
    "\n",
    "questions = [\n",
    "    \"什么是VMAX的安全加固？\", \n",
    "    \"安全加固的操作步骤？\",  # 需记忆前一轮的\"主要内容\"\n",
    "    \"整理成中文表格\"  # 需合并多轮信息\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa_chain({\"question\": question})\n",
    "    print(f\"问题：{question}\")\n",
    "    print(f\"回答：{result['answer']}\")\n",
    "    # print(f\"引用的来源：{result['source_documents'][0].metadata}\")  # 显示来源文档\n",
    "    print(\"对话历史：\", memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d3f2be-19aa-428e-b991-3755e5628c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72780ae5-b010-4eb8-8885-7c449412183f",
   "metadata": {},
   "source": [
    "## 5. 添加历史对话的记忆功能 - v2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181c6341-9529-48f6-b0d6-df0cb38692cb",
   "metadata": {},
   "source": [
    "现在我们已经实现了通过上传本地知识文档，然后将他们保存到向量知识库，通过将查询问题与向量知识库的召回结果进行结合输入到 LLM 中，我们就得到了一个相比于直接让 LLM 回答要好得多的结果。在与语言模型交互时，你可能已经注意到一个关键问题 - **它们并不记得你之前的交流内容**。这在我们构建一些应用程序（如聊天机器人）的时候，带来了很大的挑战，使得对话似乎缺乏真正的连续性。这个问题该如何解决呢？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef1dbbf-3260-4865-a71a-11d22317a195",
   "metadata": {},
   "source": [
    "### 5.1 记忆（Memory）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8ac5e-e5f2-42a2-8c5a-cf416aaa2a7a",
   "metadata": {},
   "source": [
    "在本节中我们将介绍 LangChain 中的储存模块，即如何将先前的对话嵌入到语言模型中的，使其具有连续对话的能力。我们将使用 `ConversationBufferMemory` ，它保存聊天消息历史记录的列表，这些历史记录将在回答问题时与问题一起传递给聊天机器人，从而将它们添加到上下文中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d58ef8-297f-4a56-9d7c-9cdc043ddda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",  # 与 prompt 的输入变量保持一致。\n",
    "    return_messages=True  # 将以消息列表的形式返回聊天记录，而不是单个字符串\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdb6d61-0772-453c-b507-db57afac74fe",
   "metadata": {},
   "source": [
    "关于更多的 Memory 的使用，包括保留指定对话轮数、保存指定 token 数量、保存历史对话的总结摘要等内容，请参考 langchain 的 Memory 部分的相关文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84184f40-44e0-4c25-91e4-241f7364f654",
   "metadata": {},
   "source": [
    "### 5.2 对话检索链（ConversationalRetrievalChain）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba1aea7-7d40-4101-8f9e-65b26d54ad40",
   "metadata": {},
   "source": [
    "对话检索链（ConversationalRetrievalChain）在检索 QA 链的基础上，增加了处理对话历史的能力。\n",
    "\n",
    "它的工作流程是:\n",
    "1. 将之前的对话与新问题合并生成一个完整的查询语句。\n",
    "2. 在向量数据库中搜索该查询的相关文档。\n",
    "3. 获取结果后,存储所有答案到对话记忆区。\n",
    "4. 用户可在 UI 中查看完整的对话流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848cd6ab-0471-4104-b0d7-0e5198996d59",
   "metadata": {},
   "source": [
    "这种链式方式将新问题放在之前对话的语境中进行检索，可以处理依赖历史信息的查询。并保留所有信\n",
    "息在对话记忆中，方便追踪。\n",
    "\n",
    "接下来让我们可以测试这个对话检索链的效果："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1dccc5-989f-4d92-8896-2fb16c2e429b",
   "metadata": {},
   "source": [
    "使用上一节中的向量数据库和 LLM ！首先提出一个无历史对话的问题“这门课会学习 Python 吗？”，并查看回答。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47e13b1-9208-4f04-a2ec-ac4dc5141a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "retriever=vectordb.as_retriever()\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n",
    "question = \"什么是VMAX？\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0855c0-c860-48e0-a2a0-52674526db3c",
   "metadata": {},
   "source": [
    "然后基于答案进行下一个问题“为什么这门课需要教这方面的知识？”："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167f9af-82e6-46b7-abb0-aa715aec3f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"南瓜书包含哪些功能？\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e686548-57a2-45e6-9fcd-42f4c673c7a0",
   "metadata": {},
   "source": [
    "可以看到，LLM 它准确地判断了这方面的知识，指代内容是强化学习的知识，也就\n",
    "是我们成功地传递给了它历史信息。这种持续学习和关联前后问题的能力，可大大增强问答系统的连续\n",
    "性和智能水平。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec6cd22-6607-45da-802b-dd892e3c60ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7db4e11-bc42-4289-bb04-4334d3155742",
   "metadata": {},
   "source": [
    "# 脚本汇总"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca516d-c604-473d-92c4-2b7646b23088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm():\n",
    "    return OllamaLLM(base_url='http://localhost:11434', model='deepseek-r1:14b', temperature=0.1)\n",
    "\n",
    "\n",
    "def generate_response(input_text):\n",
    "    llm = get_llm()\n",
    "    output = llm.invoke(input_text)\n",
    "    output_parser = StrOutputParser()\n",
    "    return output_parser.invoke(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ff0fe-0086-46a2-859b-2c4943f23252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "import cohere\n",
    "from langchain_community.vectorstores import Milvus\n",
    "\n",
    "\n",
    "\n",
    "# 初始化 Milvus 向量数据库\n",
    "def get_vectordb():\n",
    "    my_emb = OllamaEmbeddings(base_url='http://localhost:11434', model=\"bge-m3:latest\")\n",
    "    \n",
    "    # Milvus 连接参数\n",
    "    vectordb = Milvus(\n",
    "        embedding_function=my_emb,\n",
    "        collection_name=\"Vmaxs\",  # Milvus 集合名称\n",
    "        connection_args={\n",
    "            \"host\": \"129.201.70.35\",  # Milvus 服务器地址\n",
    "            \"port\": \"19530\",      # Milvus 默认端口\n",
    "        },\n",
    "        # 可选：是否自动创建集合（如果不存在）\n",
    "        # auto_create_collection=True,\n",
    "    )\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "def get_llm():\n",
    "    return OllamaLLM(base_url='http://localhost:11434', model='deepseek-r1:14b', temperature=0.1, streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "\n",
    "def get_qa_chain_without_memory(question: str):\n",
    "    vectordb = get_vectordb()\n",
    "    myllm = get_llm()\n",
    "\n",
    "    # Jina Rerank配置\n",
    "    JINA_API_KEY = \"jina_63bb115e2d5f42d581f42643294792b5CE4nrEINMDcT4vJZJaSLcr5tkbIB\"  # 替换为你的Jina API密钥\n",
    "    \n",
    "    compressor = JinaRerank(\n",
    "        jina_api_key=JINA_API_KEY,\n",
    "        top_n=3,\n",
    "        model=\"jina-reranker-v2-base-multilingual\"  # Jina的多语言rerank模型[5](@ref)\n",
    "    )\n",
    "\n",
    "    base_retriever = vectordb.as_retriever(\n",
    "        search_kwargs={\"k\": 15},\n",
    "        search_type=\"mmr\",\n",
    "    )\n",
    "\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=base_retriever\n",
    "    )\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=myllm,\n",
    "        retriever=compression_retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\n",
    "            \"prompt\": PromptTemplate(\n",
    "                input_variables=[\"context\", \"question\"],\n",
    "                template=\"\"\"你是DeepSeek VMAX-S知识助手。使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答\n",
    "                案。总是在回答的最后说“谢谢你的提问！”。\n",
    "                {context}\n",
    "                问题: {question}\n",
    "                \"\"\"\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    result = qa_chain({\"query\": question})\n",
    "    return result[\"result\"]\n",
    "\n",
    "\n",
    "print(get_qa_chain_without_memory(\"介绍下VMAX的上网日志业务\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e125db7b-640d-4640-b98f-9899bf48296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import cohere\n",
    "\n",
    "# Initialize memory outside the function so it persists across questions\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# 初始化 Milvus 向量数据库\n",
    "def get_vectordb():\n",
    "    my_emb = OllamaEmbeddings(base_url='http://localhost:11434', model=\"bge-m3:latest\")\n",
    "    \n",
    "    # Milvus 连接参数\n",
    "    vectordb = Milvus(\n",
    "        embedding_function=my_emb,\n",
    "        collection_name=\"Vmaxs\",  # Milvus 集合名称\n",
    "        connection_args={\n",
    "            \"host\": \"192.168.0.188\",  # Milvus 服务器地址\n",
    "            \"port\": \"19530\",      # Milvus 默认端口\n",
    "        },\n",
    "        # 可选：是否自动创建集合（如果不存在）\n",
    "        # auto_create_collection=True,\n",
    "    )\n",
    "    return vectordb\n",
    "\n",
    "def get_llm():\n",
    "    return OllamaLLM(base_url='http://localhost:11434', model='deepseek-r1:1.5b', temperature=0.1)\n",
    "\n",
    "def get_qa_chain_with_memory(question: str):\n",
    "    vectordb = get_vectordb()\n",
    "    myllm = get_llm()\n",
    "\n",
    "    # Jina Rerank配置\n",
    "    JINA_API_KEY = \"jina_63bb115e2d5f42d581f42643294792b5CE4nrEINMDcT4vJZJaSLcr5tkbIB\"  # 替换为你的Jina API密钥\n",
    "    \n",
    "    compressor = JinaRerank(\n",
    "        jina_api_key=JINA_API_KEY,\n",
    "        top_n=3,\n",
    "        model=\"jina-reranker-v2-base-multilingual\"  # Jina的多语言rerank模型[5](@ref)\n",
    "    )\n",
    "    \n",
    "\n",
    "    base_retriever = vectordb.as_retriever(\n",
    "        search_kwargs={\"k\": 15},\n",
    "        search_type=\"mmr\",\n",
    "    )\n",
    "\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=base_retriever\n",
    "    )\n",
    "\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"chat_history\", \"question\", \"context\"],\n",
    "        template=\"\"\"\n",
    "        你是DeepSeek VMAX-S知识助手。使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答案。总是在回答的最后说“谢谢你的提问！\n",
    "        \n",
    "        历史对话：\n",
    "        {chat_history}\n",
    "        \n",
    "        上下文：\n",
    "        {context}\n",
    "        \n",
    "        问题：{question}\n",
    "        \n",
    "        回答：\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    #     input_variables=[\"context\",\"question\"],\n",
    "    #     template=\"\"\"你是DeepSeek VMAX-S知识助手。使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答案。总是在回答的最后说“谢谢你的提问！”。\n",
    "    #             {context}\n",
    "    #             问题: {question}\n",
    "    #     \"\"\"\n",
    "    # )\n",
    "\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=get_llm(),\n",
    "        retriever=compression_retriever,\n",
    "        memory=memory,\n",
    "        output_key=\"answer\",\n",
    "        combine_docs_chain_kwargs={\n",
    "            \"prompt\": QA_CHAIN_PROMPT\n",
    "        },\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    result = qa_chain({\"question\": question})  # Changed from \"query\" to \"question\"\n",
    "    return result\n",
    "\n",
    "questions = [\n",
    "    \"什么是VMAX的上网日志业务？\",\n",
    "    \"上网日志业务包含哪些功能？\",  # 需记忆前一轮的\"主要内容\"\n",
    "    \"整理成中文表格\"  # 需合并多轮信息\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = get_qa_chain_with_memory(question)  # Pass string directly, not dict\n",
    "    print(f\"问题：{question}\")\n",
    "    print(f\"回答：{result['answer']}\")\n",
    "    print(\"对话历史：\", memory.load_memory_variables({}))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5837fd-4c4b-463d-884c-289f5c661bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "import cohere\n",
    "from langchain_community.vectorstores import Milvus\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Singleton pattern for vector database\n",
    "_vectordb_instance = None\n",
    "\n",
    "def get_vectordb():\n",
    "    global _vectordb_instance\n",
    "    if _vectordb_instance is None:\n",
    "        try:\n",
    "            my_emb = OllamaEmbeddings(base_url='http://localhost:11434', model=\"bge-m3:latest\")\n",
    "            \n",
    "            _vectordb_instance = Milvus(\n",
    "                embedding_function=my_emb,\n",
    "                collection_name=\"Vmaxs\",\n",
    "                connection_args={\n",
    "                    \"host\": \"192.168.0.188\",\n",
    "                    \"port\": \"19530\",\n",
    "                },\n",
    "                # Add consistency level if needed\n",
    "                consistency_level=\"Strong\",\n",
    "            )\n",
    "            logger.info(\"Successfully connected to Milvus vector database\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to connect to Milvus: {str(e)}\")\n",
    "            raise\n",
    "    return _vectordb_instance\n",
    "\n",
    "def get_llm():\n",
    "    return OllamaLLM(\n",
    "        base_url='http://localhost:11434', \n",
    "        model='deepseek-r1:1.5b', \n",
    "        temperature=0.1,\n",
    "        # Add timeout parameters\n",
    "        request_timeout=60\n",
    "    )\n",
    "\n",
    "def get_qa_chain_without_memory(question: str):\n",
    "    try:\n",
    "        vectordb = get_vectordb()\n",
    "        myllm = get_llm()\n",
    "\n",
    "    # Jina Rerank配置\n",
    "    JINA_API_KEY = \"jina_63bb115e2d5f42d581f42643294792b5CE4nrEINMDcT4vJZJaSLcr5tkbIB\"  # 替换为你的Jina API密钥\n",
    "    \n",
    "    compressor = JinaRerank(\n",
    "        jina_api_key=JINA_API_KEY,\n",
    "        top_n=3,\n",
    "        model=\"jina-reranker-v2-base-multilingual\"  # Jina的多语言rerank模型[5](@ref)\n",
    "    )\n",
    "\n",
    "\n",
    "        # Milvus-specific search parameters\n",
    "        base_retriever = vectordb.as_retriever(\n",
    "            search_kwargs={\n",
    "                \"k\": 15,\n",
    "                # Add Milvus-specific search parameters if needed\n",
    "                \"params\": {\"nprobe\": 10}\n",
    "            },\n",
    "            search_type=\"mmr\",\n",
    "        )\n",
    "\n",
    "        compression_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=compressor,\n",
    "            base_retriever=base_retriever\n",
    "        )\n",
    "\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=myllm,\n",
    "            retriever=compression_retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\n",
    "                \"prompt\": PromptTemplate(\n",
    "                    input_variables=[\"context\", \"question\"],\n",
    "                    template=\"\"\"你是DeepSeek VMAX-S知识助手。使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答\n",
    "                    案。总是在回答的最后说\"谢谢你的提问！\"。\n",
    "                    {context}\n",
    "                    问题: {question}\n",
    "                    \"\"\"\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        result = qa_chain({\"query\": question})\n",
    "        return result[\"result\"]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in QA chain: {str(e)}\")\n",
    "        return \"抱歉，处理您的请求时出现错误。\"\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(get_qa_chain_without_memory(\"介绍下VMAX的上网日志业务\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806a2da8-146e-4dbe-8195-436db64893ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27181a98-92c0-4cfc-8e39-9175a224c919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
