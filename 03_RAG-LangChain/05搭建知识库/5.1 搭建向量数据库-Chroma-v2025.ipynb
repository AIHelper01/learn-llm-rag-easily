{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建并使用向量数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前序数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data_base/knowledge_path/VMAX-S\\\\SJ-20130403110434-005-ZXCLOUD E9000（V1.0）刀片服务器 快速安装指南.pdf', '../data_base/knowledge_path/VMAX-S\\\\SJ-20140307103120-034-ZXCLOUD E9000（V1.0）刀片服务器 硬件安装指导.pdf', '../data_base/knowledge_path/VMAX-S\\\\SJ-20140326173355-003-ZXCLOUD I8350 G2（V1.0）机架服务器 硬件安装指导.pdf', '../data_base/knowledge_path/VMAX-S\\\\SJ-20151221160747-002-ZXVMAX（V6.15）多维价值分析系统 软件安装指导.pdf', '../data_base/knowledge_path/VMAX-S\\\\SJ-20151221160747-011-ZXVMAX（V6.15）多维价值分析系统 软件安装指导（CDMA分册）.pdf', '../data_base/knowledge_path/VMAX-S\\\\SJ-20151225170610-002-ZXVMAX（V6.15）多维价值分析系统 主题表安装指南（LTE分册）.pdf', '../data_base/knowledge_path/VMAX-S\\\\ZXVMAX-S（V6.19.20.10.03）硬件安装指导.pdf', '../data_base/knowledge_path/VMAX-S\\\\ZXVMAX-S（V6.23）安全加固指导.pdf']\n",
      "切分后的文件数量：695\n",
      "切分后的字符数（可以用来大致评估 token 数）：236550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4安装设备图4-6固定导轨支架\\n6.将中导轨拉出到锁定位置，并确保中导轨被锁定（A处锁定），如图4-7所示。说说说明明明：：：在装入服务器之前请确保滚珠座B在滑轨前段位置。图4-7拉出中导轨\\n7.将安装好内导轨的服务器沿着中导轨前端卡入，卡好后向前推入，如图4-8所示。说说说明明明：：：在将导轨卡入到中轨并推入的过程中，须注意将内导轨卡入到中导轨的滚珠中。\\nSJ-20140326173355-003|2014-04-01（R1.0）\\n4-5'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# 批量处理文件夹中所有文件\n",
    "import os\n",
    "\n",
    "# 获取folder_path下所有文件路径，储存在file_paths里\n",
    "file_paths = []\n",
    "folder_path = '../data_base/knowledge_path/VMAX-S'\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_paths.append(file_path)\n",
    "print(file_paths)\n",
    "\n",
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "# from langchain.document_loaders.markdown import UnstructuredMarkdownLoader\n",
    "\n",
    "# 遍历文件路径并把实例化的loader存放在loaders里\n",
    "loaders = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    file_type = file_path.split('.')[-1]\n",
    "    if file_type == 'pdf':\n",
    "        loaders.append(PyMuPDFLoader(file_path))\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {file_type} for file {file_path}\")\n",
    "\n",
    "# 下载文件并存储到text\n",
    "# 加载所有文档内容到 texts\n",
    "texts = []\n",
    "for loader in loaders:\n",
    "    texts.extend(loader.load())  # 关键步骤：初始化 texts\n",
    "\n",
    "    \n",
    "# 作数据清洗\n",
    "# 修改后的数据清洗部分（替换原始代码中对应段落）\n",
    "import re\n",
    "\n",
    "# 预编译正则表达式（提升效率）\n",
    "linebreak_pattern = re.compile(\n",
    "    r'(?<![\\\\u4e00-\\\\u9fff])\\n(?![\\\\u4e00-\\\\u9fff])',  # 负向断言匹配非中文环境换行\n",
    "    flags=re.DOTALL\n",
    ")\n",
    "space_pattern = re.compile(r'[ 　]+')  # 匹配半角/全角空格\n",
    "special_chars = ['•', '▪', '▫', '▶', '®', '©']  # 可扩展的干扰符号列表\n",
    "\n",
    "# 替换原始代码中的清洗循环\n",
    "for text in texts:\n",
    "    # 1. 清理非中文环境换行\n",
    "    text.page_content = re.sub(\n",
    "        linebreak_pattern,\n",
    "        lambda m: m.group().replace('\\n', ''),\n",
    "        text.page_content\n",
    "    )\n",
    "\n",
    "    # 2. 批量清理特殊符号\n",
    "    for char in special_chars:\n",
    "        text.page_content = text.page_content.replace(char, '')\n",
    "\n",
    "    # 3. 安全删除空格（保留URL等特殊场景）\n",
    "    text.page_content = space_pattern.sub('', text.page_content)\n",
    "\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv \n",
    "# pip install python-dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "zhipu_embed = ZhipuAIEmbeddings(\n",
    "    model=\"embedding-2\",\n",
    "    api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\",\n",
    ")\n",
    "\n",
    "\n",
    "''' \n",
    "* RecursiveCharacterTextSplitter 递归字符文本分割\n",
    "RecursiveCharacterTextSplitter 将按不同的字符递归地分割(按照这个优先级[\"\\n\\n\", \"\\n\", \" \", \"\"])，\n",
    "    这样就能尽量把所有和语义相关的内容尽可能长时间地保留在同一位置\n",
    "RecursiveCharacterTextSplitter需要关注的是4个参数：\n",
    "\n",
    "* separators - 分隔符字符串数组\n",
    "* chunk_size - 每个文档的字符数量限制\n",
    "* chunk_overlap - 两份文档重叠区域的长度\n",
    "* length_function - 长度计算函数\n",
    "'''\n",
    "#导入文本分割器\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "# 知识库中单段文本长度\n",
    "CHUNK_SIZE = 512\n",
    "\n",
    "# 知识库中相邻文本重合长度\n",
    "OVERLAP_SIZE = 50\n",
    "\n",
    "\n",
    "# 使用递归字符文本分割器\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=OVERLAP_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "split_docs = text_splitter.split_documents(texts)\n",
    "print(f\"切分后的文件数量：{len(split_docs)}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"切分后的字符数（可以用来大致评估 token 数）：{sum([len(doc.page_content) for doc in split_docs])}\")\n",
    "\n",
    "\n",
    "\n",
    "split_docs[90].page_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前序embedding模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZhipuAIEmbeddings模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "zhipu_embed = ZhipuAIEmbeddings(\n",
    "    model=\"embedding-2\",\n",
    "    api_key = os.environ['ZHIPUAI_API_KEY'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量处理文件夹中所有文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Chroma向量库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "涉及到的API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain_chroma.vectordbs.Chroma\n",
    "# class langchain_chroma.vectordbs.Chroma(\n",
    "#     collection_name: str = 'langchain', \n",
    "# embedding_function: Embeddings | None = None, \n",
    "# persist_directory: str | None = None, \n",
    "# client_settings: Settings | None = None, \n",
    "# collection_metadata: Dict | None = None, \n",
    "# client: ClientAPI | None = None, \n",
    "# relevance_score_fn: Callable[[float], float] | None = None, \n",
    "# create_collection_if_not_exists: bool | None = True)\n",
    "\n",
    "\n",
    "# classmethod from_documents(\n",
    "#     documents: List[Document], \n",
    "# embedding: Embeddings | None = None, \n",
    "# ids: List[str] | None = None, \n",
    "# collection_name: str = 'langchain', \n",
    "# persist_directory: str | None = None, \n",
    "# client_settings: Settings | None = None, \n",
    "# client: ClientAPI | None = None, \n",
    "# collection_metadata: Dict | None = None, \n",
    "# **kwargs: Any)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接调用Chroma.from_documents (推荐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量数据库已成功持久化到磁盘。\n"
     ]
    }
   ],
   "source": [
    "# 汇总\n",
    "\n",
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "# from langchain_chroma.vectorstores import Chroma\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "# 定义持久化目录\n",
    "persist_directory = '../chroma-vmax-7'\n",
    "\n",
    "# 创建嵌入模型\n",
    "embedding = ZhipuAIEmbeddings(model=\"embedding-2\", api_key = os.environ['ZHIPUAI_API_KEY'])\n",
    "\n",
    "try:\n",
    "    # 初始化 Chroma 向量数据库\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=split_docs[:20],  # 为了速度，只选择前 20 个切分的 doc 进行生成\n",
    "        # documents=split_docs,  # 为了速度，只选择前 20 个切分的 doc 进行生成\n",
    "        embedding=embedding,\n",
    "        # collection_name=\"test1\", # 如果不指定默认为langchain\n",
    "        persist_directory=persist_directory, # 允许我们将persist_directory目录保存到磁盘上\n",
    "    )\n",
    "    \n",
    "    # 持久化向量数据库\n",
    "    vectordb.persist()\n",
    "    print(\"向量数据库已成功持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码会报错： 持久化过程中发生错误: Error code: 400, with error text {\"error\":{\"code\":\"1214\",\"message\":\"input数组最大不得超过64条\"}}\n",
    "\n",
    "建议分批次导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理第 1/24 批文档 (文档 0-29)\n",
      "第 1 批文档已成功导入并持久化\n",
      "正在处理第 2/24 批文档 (文档 30-59)\n",
      "第 2 批文档已成功导入并持久化\n",
      "正在处理第 3/24 批文档 (文档 60-89)\n",
      "第 3 批文档已成功导入并持久化\n",
      "正在处理第 4/24 批文档 (文档 90-119)\n",
      "第 4 批文档已成功导入并持久化\n",
      "正在处理第 5/24 批文档 (文档 120-149)\n",
      "第 5 批文档已成功导入并持久化\n",
      "正在处理第 6/24 批文档 (文档 150-179)\n",
      "第 6 批文档已成功导入并持久化\n",
      "正在处理第 7/24 批文档 (文档 180-209)\n",
      "第 7 批文档已成功导入并持久化\n",
      "正在处理第 8/24 批文档 (文档 210-239)\n",
      "第 8 批文档已成功导入并持久化\n",
      "正在处理第 9/24 批文档 (文档 240-269)\n",
      "第 9 批文档已成功导入并持久化\n",
      "正在处理第 10/24 批文档 (文档 270-299)\n",
      "第 10 批文档已成功导入并持久化\n",
      "正在处理第 11/24 批文档 (文档 300-329)\n",
      "第 11 批文档已成功导入并持久化\n",
      "正在处理第 12/24 批文档 (文档 330-359)\n",
      "第 12 批文档已成功导入并持久化\n",
      "正在处理第 13/24 批文档 (文档 360-389)\n",
      "第 13 批文档已成功导入并持久化\n",
      "正在处理第 14/24 批文档 (文档 390-419)\n",
      "第 14 批文档已成功导入并持久化\n",
      "正在处理第 15/24 批文档 (文档 420-449)\n",
      "第 15 批文档已成功导入并持久化\n",
      "正在处理第 16/24 批文档 (文档 450-479)\n",
      "第 16 批文档已成功导入并持久化\n",
      "正在处理第 17/24 批文档 (文档 480-509)\n",
      "第 17 批文档已成功导入并持久化\n",
      "正在处理第 18/24 批文档 (文档 510-539)\n",
      "第 18 批文档已成功导入并持久化\n",
      "正在处理第 19/24 批文档 (文档 540-569)\n",
      "第 19 批文档已成功导入并持久化\n",
      "正在处理第 20/24 批文档 (文档 570-599)\n",
      "第 20 批文档已成功导入并持久化\n",
      "正在处理第 21/24 批文档 (文档 600-629)\n",
      "第 21 批文档已成功导入并持久化\n",
      "正在处理第 22/24 批文档 (文档 630-659)\n",
      "第 22 批文档已成功导入并持久化\n",
      "正在处理第 23/24 批文档 (文档 660-689)\n",
      "第 23 批文档已成功导入并持久化\n",
      "正在处理第 24/24 批文档 (文档 690-694)\n",
      "第 24 批文档已成功导入并持久化\n",
      "所有文档已成功导入并持久化到向量数据库。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "# 定义持久化目录\n",
    "persist_directory = '../chroma-vmax'\n",
    "\n",
    "# 创建嵌入模型\n",
    "embedding = ZhipuAIEmbeddings(model=\"embedding-2\", api_key=os.environ['ZHIPUAI_API_KEY'])\n",
    "\n",
    "# 定义每批处理的文档数量\n",
    "batch_size = 30\n",
    "\n",
    "try:\n",
    "    # 计算总批次数\n",
    "    total_batches = (len(split_docs) + batch_size - 1) // batch_size\n",
    "    \n",
    "    # 初始化向量数据库（如果是第一次创建）\n",
    "    vectordb = None\n",
    "    \n",
    "    for batch_num in range(total_batches):\n",
    "        # 计算当前批次的起始和结束索引\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min((batch_num + 1) * batch_size, len(split_docs))\n",
    "        \n",
    "        # 获取当前批次的文档\n",
    "        batch_docs = split_docs[start_idx:end_idx]\n",
    "        \n",
    "        print(f\"正在处理第 {batch_num + 1}/{total_batches} 批文档 (文档 {start_idx}-{end_idx-1})\")\n",
    "        \n",
    "        if batch_num == 0:\n",
    "            # 第一次创建向量数据库\n",
    "            vectordb = Chroma.from_documents(\n",
    "                collection_name='vmax-s',\n",
    "                documents=batch_docs,\n",
    "                embedding=embedding,\n",
    "                persist_directory=persist_directory\n",
    "            )\n",
    "        else:\n",
    "            # 后续批次添加到现有集合\n",
    "            vectordb.add_documents(batch_docs)\n",
    "        \n",
    "        # 每批处理后持久化\n",
    "        vectordb.persist()\n",
    "        print(f\"第 {batch_num + 1} 批文档已成功导入并持久化\")\n",
    "    \n",
    "    print(\"所有文档已成功导入并持久化到向量数据库。\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"处理过程中发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "charma服务器版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key = os.environ['ZHIPUAI_API_KEY'],\n",
    "                model_name=\"embedding-2\"\n",
    "            )\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"test4\",\n",
    "    embedding_function=openai_ef,  # 使用openai\n",
    "    metadata={\n",
    "        \"hnsw:space\": \"cosine\", \n",
    "        \"hnsw:construction_ef\": 150,\n",
    "        \"hnsw:M\": 24,\n",
    "        \"hnsw:num_threads\": 8\n",
    "    }\n",
    ")\n",
    "ids_list = [f\"doc_{i+1}\" for i in range(len(split_docs[:20]))]\n",
    "print(ids_list)\n",
    "\n",
    "try:\n",
    "    # 添加文档到集合\n",
    "    collection.add(documents=split_docs[:20], ids =ids_list )\n",
    "    # 持久化向量数据库\n",
    "    vectordb.persist()\n",
    "    print(\"向量数据库已成功持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过chroma数据库add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按官网文档进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "content_list = ['本发布日期南书', \n",
    "                '前言周志华老师的《机器学习》西瓜书是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读者通过西瓜书对机器学习有所了解所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推导细节的读者来说可能不太友好，本书旨在对西瓜书里比较难理解的公式加以解析，以及对部分公式补充具体的推导细节。读到这里，大家可能会疑问为啥前面这段话加了引号，因为这只是我们最初的遐想，后来我们了解到，周老师之所以省去这些推导细节的真实原因是，他本尊认为理工科数学基础扎实点的大二下学生应该对西瓜书中的推导细节无困难吧，要点在书里都有了，略去的细节应能脑补或做练习。所以本南瓜书只能算是我等数学渣渣在自学的时候记下来的笔记，希望能够帮助大家都成为一名合格的理工科数学基础扎实点的大二下学生。使用说明南瓜书的所有内容都是以西瓜书的内容为前置知识进行表述的，所以南瓜书的最佳使用方法是以西瓜书为主线，遇到自己推导不出来或者看不懂的公式时再来查阅南瓜书；对于初学机器学习的小白，西瓜书第章和第章的公式强烈不建议深究，简单过一下即可，等你学得', \n",
    "                '有点飘的时候再回来啃都来得及；每个公式的解析和推导我们都力争以本科数学基础的视角进行讲解，所以超纲的数学知识我们通常都会以附录和参考文献的形式给出，感兴趣的同学可以继续沿着我们给的资料进行深入学习；若南瓜书里没有你想要查阅的公式，或者你发现南瓜书哪个地方有错误，请毫不犹豫地去我们的地址：进行反馈，在对应版块提交你希望补充的公式编号或者勘误信息，我们通常会在小时以内给您回复，超过小时未回复的话可以微信联系我们微信号：；配套视频教程：在线阅读地址：仅供第版最新版获取地址：编委会', \n",
    "                '编委会主编：、、编委：、、、、封面设计：构思、创作林王茂盛致谢特别感谢、、、、、、、、、、、在最早期的时候对南瓜书所做的贡献。扫描下方二维码，然后回复关键词南瓜书，即可加入南瓜书读者交流群版权声明本作品采用知识共享署名非商业性使用相同方式共享国际许可协议进行许可。'\n",
    "               ]\n",
    "print(len(content_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=[\"doc1\", \"doc2\", \"doc3\", ...],\n",
    "    embeddings=[[1.1, 2.3, 3.2], [4.5, 6.9, 4.4], [1.1, 2.3, 3.2], ...],\n",
    "    metadatas=[{\"chapter\": \"3\", \"verse\": \"16\"}, {\"chapter\": \"3\", \"verse\": \"5\"}, {\"chapter\": \"29\", \"verse\": \"11\"}, ...],\n",
    "    ids=[\"id1\", \"id2\", \"id3\", ...]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\will\\.cache\\chroma\\onnx_models\\all-MiniLM-L6-v2\\onnx.tar.gz: 100%|███████| 79.3M/79.3M [05:29<00:00, 253kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据写入成功，已自动持久化到磁盘。\n"
     ]
    }
   ],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "import os\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "import chromadb\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "from chromadb import PersistentClient\n",
    "# 定义持久化目录\n",
    "persist_directory = '../chroma-vmax'\n",
    "# 初始化客户端（持久化模式）\n",
    "client = PersistentClient(path=persist_directory)\n",
    "\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key = os.environ['ZHIPUAI_API_KEY'],\n",
    "                model_name=\"embedding-2\"\n",
    "            )\n",
    "# collection = client.get_or_create_collection(\n",
    "#     name=\"test\",\n",
    "#     embedding_function=openai_ef,  # 使用openai\n",
    "#     metadata={\n",
    "#         \"hnsw:space\": \"cosine\", \n",
    "#         \"hnsw:construction_ef\": 150,\n",
    "#         \"hnsw:M\": 24,\n",
    "#         \"hnsw:num_threads\": 8\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# HNSW索引构建参数优化\n",
    "collection = client.create_collection(\n",
    "    name=\"high_perf\",\n",
    "    metadata={\n",
    "        \"hnsw:construction_ef\": 200,  # 构建阶段搜索范围（默认100）\n",
    "        \"hnsw:search_ef\": 320,        # 查询阶段搜索范围 \n",
    "        \"hnsw:M\": 48                  # 图节点最大连接数\n",
    "    }\n",
    ")\n",
    "\n",
    "content_list = ['本发布日期南书', \n",
    "                '前言周志华老师的《机器学习》西瓜书是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读者通过西瓜书对机器学习有所了解所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推导细节的读者来说可能不太友好，本书旨在对西瓜书里比较难理解的公式加以解析，以及对部分公式补充具体的推导细节。读到这里，大家可能会疑问为啥前面这段话加了引号，因为这只是我们最初的遐想，后来我们了解到，周老师之所以省去这些推导细节的真实原因是，他本尊认为理工科数学基础扎实点的大二下学生应该对西瓜书中的推导细节无困难吧，要点在书里都有了，略去的细节应能脑补或做练习。所以本南瓜书只能算是我等数学渣渣在自学的时候记下来的笔记，希望能够帮助大家都成为一名合格的理工科数学基础扎实点的大二下学生。使用说明南瓜书的所有内容都是以西瓜书的内容为前置知识进行表述的，所以南瓜书的最佳使用方法是以西瓜书为主线，遇到自己推导不出来或者看不懂的公式时再来查阅南瓜书；对于初学机器学习的小白，西瓜书第章和第章的公式强烈不建议深究，简单过一下即可，等你学得', \n",
    "                '有点飘的时候再回来啃都来得及；每个公式的解析和推导我们都力争以本科数学基础的视角进行讲解，所以超纲的数学知识我们通常都会以附录和参考文献的形式给出，感兴趣的同学可以继续沿着我们给的资料进行深入学习；若南瓜书里没有你想要查阅的公式，或者你发现南瓜书哪个地方有错误，请毫不犹豫地去我们的地址：进行反馈，在对应版块提交你希望补充的公式编号或者勘误信息，我们通常会在小时以内给您回复，超过小时未回复的话可以微信联系我们微信号：；配套视频教程：在线阅读地址：仅供第版最新版获取地址：编委会', \n",
    "                '编委会主编：、、编委：、、、、封面设计：构思、创作林王茂盛致谢特别感谢、、、、、、、、、、、在最早期的时候对南瓜书所做的贡献。扫描下方二维码，然后回复关键词南瓜书，即可加入南瓜书读者交流群版权声明本作品采用知识共享署名非商业性使用相同方式共享国际许可协议进行许可。'\n",
    "               ]\n",
    "# print(len(content_list))\n",
    "\n",
    "# 如果每个文档块的元数据需要个性化（例如记录分块编号），可以生成与文档数量匹配的元数据列表：\n",
    "# metadatas = [{\"source\": \"权威文档.pdf\", \"chunk_id\": i} for i in range(len(split_docs[:20]))]\n",
    "\n",
    "# 如果所有文档块的元数据相同（例如来源一致），可通过列表推导式生成等长元数据列表：\n",
    "metadatas = [{\"source\": \"权威文档.pdf\"} for _ in range(len(split_docs[:20]))]\n",
    "\n",
    "ids_list = [f\"doc_{i+1}\" for i in range(len(content_list))]\n",
    "# print(ids_list)\n",
    "\n",
    "\n",
    "# print(metadatas)\n",
    "\n",
    "try:\n",
    "    # 添加文档到集合（修正后的参数）\n",
    "    collection.add(\n",
    "        documents=content_list,  # 传入纯文本列表\n",
    "        metadatas=[{\"source\": \"权威文档.pdf\"},{\"source\": \"权威文档.pdf\"},{\"source\": \"权威文档.pdf\"},{\"source\": \"权威文档.pdf\"} ],\n",
    "        ids=ids_list\n",
    "    )\n",
    "    print(\"数据写入成功，已自动持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")\n",
    "except chromadb.errors.ChromaError as ce:\n",
    "    print(f\"数据库错误: {ce}\")\n",
    "except ValueError as ve:\n",
    "    print(f\"数据格式错误: {ve}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正式方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document embeddings:\n",
      "Document 1 embedding: [-0.1225031167268753, 0.6253135800361633, -3.54880428314209, -2.0378825664520264, 1.4127538204193115, -0.17774423956871033, -0.39122408628463745, 0.49636057019233704, -0.22381550073623657, -0.19290287792682648]\n",
      "Document 2 embedding: [0.377452552318573, 0.9425770044326782, -3.3817975521087646, -1.5095171928405762, 1.954257845878601, -0.23311898112297058, -0.04492451250553131, -0.3590812087059021, -0.6258770823478699, -0.5528176426887512]\n",
      "\n",
      "Query embedding:\n",
      "Query embedding: [0.5814239978790283, 0.08968696743249893, -3.2821342945098877, -1.6575878858566284, 1.7467120885849, -0.9489914774894714, -0.8352117538452148, -0.02159128151834011, -0.279095858335495, -0.3286992311477661]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "ollama_emb = OllamaEmbeddings(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"nomic-embed-text:latest\"\n",
    ")\n",
    "r1 = ollama_emb.embed_documents(\n",
    "    [\n",
    "        \"Alpha is the first letter of Greek alphabet\",\n",
    "        \"Beta is the second letter of Greek alphabet\",\n",
    "    ]\n",
    ")\n",
    "r2 = ollama_emb.embed_query(\n",
    "    \"What is the second letter of Greek alphabet\"\n",
    ")\n",
    "\n",
    "# 打印结果\n",
    "print(\"Document embeddings:\")\n",
    "for i, embedding in enumerate(r1):\n",
    "    print(f\"Document {i+1} embedding: {embedding[:10]}\")\n",
    "\n",
    "print(\"\\nQuery embedding:\")\n",
    "print(f\"Query embedding: {r2[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始响应内容: 404 page not found\n",
      "错误详情: Extra data: line 1 column 5 (char 4)\n"
     ]
    }
   ],
   "source": [
    "from chromadb.utils.embedding_functions.ollama_embedding_function import OllamaEmbeddingFunction\n",
    "import requests\n",
    "\n",
    "ollama_ef = OllamaEmbeddingFunction(\n",
    "    url=\"http://localhost:11434\",\n",
    "    model_name=\"qwen2.5:0.5b \"\n",
    ")\n",
    "\n",
    "try:\n",
    "    response = ollama_ef._session.post(\n",
    "        ollama_ef._api_url,\n",
    "        json={\"model\": \"nnomic-embed-text:latest\", \"prompt\": \"Test\"}\n",
    "    )\n",
    "    print(\"原始响应内容:\", response.text)  # 打印原始响应\n",
    "    embeddings = ollama_ef([\"Test\"])\n",
    "except Exception as e:\n",
    "    print(f\"错误详情: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 1 column 5 (char 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[116]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membedding_functions\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mollama_embedding_function\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OllamaEmbeddingFunction\n\u001b[32m      2\u001b[39m ollama_ef = OllamaEmbeddingFunction(url=\u001b[33m\"\u001b[39m\u001b[33mhttp://localhost:11434/v1\u001b[39m\u001b[33m\"\u001b[39m,model_name=\u001b[33m\"\u001b[39m\u001b[33mnomic-embed-text:latest\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m embeddings = \u001b[43mollama_ef\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mThis is my first text to embed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\ProgramData\\miniconda3\\envs\\langchain\\Lib\\site-packages\\chromadb\\api\\types.py:466\u001b[39m, in \u001b[36mEmbeddingFunction.__init_subclass__.<locals>.__call__\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m: EmbeddingFunction[D], \u001b[38;5;28minput\u001b[39m: D) -> Embeddings:\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     result = \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    468\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m validate_embeddings(cast(Embeddings, normalize_embeddings(result)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\ProgramData\\miniconda3\\envs\\langchain\\Lib\\site-packages\\chromadb\\utils\\embedding_functions\\ollama_embedding_function.py:48\u001b[39m, in \u001b[36mOllamaEmbeddingFunction.__call__\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Call Ollama Server API for each document\u001b[39;00m\n\u001b[32m     44\u001b[39m texts = \u001b[38;5;28minput\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;28minput\u001b[39m]\n\u001b[32m     45\u001b[39m embeddings = [\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts\n\u001b[32m     50\u001b[39m ]\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m     52\u001b[39m     Embeddings,\n\u001b[32m     53\u001b[39m     [\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m     ],\n\u001b[32m     58\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\ProgramData\\miniconda3\\envs\\langchain\\Lib\\site-packages\\httpx\\_models.py:832\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mjson\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m--> \u001b[39m\u001b[32m832\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjsonlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\ProgramData\\miniconda3\\envs\\langchain\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\ProgramData\\miniconda3\\envs\\langchain\\Lib\\json\\decoder.py:341\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    339\u001b[39m end = _w(s, end).end()\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExtra data\u001b[39m\u001b[33m\"\u001b[39m, s, end)\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Extra data: line 1 column 5 (char 4)"
     ]
    }
   ],
   "source": [
    "from chromadb.utils.embedding_functions.ollama_embedding_function import OllamaEmbeddingFunction\n",
    "ollama_ef = OllamaEmbeddingFunction(url=\"http://localhost:11434\",model_name=\"nomic-embed-text:latest\")\n",
    "embeddings = ollama_ef(\"This is my first text to embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "持久化过程中发生错误: JSONDecodeError.__init__() missing 2 required positional arguments: 'doc' and 'pos'\n"
     ]
    }
   ],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "import os\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "import chromadb\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "from chromadb import PersistentClient\n",
    "# 定义持久化目录\n",
    "persist_directory = '../chroma-vmax-3'\n",
    "# 初始化客户端（持久化模式）\n",
    "client = PersistentClient(path=persist_directory)\n",
    "\n",
    "from chromadb.utils.embedding_functions.ollama_embedding_function import OllamaEmbeddingFunction\n",
    "ollama_ef = OllamaEmbeddingFunction(url=\"http://localhost:11434\",model_name=\"nomic-embed-text:latest\")\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"vmax-s\",\n",
    "    embedding_function=ollama_ef,  # 可选，但建议保持一致性\n",
    "    metadata={\n",
    "        \"hnsw:construction_ef\": 200,  # 构建阶段搜索范围（默认100）\n",
    "        \"hnsw:search_ef\": 320,        # 查询阶段搜索范围 \n",
    "        \"hnsw:M\": 48                  # 图节点最大连接数\n",
    "    }\n",
    ")\n",
    "\n",
    "# content_list = [doc.page_content for doc in split_docs[:20]]\n",
    "content_list = [doc.page_content for doc in split_docs]\n",
    "\n",
    "# 如果每个文档块的元数据需要个性化（例如记录分块编号），可以生成与文档数量匹配的元数据列表：\n",
    "# metadatas = [{\"source\": \"权威文档.pdf\", \"chunk_id\": i} for i in range(len(split_docs[:20]))]\n",
    "\n",
    "# 如果所有文档块的元数据相同（例如来源一致），可通过列表推导式生成等长元数据列表：\n",
    "# metadatas = [{\"source\": \"VMAX安装系列.pdf\"} for _ in range(len(split_docs[:20]))]\n",
    "metadatas = [{\"source\": \"VMAX安装系列.pdf\"} for _ in range(len(split_docs))]\n",
    "\n",
    "# ids_list = [f\"doc_{i+1}\" for i in range(len(split_docs[:20]))]\n",
    "ids_list = [f\"doc_{i+1}\" for i in range(len(split_docs))]\n",
    "# print(ids_list)\n",
    "\n",
    "\n",
    "# print(metadatas)\n",
    "\n",
    "try:\n",
    "    # 添加文档到集合（修正后的参数）\n",
    "    collection.add(\n",
    "        documents=content_list,  # 传入纯文本列表\n",
    "        metadatas=metadatas, \n",
    "        ids=ids_list\n",
    "    )\n",
    "    print(\"数据写入成功，已自动持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")\n",
    "except chromadb.errors.ChromaError as ce:\n",
    "    print(f\"数据库错误: {ce}\")\n",
    "except ValueError as ve:\n",
    "    print(f\"数据格式错误: {ve}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过chroma数据库add-改良版 \n",
    "每次三个doc，分批次导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "持久化过程中发生错误: status_code: 429, body: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'} in add.\n"
     ]
    }
   ],
   "source": [
    "# 汇总\n",
    "\n",
    "import os\n",
    "# from langchain.vectordbs.chroma import Chroma\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "from chromadb import PersistentClient\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "# 定义持久化目录\n",
    "persist_directory = '../chroma-vmax-batch'\n",
    "# 初始化客户端（持久化模式）\n",
    "client = PersistentClient(path=persist_directory)\n",
    "\n",
    "# 创建嵌入模型\n",
    "# import chromadb.utils.embedding_functions as embedding_functions\n",
    "# openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "#                 api_key = os.environ['ZHIPUAI_API_KEY'],\n",
    "#                 model_name=\"embedding-2\"\n",
    "#             )\n",
    "\n",
    "# import chromadb.utils.embedding_functions as embedding_functions\n",
    "# cohere_ef  = embedding_functions.CohereEmbeddingFunction(api_key=\"Tahx1eySFbKvu9sTyTXrRLf59la3ZUG9vy02stRZ\",  model_name=\"embed-multilingual-v3.0\")\n",
    "\n",
    "zhipu_ef = ZhipuAIAdapter(model_name=\"embedding-2\", api_key=\"5713143e8fdc4b4a8b284cf97092e70f.qEK71mGIlavzO1Io\")\n",
    "\n",
    "# 创建集合并配置HNSW参数\n",
    "# collection = client.create_collection( \n",
    "# collection = client.get_or_create_collection(\n",
    "#     name=\"test4\",\n",
    "#     embedding_function=openai_ef,\n",
    "#     metadata={\n",
    "#         \"hnsw:space\": \"cosine\", \n",
    "#         \"hnsw:construction_ef\": 150,\n",
    "#         \"hnsw:M\": 24,\n",
    "#         \"hnsw:num_threads\": 8\n",
    "#     }\n",
    "# )\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"vmax-s\",\n",
    "    embedding_function=cohere_ef,  # 可选，但建议保持一致性\n",
    "    metadata={\n",
    "        \"hnsw:construction_ef\": 200,  # 构建阶段搜索范围（默认100）\n",
    "        \"hnsw:search_ef\": 320,        # 查询阶段搜索范围 \n",
    "        \"hnsw:M\": 48                  # 图节点最大连接数\n",
    "    }\n",
    ")\n",
    "# content_list = [doc.page_content for doc in split_docs[:20]]\n",
    "content_list = [doc.page_content for doc in split_docs]\n",
    "\n",
    "# 如果每个文档块的元数据需要个性化（例如记录分块编号），可以生成与文档数量匹配的元数据列表：\n",
    "# metadatas = [{\"source\": \"权威文档.pdf\", \"chunk_id\": i} for i in range(len(split_docs[:20]))]\n",
    "\n",
    "# 如果所有文档块的元数据相同（例如来源一致），可通过列表推导式生成等长元数据列表：\n",
    "# metadatas = [{\"source\": \"VMAX安装系列.pdf\"} for _ in range(len(split_docs[:20]))]\n",
    "metadatas = [{\"source\": \"VMAX安装系列.pdf\"} for _ in range(len(split_docs))]\n",
    "\n",
    "# ids_list = [f\"doc_{i+1}\" for i in range(len(split_docs[:20]))]\n",
    "ids_list = [f\"doc_{i+1}\" for i in range(len(split_docs))]\n",
    "# print(ids_list)\n",
    "\n",
    "try:\n",
    "    # 添加文档到集合\n",
    "    # collection.add(documents=split_docs)\n",
    "\n",
    "    from itertools import batched\n",
    "    docs_batches = batched(content_list, 3)  # 分3个批次\n",
    "    ids_batches = batched(ids_list, 3)\n",
    "\n",
    "    for docs, ids in zip(docs_batches, ids_batches):\n",
    "        collection.add(\n",
    "            documents=content_list,  # 传入纯文本列表\n",
    "            metadatas=metadatas, \n",
    "            ids=ids_list\n",
    "        )\n",
    "    \n",
    "    # 持久化向量数据库\n",
    "    vectordb.persist()\n",
    "    print(\"向量数据库已成功持久化到磁盘。\")\n",
    "except Exception as e:\n",
    "    print(f\"持久化过程中发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 连接chroma数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据库路径 ./chroma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client  = chromadb.PersistentClient(path=\"../chroma-vmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vmax-s']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取一个存在的Collection对象\n",
    "collection = chroma_client.get_collection(\"vmax-s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "695"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count()  #  returns the number of items in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.peek()  # returns a list of the first 10 items in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取所有数据（默认不返回嵌入向量）\n",
    "all_data = collection.get()\n",
    "print(all_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量检索\n",
    "### 3.1 相似度检索\n",
    "Chroma的相似度搜索使用的是余弦距离，即：\n",
    "$$\n",
    "similarity = cos(A, B) = \\frac{A \\cdot B}{\\parallel A \\parallel \\parallel B \\parallel} = \\frac{\\sum_1^n a_i b_i}{\\sqrt{\\sum_1^n a_i^2}\\sqrt{\\sum_1^n b_i^2}}\n",
    "$$\n",
    "其中$a_i$、$b_i$分别是向量$A$、$B$的分量。\n",
    "\n",
    "当你需要数据库返回严谨的按余弦相似度排序的结果时可以使用`similarity_search`函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Chroma.from_documents时，用这个方法： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"vmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Client' object has no attribute 'similarity_search'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m sim_docs = \u001b[43mchroma_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m(question,k=\u001b[32m3\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m检索到的内容数：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sim_docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Client' object has no attribute 'similarity_search'"
     ]
    }
   ],
   "source": [
    "sim_docs = vectordb.similarity_search(question,k=3)\n",
    "print(f\"检索到的内容数：{len(sim_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的第0个内容: \n",
      "3.7加固后检查\n",
      "3.7.1账⼾系统共安全加固检查安全加固指导\n",
      "19-25\n",
      "--------------\n",
      "检索到的第1个内容: \n",
      "数据库数据安全加固体系数据库合规与漏洞参⻅第3.5章节\n",
      "3.安全加固操作\n",
      "3.1加固流程⾸次开局/新建⽹络、升级、周期性维护等的加固流程如下图所⽰。安全加固分为三个阶段：前期加固、现场加固、后期加固。\n",
      "1)前期加固：主要是出⼚前的预加固，⽐如操作系统等，在初始版本的基础上保证将补丁安装⾄最新，合规配置符合基本要求等。\n",
      "2)现场加固：现场安装配置的加固，由于现场客⼾有⾃⼰的⼯具扫描，会发现⼀些未符合\n",
      "--------------\n",
      "检索到的第2个内容: \n",
      "11..适适⽤⽤范范围围产品安全加固的⽬的是“通过配置、设置、协议限制来减⼩攻击⾯”（来源于《产品安全要求总则》），使得产品免于或减⼩所受⽹络安全攻击的影响。产品安全加固对象包括产品所使⽤的第三⽅软件（如操作系统等），以及部分⾃研模块。安全合加固置是通过对加固对象中安全相关的配置项进⾏合理的设置，减⼩对象的攻击⾯，例如关闭不必要的端⼝、不必要的系统服务等；安全漏洞补丁是通过对加固对象的漏洞缺陷通过\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for i, sim_doc in enumerate(sim_docs):\n",
    "    print(f\"检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 MMR检索\n",
    "如果只考虑检索出内容的相关性会导致内容过于单一，可能丢失重要信息。\n",
    "\n",
    "最大边际相关性 (`MMR, Maximum marginal relevance`) 可以帮助我们在保持相关性的同时，增加内容的丰富度。\n",
    "\n",
    "核心思想是在已经选择了一个相关性高的文档之后，再选择一个与已选文档相关性较低但是信息丰富的文档。这样可以在保持相关性的同时，增加内容的多样性，避免过于单一的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr_docs = vectordb.max_marginal_relevance_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMR 检索到的第0个内容: \n",
      "3.7加固后检查\n",
      "3.7.1账⼾系统共安全加固检查安全加固指导\n",
      "19-25\n",
      "--------------\n",
      "MMR 检索到的第1个内容: \n",
      "数据库数据安全加固体系数据库合规与漏洞参⻅第3.5章节\n",
      "3.安全加固操作\n",
      "3.1加固流程⾸次开局/新建⽹络、升级、周期性维护等的加固流程如下图所⽰。安全加固分为三个阶段：前期加固、现场加固、后期加固。\n",
      "1)前期加固：主要是出⼚前的预加固，⽐如操作系统等，在初始版本的基础上保证将补丁安装⾄最新，合规配置符合基本要求等。\n",
      "2)现场加固：现场安装配置的加固，由于现场客⼾有⾃⼰的⼯具扫描，会发现⼀些未符合\n",
      "--------------\n",
      "MMR 检索到的第2个内容: \n",
      "6安装机柜图6-38安装抗震加固组件（2）\n",
      "1.横架板\n",
      "2.竖架板\n",
      "3.机房走线架\n",
      "4.内六角螺栓M12×25/M8×25（含弹簧垫圈、垫片和绝缘垫圈）\n",
      "5.绝缘垫片\n",
      "6.锁紧杆（含六角螺母M8、弹簧垫圈和垫片）\n",
      "7.螺栓（含螺母M8、弹簧垫圈和垫片）\n",
      "SJ-20190815094124-009|2020-07-30（R1.0）\n",
      "6-33\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for i, sim_doc in enumerate(mmr_docs):\n",
    "    print(f\"MMR 检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
